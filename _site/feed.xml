<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Robotics</title>
    <description>Robotics</description>
    <link>/robotics-course-materials/robotics-course-materials/</link>
    <atom:link href="/robotics-course-materials/robotics-course-materials/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 09 Mar 2016 12:49:30 -0500</pubDate>
    <lastBuildDate>Wed, 09 Mar 2016 12:49:30 -0500</lastBuildDate>
    <generator>Jekyll v2.4.0</generator>
    
      
      <item>
        <title>Programming autonomous robots</title>
        <description>&lt;p&gt;This module will discuss some topics for those new to robot programming. 
I&amp;#39;ll start with programming general real-time systems, talk next about
an architecture for a typical robot system- including &lt;em&gt;planners&lt;/em&gt; and 
&lt;em&gt;controllers&lt;/em&gt;, and conclude with many thoughts on the open problem of 
combining reactive and deliberative behaviors for dextrous robots.&lt;/p&gt;

&lt;h2 id=&quot;programming-real-time-systems&quot;&gt;Programming real-time systems&lt;/h2&gt;

&lt;p&gt;Real-time systems are generally best controlled by a real time operating 
system, which provide specific process scheduling policies and minimal
interrupt latency. The key feature of a real time operating system is its
predictability.&lt;/p&gt;

&lt;p&gt;For controlling robots, the user selects a control frequency (e.g., 100 Hz),
and the real time operating system ensures that this process is run 
regularly- every 10 ms (plus or minus some small amount of error). &lt;em&gt;It is the
programmer&amp;#39;s responsibility to ensure that their process does not overrun the
process&amp;#39; allotted time&lt;/em&gt; (10 ms in the example above). If such an overrun
occurs, the real time operating system will typically generate an exception.
Programmers typically avoid I/O and memory allocation in real-time control
loops, as these procedures are often unpredictable. &lt;/p&gt;

&lt;h2 id=&quot;architecture-of-a-robotic-system&quot;&gt;Architecture of a robotic system&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Autonomous&lt;/em&gt; robotic systems- as opposed to those robots situated in controlled
environments (like factories)- are typically driven using a small number of 
&lt;em&gt;behaviors&lt;/em&gt;, modular components that focus on getting the robot to perform a single task (like avoiding obstacles, homing to a light source, or following a
wall). Many robot architectures are built on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Finite-state_machine&quot;&gt;finite state machine&lt;/a&gt; model, with behaviors representing the states and transitions between behaviors occuring in response to events. &lt;/p&gt;

&lt;p&gt;The reason these interacting behaviors are used, instead of the previously
dominant &lt;em&gt;sense-plan-act&lt;/em&gt; approach, is depicted in &lt;a href=&quot;https://www.youtube.com/watch?v=qXdn6ynwpiI&quot;&gt;this video of an early robot, Shakey&lt;/a&gt;: the robot moves too
slowly to act and react in uncontrolled environments.&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Data flow and computation using the sense-plan-act approach to robot architecture.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/SPA.png&quot; alt=&quot;Data flow and computation using the sense-plan-act approach to robot architecture.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;For example, here is a finite state machine-based architecture for a simple
foraging robot:&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Architecture for a simple foraging robot. Graph nodes represent machine states and transitions represent events and sensory signals. Image from Bristol Robotics Laboratory.&lt;/caption&gt;
&lt;img src=&quot;http://www.brl.ac.uk/images/finite-state-machine.png&quot; alt=&quot;Architecture for a simple foraging robot. Graph nodes represent machine states and transitions represent events and sensory signals. Image from Bristol Robotics Laboratory.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;The figure below shows the architecture of a more sophisticated autonomous 
robot capable of manipulating its environment. The depicted architecture does 
not represent a single task, but rather depicts the&lt;br&gt;
flow of data between software and hardware and real-time and non-real-time
software components. &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;An example of a software architecture for an autonomous robot, the Willow Garage PR2 mobile manipulator.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/robot-software-architecture.png&quot; alt=&quot;An example of a software architecture for an autonomous robot, the Willow Garage PR2 mobile manipulator.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h3 id=&quot;important-components&quot;&gt;Important components&lt;/h3&gt;

&lt;h4 id=&quot;interprocess-communication&quot;&gt;Interprocess communication&lt;/h4&gt;

&lt;p&gt;As can be seen from the figure, communication between multiple computational
processes running simultaneously is important. The particular mechanism
chosen for &lt;em&gt;interprocess communication&lt;/em&gt; (IPC) is critical: latency between
one process transmitting data and the other receiving it should be minimized.
The IPC mechanism depicted in this figure is &lt;em&gt;shared memory&lt;/em&gt;, which is not
necessarily friendly to program with, but is quite fast.&lt;/p&gt;

&lt;h4 id=&quot;sensors&quot;&gt;Sensors&lt;/h4&gt;

&lt;p&gt;The robot depicted in this example possesses several types of sensors:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;LIDAR&lt;/strong&gt;: a time of flight sensing system that operates in the same manner as police using lasers to catch speeding motorists. LIDAR sensors for robots use a rotating laser beam to capture a two-dimensional or three-dimensional depth scan of a surface. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RGB&lt;/strong&gt;: a camera that captures a color (red-green-blue) image from its viewpoint for processing using computer vision algorithms. Computer vision is not a current mainstay of robotic sensing for manipulation, likely because of the significant processing time required, imperfect identification (see Google image search), and susceptibility to sensory noise and artifacts. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kinect&lt;/strong&gt;: a single hardware unit that combines both depth sensing and RGB&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IMU&lt;/strong&gt;: an &lt;em&gt;inertial measurement unit&lt;/em&gt; that combines accelerometers, gyroscopic sensors, magnetometers, and (often) GPS measurements. The accelerometers give linear acceleration. The gyroscopic sensors yield angular acceleration. 
Magnetometers can help determine orientation.&lt;/li&gt;
&lt;/ul&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;A point cloud, as might be produced by LIDAR or the Kinect sensor.&lt;/caption&gt;
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/4/4c/Point_cloud_torus.gif&quot; alt=&quot;A point cloud, as might be produced by LIDAR or the Kinect sensor.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h4 id=&quot;controller&quot;&gt;Controller&lt;/h4&gt;

&lt;p&gt;A &lt;em&gt;controller&lt;/em&gt; is a real-time process that runs at a specified frequency
(commonly between 100 Hz and 10,000 Hz for robots). Controllers attempt to
regulate a dynamical system using a model of the system and/or error
between the desired state of the system and its current state.&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Extremely simple controller (thermostat), actuators (A/C unit and furnace), and sensor (thermometer). The dynamical system is the temperature in the dwelling. The controller operates using a simple _error feedback_ mechanism: a voltage signal is sent to the furnace or A/C unit to increase/decrease temperature until the desired setpoint is reached.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/thermostat.png&quot; alt=&quot;Extremely simple controller (thermostat), actuators (A/C unit and furnace), and sensor (thermometer). The dynamical system is the temperature in the dwelling. The controller operates using a simple _error feedback_ mechanism: a voltage signal is sent to the furnace or A/C unit to increase/decrease temperature until the desired setpoint is reached.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;One of the earliest controllers is the centrifugal governor. A video depiction of the centrifugal governor is shown &lt;a href=&quot;https://www.youtube.com/watch?v=iO0CxOTe4Uk&quot;&gt;here&lt;/a&gt;. An engine causes the governor to rotate, and centrifugal force on the governor- caused by its faster rotation- closes the throttle valve. When the engine slows, the centrifugal force attenuates, and the throttle re-opens. &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;A depiction of a centrifugal governor.&lt;/caption&gt;
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/1/1e/Centrifugal_governor.png&quot; alt=&quot;A depiction of a centrifugal governor.&quot; width=&quot;500&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h4 id=&quot;motor-servos&quot;&gt;Motor servos&lt;/h4&gt;

&lt;p&gt;A motor servo is the electronic interface between the robot software and
the hardware. We can communicate with this interface using low level commands
over the serial port, USB, or CAN bus or using higher level driver software.
The commands we send to the interface may consist of desired current
(for electromagnetic motors), desired torque, or desired position and velocity.
The interface will typically output some data, including joint position and-
sometimes- speed, and torque.&lt;/p&gt;

&lt;h4 id=&quot;planning-modules&quot;&gt;Planning modules&lt;/h4&gt;

&lt;p&gt;Many computational processes require more time to compute than the control 
loop frequency would allow. For example, determining viable foot placements
for a humanoid robot can require on the order of a second. Planning modules
are non-real-time processes that run &amp;quot;in the background&amp;quot;. When a planning
module has completed its computation, it is able to begin feeding inputs
to the controller. &lt;/p&gt;

&lt;h4 id=&quot;perceptual-modules&quot;&gt;Perceptual modules&lt;/h4&gt;

&lt;p&gt;Raw sensory data from robots is generally not usuable without further 
processing. We may wish to remove outlier points from range data, to
&amp;quot;self filter&amp;quot; the robot from RGB or range data, and fuse measurements from
inertial measurement units with other sensors to combat drift.&lt;/p&gt;

&lt;h3 id=&quot;mixing-real-time-reactivity-and-planning&quot;&gt;Mixing real-time reactivity and planning&lt;/h3&gt;

&lt;p&gt;Control strategies for highly dextrous (dynamic) robots, like walking robots,
humanoid robots, and mobile manipulators are still an active area of research. The researchers must usually be focused on basic functionality (standing, walking, opening a door), causing issues like providing near autonomy in dynamic environments (like those populated by humans) to be ignored.  &lt;/p&gt;

&lt;p&gt;Researchers &lt;em&gt;have&lt;/em&gt; investigated &lt;a href=&quot;http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;amp;arnumber=219995&amp;amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D219995&quot;&gt;getting wheeled robots to move rapidly through
such environments&lt;/a&gt;, for example. The strategy allowing such autonomy has been a &lt;a href=&quot;https://en.wikipedia.org/wiki/Three-layer_architecture&quot;&gt;three layer (hybrid reactive/deliberative) architecture&lt;/a&gt;. Consider the problem of having a robot navigate amongst furniture to reach a corner of a room. In a three layer architecture, a planner finds a path through the furniture, a reactive component avoids humans and other unforeseen obstacles, and an arbitration layer decides whether to follow the command from the planner or from the reactive component. &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;The planner generates a sequence of desired state configurations, which the controller transforms into commands.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/plannercontroller.png&quot; alt=&quot;The planner generates a sequence of desired state configurations, which the controller transforms into commands.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;In the case of wheeled robots, stopping movement is usually an option (though moving away from the human might be best). There is not usually such a good default command available for walking robots, however. Failing to give a good enough command to a walking robot may cause a multi-hundred pound machine to fall over, breaking itself or hurting or
killing a human.&lt;/p&gt;

&lt;h4 id=&quot;doesn-39-t-the-passage-of-time-invalidate-plans-consisting-of-long-command-sequences&quot;&gt;Doesn&amp;#39;t the passage of time invalidate plans consisting of long command sequences?&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;(This question is relevant to current questions in AI.)&lt;/em&gt; &lt;/p&gt;

&lt;p&gt;Autonomous robots are expected to execute in dynamic, human populated environments. The right level of abstraction is important: &amp;quot;go to the grocery&amp;quot; is likely a safe action for timespans of years, while &amp;quot;pickup the hundred dollar bill from the sidewalk&amp;quot; may be valid for only seconds. &lt;/p&gt;

&lt;h4 id=&quot;how-can-plans-be-adapted-as-a-robot-deviates-from-its-plan&quot;&gt;How can plans be adapted as a robot deviates from its plan?&lt;/h4&gt;

&lt;p&gt;Robots are nondeterministic systems. A robot may move one degree to the right when we command it to move ten degrees to the left. Deviation from a plan is
nearly guaranteed. Depending on the dynamic characteristics of the system and
the commands sent to the robot, deviation may become magnified or attenuated.
The conditions under which a dynamical system &lt;em&gt;stabilizes&lt;/em&gt; to a equilibrium
point or under which deviations do not tend to grow is studied under the
theory of stability of dynamical systems. &lt;/p&gt;

&lt;h4 id=&quot;what-happens-if-a-plan-does-not-consider-effects-sufficiently-far-into-the-future&quot;&gt;What happens if a plan does not consider effects sufficiently far into the future?&lt;/h4&gt;

&lt;p&gt;An engine can be controlled using a simple error feedback strategy: make
throttle closure proportional to speed above the designated setpoint (the greater the speed above the setpoint, the faster the throttle closure). Many robotic systems- particularly underactuated robots- do not admit such a straightforward control strategy. Russ Tedrake, an MIT roboticist, has an entire &lt;a href=&quot;http://underactuated.csail.mit.edu/underactuated.html&quot;&gt;online course&lt;/a&gt; dedicated to this topic. &lt;/p&gt;

&lt;p&gt;If a control strategy does not consider likely next effects of actions, the
control strategy is a &lt;em&gt;Horizon-One&lt;/em&gt; strategy. If the control stategy considers the (long-term) effect, \(n\) steps into the future, of an action, the
control strategy is a &lt;em&gt;Horizon-\(n\)&lt;/em&gt; strategy (\(n = \infty\) is commonly considered).&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;If Black greedily captures the pawn at F3, White can move the rook from D1 to F1, capturing Black&#39;s knight, and likely winning the game. Like playing Chess, robots in dynamic environments must consider longer term consequences of actions.&lt;/caption&gt;
&lt;img src=&quot;http://www.expert-chess-strategies.com/images/poisoned-pawn.jpg&quot; alt=&quot;If Black greedily captures the pawn at F3, White can move the rook from D1 to F1, capturing Black&#39;s knight, and likely winning the game. Like playing Chess, robots in dynamic environments must consider longer term consequences of actions.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h4 id=&quot;a-lookup-table-for-control&quot;&gt;A lookup table for control&lt;/h4&gt;

&lt;p&gt;Since computing controls fast enough is such a problem, one holy grail would
be a lookup table (also known as a &lt;em&gt;control policy&lt;/em&gt;) for dextrous robot control.A sketch of such a table is below. &lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x\(_1\)&lt;/th&gt;
&lt;th&gt;x\(_2\)&lt;/th&gt;
&lt;th&gt;Action&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;td&gt;Move up&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;td&gt;b&lt;/td&gt;
&lt;td&gt;Move down&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;\(\vdots\)&lt;/td&gt;
&lt;td&gt;\(\vdots\)&lt;/td&gt;
&lt;td&gt;\(\vdots\)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;z&lt;/td&gt;
&lt;td&gt;z&lt;/td&gt;
&lt;td&gt;Climb right&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;For robots, such a lookup table would have to consider- at minimum- each 
joint&amp;#39;s angle and speed; these are the &lt;a href=&quot;../dynamical-systems&quot;&gt;state variables&lt;/a&gt;. For a &amp;quot;simple&amp;quot; humanoid robot, over seventy state variables may be required.
To make a table, we have to discretize these state variables. If we assume
ten (only ten!) gradations per variable, the size of the lookup table will be \(10^{70}\) entries. If we want to explore the effect of taking each action &lt;em&gt;just once&lt;/em&gt;, and our robot could somehow perform and evaluate one million actions per second, \(10^{64}\) seconds would be required, This number far exceeds the \(10^{18}\) seconds believed to be the age of the universe. Even if this could be magically computed quickly enough, tem gradations is at least an order of magnitude too coarse! &lt;/p&gt;

&lt;p&gt;What about function approximation approaches, which have been studied heavily
by machine learning researchers? Function approximation can often
approximate highly nonlinear functions using much smaller numbers of basis
functions. How would we know that an approximation would be good enough?&lt;/p&gt;

&lt;p&gt;Even before we ask that question, we need to know how an action is chosen.
As I established above, greedily choosing the action that appears best at any time is often a poor
choice. Similarly, a legged robot might fall over if a good enough &lt;em&gt;sequence&lt;/em&gt; of commands isn&amp;#39;t found. We wish for the robot to act to optimize an objective function over a series of commands. &lt;/p&gt;

&lt;p&gt;This problem falls under the domain of &lt;em&gt;optimal control&lt;/em&gt;, for which globally
optimal solutions are often intractable to find. Current optimal control and
&lt;a href=&quot;https://en.wikipedia.org/wiki/Model_predictive_control&quot;&gt;model predictive control&lt;/a&gt; approaches often focus on algorithms for robustly finding locally optimal solutions (numerically &amp;quot;brittle&amp;quot; algorithms are presently required). Even if a tractable algorithm for solving optimal control problems were to exist, selecting 
a good objective function would remain a challenging problem. &lt;/p&gt;
</description>
        
          <description>&lt;p&gt;This module will discuss some topics for those new to robot programming. 
I&amp;#39;ll start with programming general real-time systems, talk next about
an architecture for a typical robot system- including &lt;em&gt;planners&lt;/em&gt; and 
&lt;em&gt;controllers&lt;/em&gt;, and conclude with many thoughts on the open problem of 
combining reactive and deliberative behaviors for dextrous robots.&lt;/p&gt;
</description>
        
        <pubDate>Thu, 07 Jan 2016 00:00:00 -0500</pubDate>
        <link>/robotics-course-materials/robotics-course-materials/blog/robot-programming/</link>
        <guid isPermaLink="true">/robotics-course-materials/robotics-course-materials/blog/robot-programming/</guid>
        
        
      </item>
      
    
      
      <item>
        <title>Introduction to dynamical systems and ordinary differential equations</title>
        <description>&lt;p&gt;The &lt;em&gt;equations of motion&lt;/em&gt;- that transform forces applied to a robot into motion- represent a &lt;em&gt;dynamical system&lt;/em&gt;, a mathematical concept. Knowledge of 
dynamical systems allows robots to predict how a thrown ball will move and
to pick footsteps to prevent falling over, among other uses.   &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Dynamical systems predict the motion of objects to which forces have been applied.&lt;/caption&gt;
&lt;img src=&quot;http://i.giphy.com/92kIGbLtN3sZO.gif&quot; alt=&quot;Dynamical systems predict the motion of objects to which forces have been applied.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h2 id=&quot;dynamical-systems&quot;&gt;Dynamical systems&lt;/h2&gt;

&lt;p&gt;Let us start by defining the &lt;em&gt;state&lt;/em&gt; of a dynamical system. The state is
typically denoted with the variable \(\mathbf{x} \in \mathbb{R}^n\) and
may represent position, orientation, temperature, pressure, force, current, etc. 
&lt;em&gt;Time \(t\) and state of the dynamical
system at \(t\)&lt;/em&gt; must be sufficient to predict the state of the dynamical system at some time in the future. For example, let us assume that we know all of the forces acting on the moon Europa, that Europa is well modeled as a point mass, and that we have an accurate estimate of Europa&amp;#39;s mass. Given this information and the current location of Europa, we still cannot predict Europa&amp;#39;s location a year into the future: we also need
Europa&amp;#39;s current velocity. In summary, if we treat Europa as a dynamical system- and planetary objects were the first objects of study in dynamical systems- then its state should consist (at least) of its position and velocity. &lt;/p&gt;

&lt;p&gt;The path that the state takes over time is known as its &lt;em&gt;trajectory&lt;/em&gt; or 
&lt;em&gt;orbit&lt;/em&gt;.&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Predicting planetary motion from an initial observation was one of the first applications of dynamical systems.&lt;/caption&gt;
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/2/28/Copernican_heliocentrism_diagram-2.jpg&quot; alt=&quot;Predicting planetary motion from an initial observation was one of the first applications of dynamical systems.&quot; width=&quot;400&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h3 id=&quot;evolution-of-a-dynamical-system&quot;&gt;Evolution of a dynamical system&lt;/h3&gt;

&lt;p&gt;Another key attribute of a dynamical system is that it should have an update
or transition rule. This rule may be deterministic or stochastic (probabilistic or nondeterministic)- if the former, we can &lt;em&gt;accurately predict the state of
a dynamical system at a specified time into the future given only its state
at the current time&lt;/em&gt;. An interesting example of the latter is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Multi-armed_bandit&quot;&gt;N-armed bandit&lt;/a&gt;. Stochastic dynamical systems are effectively &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_chain&quot;&gt;Markov Chains&lt;/a&gt;/&lt;a href=&quot;https://en.wikipedia.org/wiki/Continuous-time_Markov_chain&quot;&gt;Markov Processes&lt;/a&gt;, for those readers familiar with stochastic processes. The remainder of this learning module will discuss only deterministic
dynamical systems.&lt;/p&gt;

&lt;p&gt;Dynamical systems can also be discrete, meaning that they update discontinuously (the rule is then a &lt;em&gt;transition rule&lt;/em&gt;). Alternatively, the system is continuous, which means that the dynamical system is continuous (and generally smooth). Some systems are &lt;a href=&quot;https://en.wikipedia.org/wiki/Hybrid_system&quot;&gt;really a mix of the two&lt;/a&gt;, but we will not consider these hybrid systems further.&lt;/p&gt;

&lt;p&gt;If the dynamical system is discrete, the transition rule is represented by a &lt;em&gt;difference equation&lt;/em&gt;, shown in an example below:&lt;/p&gt;

&lt;p&gt;\begin{align}
\mathbf{x}_{i+1} = f(\mathbf{x}_i)
\end{align}&lt;/p&gt;

&lt;p&gt;Notice that the next state of \(\mathbf{x}\) is completely a function of the
current state and that the transition is immediate (discontinuous).&lt;/p&gt;

&lt;p&gt;If the dynamical system is continuous, the update rule is represented by a 
(ordinary) &lt;em&gt;differential equation&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;\begin{align}
\dot{\mathbf{x}} = f(\mathbf{x}, t)
\end{align}&lt;/p&gt;

&lt;p&gt;The dot notation \(\dot{\mathbf{x}}\) is a synonym for \(\textrm{d}\mathbf{x}/\textrm{d}t\).&lt;/p&gt;

&lt;p&gt;We will only consider ordinary differential equations (ODEs), as opposed to differential equations that include partial derivatives, known as partial
differential equations (PDEs). The remainder of this learning module will
also consider only differential equations- difference equations will not be
discussed further.&lt;/p&gt;

&lt;h3 id=&quot;digression-mathematical-notation&quot;&gt;Digression: mathematical notation&lt;/h3&gt;

&lt;p&gt;For this learning module and all others, I will denote scalar real numbers using lowercase type (like \(s \in \mathbb{R}\)), vectors in boldface lowercase type (like \(\mathbf{v} \in \mathbb{R}^n\)), matrices in boldface uppercase type (like \(\mathbf{A} \in \mathbb{R}^{n \times n}\)).&lt;/p&gt;

&lt;p&gt;Functions will be typeset in lowercase type, as in \(f(.)\), and will
be typeset in boldface type if they return a vector output, as in \(\mathbf{g} \to \mathbb{R}^m\).&lt;/p&gt;

&lt;h2 id=&quot;a-simple-dynamical-system-the-lorenz-equations&quot;&gt;A simple dynamical system: the Lorenz Equations&lt;/h2&gt;

&lt;p&gt;A well-studied dynamical system was described by Lorenz for modeling climate. 
His dynamical system is a simple &lt;a href=&quot;https://en.wikipedia.org/wiki/Chaos_theory&quot;&gt;chaotic system&lt;/a&gt;, which- like the weather- is highly sensitive to initial conditions. Lorenz&amp;#39;s work spawned the thought that a
butterfly&amp;#39;s wings flapping in Texas might be able to set off a tornado in 
Brazil.&lt;/p&gt;

&lt;p&gt;The Lorenz equations follow:&lt;/p&gt;

&lt;p&gt;\begin{align}
\frac{dx}{dt} &amp;amp; = \sigma(y - x) \\
\frac{dy}{dt} &amp;amp; = x(\rho - z) - y \\
\frac{dz}{dt} &amp;amp; = xy - \beta z 
\end{align}&lt;/p&gt;

&lt;p&gt;where the system state is determined using variables \((x, y, z)\), \(t\) is time, and \(\sigma \textrm{ (sigma)}, \rho \textrm{ (rho)}, \textrm{ and } \beta \textrm{ (beta)}\) are constant parameters.&lt;/p&gt;

&lt;p&gt;The Lorenz equations are plotted below using some unknown initial values and unknown \(t\):&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Depiction of the Lorenz attractor for rho = 28, sigma=10, beta = 8/3.&lt;/caption&gt;
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/1/13/A_Trajectory_Through_Phase_Space_in_a_Lorenz_Attractor.gif&quot; alt=&quot;Depiction of the Lorenz attractor for rho = 28, sigma=10, beta = 8/3.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h3 id=&quot;plotting-the-lorenz-system-using-octave-matlab&quot;&gt;Plotting the Lorenz system using Octave/Matlab&lt;/h3&gt;

&lt;p&gt;I will now show how to use GNU Octave and Matlab to integrate the Lorenz equations
numerically.&lt;/p&gt;

&lt;p&gt;First, we set the initial conditions of x, y, and z in Matlab/Octave to x=1, y=1, z=1. We will label all variables \(\mathbf{x}\) to use Octave/Matlab vector notation.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&amp;gt;&amp;gt; % set the initial conditions
&amp;gt;&amp;gt; x0 = [ 1 1 1 ]&amp;#39;;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;In Matlab&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Create the file &lt;code&gt;lorenz.m&lt;/code&gt; and save it in your current directory.
The contents of this file should be:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;function dxdt = lorenz(t, x)

  % set values for rho, sigma, and beta
  rho = 28;
  sigma = 10;
  beta = 8/3;

  % setup the right hand side of the ordinary differential equation 
  dxdt(1,1) = sigma*(x(2) - x(1));
  dxdt(2,1) = x(1)*(rho - x(3)) - x(2);
  dxdt(3,1) = x(1)*x(2) - beta*x(3);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At the Matlab prompt (&amp;quot;&amp;gt;&amp;gt;&amp;quot;) run the following commands:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;  &amp;gt;&amp;gt; t = 0:1e-2:20;
  &amp;gt;&amp;gt; [tout, x] = ode45(@lorenz, t, x0);
  &amp;gt;&amp;gt; plot3(x(:,1), x(:,2), x(:,3));
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;In GNU/Octave&lt;/strong&gt;, the file &lt;code&gt;lorenz.m&lt;/code&gt; should look almost exactly the same- the arguments &lt;code&gt;t&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt; to the &lt;code&gt;lorenz&lt;/code&gt; function are swapped. The file is otherwise identical:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;function dxdt = lorenz(x, t)

  % set values for rho, sigma, and beta
  rho = 28;
  sigma = 10;
  beta = 8/3;

  % setup the right hand side of the ordinary differential equation 
  dxdt(1,1) = sigma*(x(2) - x(1));
  dxdt(2,1) = x(1)*(rho - x(3)) - x(2);
  dxdt(3,1) = x(1)*x(2) - beta*x(3);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At the Octave prompt (&amp;quot;octave:n&amp;gt;&amp;quot;) run the following commands:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;  octave:1&amp;gt; t = 0:1e-2:20;
  octave:2&amp;gt; x = lsode(@lorenz, x0, t);
  octave:3&amp;gt; plot3(x(:,1), x(:,2), x(:,3));
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;solving-odes&quot;&gt;Solving ODEs&lt;/h2&gt;

&lt;p&gt;&amp;quot;Solving an ordinary differential equation&amp;quot; indicates solving the initial value problem in closed form. 
In other words, given initial conditions, what is the state at some time in the future? As an example, consider one ODE:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\dot{x} = -10x
\end{equation}&lt;/p&gt;

&lt;p&gt;The solution to this ODE is:&lt;/p&gt;

&lt;p&gt;\begin{equation}
x(t) = Ce^{-10t}
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(C\) is an unknown constant. When we are given \(x(0)\), then we
can solve algebraically for \(C\).&lt;/p&gt;

&lt;h4 id=&quot;wait-can-39-t-we-just-use-the-anti-derivative-integrals&quot;&gt;Wait, can&amp;#39;t we just use the anti-derivative? (integrals?)&lt;/h4&gt;

&lt;p&gt;The anti-derivative is usable to solve &lt;em&gt;separable&lt;/em&gt; differential equations,
like:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\frac{\textrm{d}y}{\textrm{d}x} = 6y^2x
\end{equation}&lt;/p&gt;

&lt;p&gt;All \(y\) and \(x\) variables simply need to be on different sides of the
equation to use anti-derivatives:&lt;/p&gt;

&lt;p&gt;\begin{align}
\int \frac{\textrm{d}y}{y^2} &amp;amp; = \int 6x\ \textrm{d}x \\
\frac{-1}{y} + C &amp;amp; = 3x^2 
\end{align}&lt;/p&gt;

&lt;p&gt;Now consider the ordinary differential equation below:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\frac{\textrm{d}y}{\textrm{d}x} + xy = x^2 
\end{equation}&lt;/p&gt;

&lt;p&gt;See whether you can find a way to separate it as above.&lt;/p&gt;

&lt;p&gt;A course on ODEs teaches you to solve initial value problems for 
ordinary differential equations (particularly inseparable ODEs like that above), in closed form. In robotics, our ODEs are
often unknown or too complex to compute closed form solutions for. We 
generally focus on &lt;em&gt;numerical&lt;/em&gt; solutions to ODEs.&lt;/p&gt;

&lt;h3 id=&quot;solving-odes-numerically&quot;&gt;Solving ODEs numerically&lt;/h3&gt;

&lt;p&gt;When initial value problems for ODEs cannot be solved in closed form, they must be solved numerically. This section discusses doing just that. &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Illustration of Euler&#39;s Method for integrating ODE&#39;s numerically. The derivative at the starting point of each interval is extrapolated to find the next function value. Taken from Numerical Recipes in C.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/Euler.png&quot; alt=&quot;Illustration of Euler&#39;s Method for integrating ODE&#39;s numerically. The derivative at the starting point of each interval is extrapolated to find the next function value. Taken from Numerical Recipes in C.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h4 id=&quot;another-dynamical-system-the-pendulum&quot;&gt;Another dynamical system, the pendulum&lt;/h4&gt;

&lt;p&gt;The regular motion of the pendulum has fascinated humans for centuries, and
the pendulum acted as the underlying mechanism of the first accurate clocks. &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Animation of a simple model of a pendulum. The &#39;phase space&#39;- position and velocity variables- of the pendulum is depicted above. Which axis corresponds to which?&lt;/caption&gt;
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/8/80/Pendulum_60deg.gif&quot; alt=&quot;Animation of a simple model of a pendulum. The &#39;phase space&#39;- position and velocity variables- of the pendulum is depicted above. Which axis corresponds to which?&quot; width=&quot;300&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;The ordinary differential equation of motion for the pendulum is:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\ddot{\theta} + g/L \sin \theta = 0
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(g\) is the acceleration due to gravity, \(L\) is the length of the rod, and \(\theta\) is the angular displacement. The pendulum is a classical dynamical system and deriving its ordinary differential equation is a seminal task in courses on ordinary differential equations and classical mechanics.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The pendulum is an essential component of many seminal dynamic tasks
in control theory, including the &lt;a href=&quot;http://www4.ncsu.edu/%7Ersmith/MA731_S09/Astrom_Furuta.pdf&quot;&gt;pendulum swing-up task&lt;/a&gt; and the &lt;a href=&quot;http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;amp;arnumber=6313077&amp;amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6313077&quot;&gt;cart-pole balancing problem&lt;/a&gt;.&lt;/em&gt; &lt;a href=&quot;http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;amp;arnumber=973365&amp;amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D973365&quot;&gt;The cart-pole 
balancing problem has been effectively used as a balancing model for 
stabilizing legged robots.&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;canonical-system-of-first-order-odes&quot;&gt;Canonical system of first order ODEs&lt;/h4&gt;

&lt;p&gt;Most algorithms for numerically integrating ordinary differential equations
require that the ODEs be first order, where the &lt;em&gt;order&lt;/em&gt; of an ODE is the highest derivative in the equation. This problem is readily solved using variable substitution, as seen applied to the second order ODE for the pendulum below: &lt;/p&gt;

&lt;p&gt;\begin{align}
\dot{\theta} &amp;amp; = \vartheta \\
\dot{\vartheta} + g/L \sin \theta &amp;amp; = 0 \label{eqn:varphi}
\end{align}&lt;/p&gt;

&lt;h4 id=&quot;euler-39-s-method-for-integrating-odes&quot;&gt;Euler&amp;#39;s Method for integrating ODEs&lt;/h4&gt;

&lt;p&gt;Euler&amp;#39;s Method for integrating ODEs is best derived using &lt;a href=&quot;https://en.wikipedia.org/wiki/Taylor%27s_theorem&quot;&gt;Taylor&amp;#39;s theorem&lt;/a&gt; for approximating a function in a particular neighborhood using an infinite polynomial sum. However, I&amp;#39;ll cover Euler&amp;#39;s Method using the limit definition of the derivative instead. Recall:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\frac{\textrm{d}x}{dt} = \lim_{\Delta t \to 0} \frac{x(t+\Delta t) - x(t)}{\Delta t}
\end{equation}&lt;/p&gt;

&lt;p&gt;Thus, for sufficiently small \(\Delta t\), we have:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\frac{\textrm{d}x}{\textrm{d}t} = \frac{x(t+\Delta t) - x(t)}{\Delta t}
\end{equation}&lt;/p&gt;

&lt;p&gt;and after just a little algebraic manipulation:&lt;/p&gt;

&lt;p&gt;\begin{equation}
x(t+\Delta t) = x(t) + \Delta t \frac{\textrm{d}x}{\textrm{d}t}
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This formula is the basis for an algorithm&lt;/em&gt;. Use the current value of \(x\) and the current derivative, which is itself a function of the current value of \(x\), to compute the next value of \(x\). For the simplest version of this
algorithm, \(\Delta t\) is constant. &lt;/p&gt;

&lt;h4 id=&quot;integrating-the-pendulum-using-euler-39-s-method&quot;&gt;Integrating the pendulum using Euler&amp;#39;s Method&lt;/h4&gt;

&lt;p&gt;To numerically integrate the pendulum&amp;#39;s equations of motion using initial
conditions \(\theta = \pi/2, \vartheta = \dot{\theta} = 0\), we take
the following steps, &lt;em&gt;valid for both Matlab and GNU/Octave.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Setup the ODE function.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;function dxdt = pendulum(x)

  % setup gravity and pendulum rod length
  g = 9.8;
  L = 1;

  % setup theta and dtheta/dt
  theta = x(1);
  dtheta = x(2);

  % compute the ODE
  dxdt(1) = dtheta;
  dxdt(2) = -g/L * sin(theta);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In contrast to &lt;code&gt;lorenz&lt;/code&gt;, &lt;code&gt;pendulum&lt;/code&gt; does not accept \(t\)
as a formal parameter, because the ODE is not explicitly dependent upon \(t\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Setup the initial conditions at the Matlab/Octave command prompt.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;  &amp;gt;&amp;gt; x(1,:) = [pi/2 0];
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Pick the integration step size. I use all caps in my programming code to denote constants. We&amp;#39;ll want to explore the effect of varying this value. &lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;  &amp;gt;&amp;gt; DT = .01;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: Pick the time interval to integrate over. Because the pendulum is a periodic dynamical system, I want the time interval to be sufficiently large to observe the periodic motion.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;  &amp;gt;&amp;gt; TEND = 10;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Step 5&lt;/strong&gt;: Perform the Euler integration.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;  &amp;gt;&amp;gt; for t=DT:DT:TEND  % integrate for 10 seconds
  i = size(x,1); % get the last state index
  dxdt = pendulum(x(i,:)); % evaluate the ODE using current x 
  x(i+1,:) = x(i,:) + dxdt*DT; % integrate state forward 
  end
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Step 6&lt;/strong&gt;: Plot the joint angle of the pendulum over time.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;  &amp;gt;&amp;gt; plot(0:DT:tend, x(:,1));
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;stability-of-a-dynamical-system&quot;&gt;Stability of a dynamical system&lt;/h2&gt;

&lt;p&gt;If the state of a dynamical system is two dimensional, we can readily
visualize the change in state using arrow depictions. Examine some 
visualizations of dynamical systems below carefully.&lt;/p&gt;

&lt;p&gt;The dynamical system below can be seen to drive the state to \(\mathbf{x}=0,0\). This point is known as a &lt;em&gt;equilibrium point&lt;/em&gt; or a &lt;em&gt;fixed point&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Definition of an equilibrium point / fixed point&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A state \(\mathbf{x} \in \mathbb{R}^n\) is an equilibrium point if \(\mathbf{f(x)} = \mathbf{0}\), where \(\dot{\mathbf{x}} = \mathbf{f}(\mathbf{x}, t)\).&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The dynamical system below is &lt;em&gt;stable&lt;/em&gt; in the region that we are viewing: if the
system enters this region, the state will eventually end up at the equilibrium
point. This concept is important in airplanes. If the pilot puts the plane into a dive
and then removes her hands from the yoke, the plane will return to level
flight after a short time. &lt;em&gt;This concept is important in robotics for the same reason&lt;/em&gt;:
a robot will ideally &amp;quot;go to rest&amp;quot; if no controls are applied to it for some
time. &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;&lt;/caption&gt;
&lt;img src=&quot;http://www.entropy.energy/scholar/node/dynamical-systems-maps/dynamical-map-dampedho.png&quot; alt=&quot;&quot; width=&quot;600&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;The dynamical system below depicts many fixed points (where arrows are
missing). Two regions of such fixed points are stable: the left and upper
right vortices. Equilibrium points in other regions are not stable because,
if sufficiently far from the region, the state can get swept into one of the 
two vortices.&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;&lt;/caption&gt;
&lt;img src=&quot;http://www.lct.jussieu.fr/pagesperso/silvi/elfhtml/dynamic.gif&quot; alt=&quot;&quot; width=&quot;600&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h3 id=&quot;determining-whether-a-dynamical-system-is-stable-around-a-fixed-point&quot;&gt;Determining whether a dynamical system is stable around a fixed point&lt;/h3&gt;

&lt;p&gt;Determining stability in the neighborhood of an equilibrium point is
mostly beyond the scope of this learning module. I will cover a few key concepts, however.&lt;/p&gt;

&lt;h4 id=&quot;stability-type&quot;&gt;Stability type&lt;/h4&gt;

&lt;p&gt;Stability considers the neighborhood of states around an equilibrium point. &lt;/p&gt;

&lt;p&gt;The following types of stability are listed in decreasing order from the
strongest sense of stability to the weakest. &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Exponential stability&lt;/strong&gt;: neighboring states converge exponentially fast to the fixed point&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Asymptotic stability&lt;/strong&gt;: neighboring states eventually converge to the fixed point&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lyapunov stability&lt;/strong&gt;: neighboring states stay within a small neighborhood &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;methods-for-determining-stability&quot;&gt;Methods for determining stability&lt;/h4&gt;

&lt;p&gt;Without going too far into stability analysis of dynamical systems, I will
quickly describe two methods for analyzing stability.&lt;/p&gt;

&lt;h5 id=&quot;linearization&quot;&gt;Linearization&lt;/h5&gt;

&lt;p&gt;A linear dynamical system can be expressed in the form:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\dot{\mathbf{x}} = \mathbf{A}\mathbf{x}
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(\mathbf{A}\) is a matrix. If the dynamical system is linear or
it can effectively be approximated by a linear dynamical system in the 
neighborhood of an equilibrium point, we can analyze the eigenvalues of \(\mathbf{A}\). Unless \(\mathbf{A}\) is a symmetric matrix, it will have 
both real and imaginary eigenvalues.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If the real components of all eigenvalues are negative, the linear system is asymptotically stable&lt;/li&gt;
&lt;li&gt;If all real components of all eigenvalues are non-positive, the linear system is marginally stable&lt;/li&gt;
&lt;li&gt;If there exists a positive real eigenvalue, the linear system is unstable around the equilibrium point&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Within the realm of stable systems, a system is:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Underdamped&lt;/strong&gt; if the system oscillates about equilibrium with oscillations gradually decreasing in amplitude until finally reaching zero. A system is underdamped if the eigenvalues have complex components (but the real components are necessarily non-positive).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overdamped&lt;/strong&gt; if the system exponentially decays to equilibrium without oscillating. A system is overdamped if the eigenvalues are real and unequal (and are negative). &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Critically damped&lt;/strong&gt; if the system returns to equilibrium as quickly as possible without oscillating. A system is underdamped if all eigenvalues are real and equal (and negative).&lt;/li&gt;
&lt;/ul&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Depiction of overdamping, critical damping, and underdamping for a mass-spring-damper system.&lt;/caption&gt;
&lt;img src=&quot;https://www.softintegration.com/docs/ch/qanimate/examples/vibration/vibration_large.gif&quot; alt=&quot;Depiction of overdamping, critical damping, and underdamping for a mass-spring-damper system.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;The table also depicts most of these conditions.&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Plots of eigenvalues for a linear system.&lt;/caption&gt;
&lt;img src=&quot;https://controls.engin.umich.edu/wiki/images/3/30/Eigenvalue_graphs.jpg&quot; alt=&quot;Plots of eigenvalues for a linear system.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;&lt;strong&gt;An example:&lt;/strong&gt; Consider the pendulum that we integrated above. Let us
first look for equilibrium points. Equilibrium points for the pendulum exist where \(\dot{\theta} = 0, \dot{\varphi} = 0\). \(\dot{\theta} = 0\) when the velocity of the pendulum is zero,
so that part is easy. We next solve Equation \ref{eqn:varphi} for \(\dot{\varphi} = 0\):
\begin{align}
\dot{\varphi} &amp;amp; = -g/L \sin \theta \\
0 &amp;amp; = -g/L \sin \theta
\end{align}
\(\sin \theta\) is zero for \(\theta = \{ 0, \pi \}\). Therefore, the
pendulum has two equilibrium points: \({\theta = 0, \dot{\theta} = 0}\) and \({\theta=\pi, \dot{\theta} = 0} \).&lt;/p&gt;

&lt;p&gt;The pendulum is nonlinear
because of the presence of the \(\sin\) term, which means that it seems that we can&amp;#39;t express it in the form \(\dot{\mathbf{x}} = \mathbf{Ax}\). However,
if we notice that \(\sin \theta \approx \theta\) for \(\theta \approx 0\) and \(\sin \theta \approx \pi - \theta\) for \(\theta \approx \pi\), then
we can get a &lt;em&gt;linearization&lt;/em&gt; for \(\dot{\varphi}\):
\begin{equation}
\dot{\varphi} \approx \begin{cases} -g/L\ \theta &amp;amp; \textrm{ if } \theta \approx 0 \\
-g/L\ (\pi - \theta) &amp;amp; \textrm{ if } \theta \approx \pi \end{cases}
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What is \(\mathbf{A}\) in this case?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We need to be able to put the equations in the form:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{bmatrix}
\dot{\theta} \\
\dot{\varphi}
\end{bmatrix} =
\mathbf{A}
\begin{bmatrix}
\theta \\
\varphi
\end{bmatrix}
\end{equation} &lt;/p&gt;

&lt;p&gt;This means \(\mathbf{A}\) can take two forms, depending on which equilibrium point we are using:&lt;/p&gt;

&lt;p&gt;\begin{align}
\mathbf{A}_0 &amp;amp; = \begin{bmatrix} 
0 &amp;amp; 1 \\
-g/L &amp;amp; 0
\end{bmatrix}\\
\mathbf{A}_\pi &amp;amp; = \begin{bmatrix}
0 &amp;amp; 1 \\
g/L &amp;amp; 0
\end{bmatrix} 
\end{align}&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note that to get \(\mathbf{A}_\pi\) we must make the variable substitution: \(\theta^* = \theta + \pi\) to put the equations in the form \(\dot{\mathbf{ x}} = \mathbf{Ax}\). This substitution need not be considered further in
the stability analysis.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Examining the eigenvalues:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the case of \(\mathbf{A}_0\), positive \(g\) and \(L\) yields
eigenvalues of \(0 \pm xi \), where \(x\) is an imaginary scalar. In the 
case of
\(\mathbf{A}\_\pi\), positive \(g\) and \(L\) yields eigenvalues of
\(\pm y\), where \(y\) is a real scalar.&lt;/p&gt;

&lt;p&gt;We can see this using Matlab or Octave:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&amp;gt;&amp;gt; g = 9.8;
&amp;gt;&amp;gt; L = 1.0;
&amp;gt;&amp;gt; A = [0 1; -g/L 0]; % first equilibrium point
&amp;gt;&amp;gt; eig(A)

ans =
       0 + 3.1305i
       0 - 3.1305i

&amp;gt;&amp;gt; A = [0 1; g/L 0];  % second equilibrium point
&amp;gt;&amp;gt; eig(A)

ans =

       3.1305
      -3.1305
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Deeper questions&lt;/em&gt;: what do the eigenvalues for \(\mathbf{A}_0\) and 
\(\mathbf{A}_\pi\) indicate about stability? Now think about the configuration of the pendulum at the two equilibrium points. Does the stability analysis 
make sense when examining these two configurations?&lt;/p&gt;

&lt;h5 id=&quot;lyapunov-functions&quot;&gt;Lyapunov functions&lt;/h5&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Lyapunov_function&quot;&gt;Lyapunov functions&lt;/a&gt; are the
go to approach for proving stability of &lt;em&gt;nonlinear&lt;/em&gt; dynamical systems.&lt;/p&gt;

&lt;p&gt;Wikipedia&amp;#39;s informal description of Lyapunov functions is accessible: &amp;quot;a Lyapunov function is a function that takes positive values everywhere except at any stasis in question, and decreases (or is non-increasing) along every trajectory of the ODE&amp;quot;. If a Lyapunov
function can be found for an equilibrium point of a dynamical system, the
system is Lyapunov stable in the neighborhood of that point. Finding a
Lyapunov function for an arbitrary ODE is currently an open problem. &lt;/p&gt;
</description>
        
          <description>&lt;p&gt;The &lt;em&gt;equations of motion&lt;/em&gt;- that transform forces applied to a robot into motion- represent a &lt;em&gt;dynamical system&lt;/em&gt;, a mathematical concept. Knowledge of 
dynamical systems allows robots to predict how a thrown ball will move and
to pick footsteps to prevent falling over, among other uses.   &lt;/p&gt;
</description>
        
        <pubDate>Thu, 07 Jan 2016 00:00:00 -0500</pubDate>
        <link>/robotics-course-materials/robotics-course-materials/blog/dynamical-systems/</link>
        <guid isPermaLink="true">/robotics-course-materials/robotics-course-materials/blog/dynamical-systems/</guid>
        
        
      </item>
      
    
      
      <item>
        <title>Rigid body poses in 2D</title>
        <description>&lt;p class=&quot;intro&quot;&gt;
The rigid body assumption is a common simplification used to easily and quickly (with respect to computation) model rigid or even semi-rigid bodies. 
&lt;/p&gt;

&lt;h2 id=&quot;rigid-body-modeling&quot;&gt;Rigid body modeling&lt;/h2&gt;

&lt;p&gt;The rigid body assumption requires that, for any two points \(\mathbf{p}_1\) and \(\mathbf{p}_2\) on a rigid body, the distance between the two points remains constant for all time:&lt;/p&gt;

&lt;p&gt;\begin{equation}
||p_1 - p_2|| = c
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;kinematics&quot;&gt;Kinematics&lt;/h3&gt;

&lt;p&gt;Kinematics is the study of the position, velocity, acceleration, and further derivatives (including the third and fourth derivatives of position, jerk and snap, respectively) of bodies, which can include particles, rigid bodies, non-rigid bodies, fluids, etc. &lt;/p&gt;

&lt;p&gt;We start with kinematics in two dimensions, which is easier to visualize and conceptualize than 3D. Assume that we are looking at 2D kinematics on the \(z = 0\) plane in 3D. When we look at angular motion in 2D, the axis of rotation will be the z-axis.&lt;/p&gt;

&lt;h3 id=&quot;kinematics-of-a-particle&quot;&gt;Kinematics of a particle&lt;/h3&gt;

&lt;p&gt;Presumably you already have knowledge of the very simple kinematics of a particle, which is a body that can be reasonably modeled as a point. We note the 
location of that particle with respect to some frame of reference as \(\mathbf{x}(t)\), where \(\mathbf{x}\) is a two-dimensional vector. The first time-derivative of position is the linear velocity and is written \(\dot{\mathbf{x}}(t)\). The second time-derivative of position is the linear acceleration and is written  \(\ddot{\mathbf{x}}(t)\).&lt;/p&gt;

&lt;h4 id=&quot;frame-of-reference-translation&quot;&gt;Frame of reference (translation)&lt;/h4&gt;

&lt;p&gt;Let’s immediately get into a seemingly cumbersome, but ultimately very useful habit of marking all relevant quantities with their reference frame. For example, assume that there does exist, somewhere in the universe, an absolute reference frame, which we’ll denote \(w\). We can then mark our
position vector and its derivatives as:&lt;/p&gt;

&lt;p&gt;\begin{equation} 
\mathbf{q}_w(t), \dot{\mathbf{q}}_w(t), \ddot{\mathbf{q}}_w(t)
\end{equation} &lt;/p&gt;

&lt;p&gt;which means that the position (and velocity and acceleration) of a point in
space is observed from \(w\). Now assume another reference frame, \(p\),
which has the same orientation as \(w\), but is located in a different place &lt;em&gt;that is a constant offset from \(\mathbf{q}_w\)&lt;/em&gt;. We can then define \(\mathbf{q}_p = \mathbf{q}_w + \mathbf{v}_{\overrightarrow{qp}}\). Notice that velocity and acceleration do not change- \(\dot{\mathbf{q}}_p = \dot{\mathbf{q}}_w\) and \(\ddot{\mathbf{q}}_p = \ddot{\mathbf{q}}_w\)- because the vector from \(\mathbf{q}_w\) to frame \(p\) is constant.&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Depiction of two frames (w and p) and a particle (q). There is a constant offset between the particle and frame p. Origins of frames are labeled with a capital &#39;O&#39;. &lt;/caption&gt;
&lt;img src=&quot;../../assets/img/translated-frame.png&quot; alt=&quot;Depiction of two frames (w and p) and a particle (q). There is a constant offset between the particle and frame p. Origins of frames are labeled with a capital &#39;O&#39;. &quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h4 id=&quot;frame-of-reference-orientation&quot;&gt;Frame of reference (orientation)&lt;/h4&gt;

&lt;p&gt;Let&amp;#39;s now assume a new frame of reference, which we denote \(s\), and which is collocated with \(w\) but is rotated. If we assume that the first axis of \(w\) is named \(\mathbf{a}_x\) and the second axis is named \(\mathbf{a}_y\), we can similarly denote the first and second axes of \(s\) as \(\mathbf{a}&amp;#39;_x\) and \(\mathbf{a}&amp;#39;_y\). Since we are considering only two dimensions, each vector is two-dimensional. We can represent \(\mathbf{a}_x\) using the vector \(\begin{bmatrix}1 &amp;amp; 0\end{bmatrix}^T\) and \(\mathbf{a}_y\) using the vector \(\begin{bmatrix}0 &amp;amp; 1\end{bmatrix}^T\). We can transform vectors between the two reference frames using a matrix:&lt;/p&gt;

&lt;p&gt;\begin{equation} 
\ _{w}\mathbf{R}_{s} \triangleq \begin{bmatrix}\mathbf{a}&amp;#39;_{x_1} &amp;amp; \mathbf{a}&amp;#39;_{y_1} \\ \mathbf{a}&amp;#39;_{x_2} &amp;amp; \mathbf{a}&amp;#39;_{y_2} \end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;For example, try \(\mathbf{a}&amp;#39;_x \triangleq \begin{bmatrix}0 &amp;amp; 1\end{bmatrix}^T\) and \(\mathbf{a}&amp;#39;_y \triangleq \begin{bmatrix}-1 &amp;amp; 0\end{bmatrix}^T\). This corresponds to the matrix: &lt;/p&gt;

&lt;p&gt;\begin{equation}\ _{w}\mathbf{R}_{s} \triangleq \begin{bmatrix}0 &amp;amp; -1 \\ 1 &amp;amp; 0\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;These vectors should both be unit vectors and should be orthogonal, which makes this matrix an &lt;em&gt;orthogonal&lt;/em&gt; matrix, giving it a determinant of \(\pm 1\). To keep everything consistent for our work, we henceforth require that the determinant always be equal to \(+1\).&lt;/p&gt;

&lt;p&gt;We can set \(\ _{w}\mathbf{R}_{s}\) more simply using an angular rotation by \(\theta\) in the plane:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\ _{w}\mathbf{R}_{s} = \begin{bmatrix} \cos{\theta} &amp;amp; -\sin{\theta} \\ \sin{\theta} &amp;amp; \cos{\theta} \end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Given \(\theta = \frac{\pi}{2}\), \(\ _{w}\mathbf{R}_{s}\) will be the matrix:
\begin{equation*}
\begin{bmatrix}
0 &amp;amp; -1 \\
1 &amp;amp; 0
\end{bmatrix}
\end{equation*}&lt;/p&gt;

&lt;p&gt;The two frames \(s\) and \(w\) are depicted in the figure below. &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Depiction of two frames with collocated origins but different orientations.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/rotation-only.png&quot; alt=&quot;Depiction of two frames with collocated origins but different orientations.&quot; width=&quot;400px&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;&lt;em&gt;You can make sense of the rotation matrix without knowing \(\theta\).&lt;/em&gt; The first column of the matrix determines the direction of \(\mathbf{x}_s\) in \(w\)&amp;#39;s frame. The second column of the matrix determines the direction of \(\mathbf{y}_y\) in \(w\)&amp;#39;s frame. &lt;strong&gt;Make sure you understand this point before moving on.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We now look at how we can transform position and its time derivatives between the reference frames. For our position vector in frame \(w\), we have:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{x}_{w} = \ _{w}\mathbf{R}_{s}\ \mathbf{x}_{s}
\end{equation}&lt;/p&gt;

&lt;p&gt;Here is where the subscripts for frame of reference start to come in handy. \(\ _{w}\mathbf{R}_{s}\) transforms from frame \(s\) to \(w\) (the convention is that the &amp;quot;source&amp;quot; frame is on the right hand side and the &amp;quot;target&amp;quot; frame is on the left). You can &lt;strong&gt;and should&lt;/strong&gt; verify that all of the subscripts match up when moving from left to right. &lt;/p&gt;

&lt;p&gt;Also, note that, due to the determinant of \(\ _{w}\mathbf{R}_{s}\) always being non-zero, the inverse of this matrix always exists. Indeed, we can get the transformation from \(w\) to \(s\) by inverting \(\ _{w}\mathbf{R}_{s}\): \({\ _{w}\mathbf{R}_{s}}^{-1} = \ _{s}\mathbf{R}_{w}\).&lt;/p&gt;

&lt;h4 id=&quot;translated-and-oriented-frame-of-reference&quot;&gt;Translated and oriented frame of reference&lt;/h4&gt;

&lt;p&gt;Combining translation and orientation for a frame of reference is straightforward using the individual elements of the previous two sections. We&amp;#39;ll denote our translated and oriented frame as \(s\), and note that it is oriented with respect to \(w\) (which we can specify using the matrix \(_w\mathbf{R}_s\) and vector \(\overrightarrow{\mathbf{w}_o\mathbf{s}_o}\)- the subscript \(\ _o\) indicates the origin of a frame). We can then define a position vector in that frame as \(\mathbf{x}_s =\ _s\mathbf{R}_w \mathbf{x}_w + \overrightarrow{\mathbf{w}_o\mathbf{s}_o}\), using the position vector \(\mathbf{x}_w\) defined in the global frame. &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Depiction of two frames with offset origins and different orientations.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/rotation+translation.png&quot; alt=&quot;Depiction of two frames with offset origins and different orientations.&quot; width=&quot;400px&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h3 id=&quot;concatenated-transformations-and-homogeneous-transformation-matrices&quot;&gt;Concatenated transformations and homogeneous transformation matrices&lt;/h3&gt;

&lt;p&gt;For articulated characters, animals, robots, etc., it is convenient to
compute the position and orientation of each link as a sequence of operations. For example, in the diagram of a robot below, the second arm cover will move
when the first arm cover does. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s-media-cache-ak0.pinimg.com/736x/a4/63/9d/a4639d499d14aaedc7b6ce3637c1c301.jpg&quot; alt=&quot;Diagram of a robot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Similarly, for the double link pendulum below, the endpoint (\(m_2\)) can
be found by- starting from the black circle- rotating by \(\theta_1\), then translating by \(l_1\), then rotating by \(\theta_2\), then finally translating by \(l_2\).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://scienceworld.wolfram.com/physics/dimg270.gif&quot; alt=&quot;Depiction of a double pendulum&quot;&gt;&lt;/p&gt;

&lt;p&gt;Performing this sequence of transformations is painful if we have to treat
rotation and translation separately, as in:
$$
\mathbf{p} = \mathbf{r} + \mathbf{R}\mathbf{u}
$$
where \(\mathbf{r}\) is the vector describing translation and \(\mathbf{R}\) is a rotation matrix. If we combine two such transformations, we arrive at:
$$
\mathbf{p}&amp;#39; = \mathbf{r}&amp;#39; + \mathbf{R}&amp;#39;(\mathbf{r} + \mathbf{R}\mathbf{u})
$$
and three transformations appears even more cumbersome:
$$
\mathbf{p}&amp;#39;&amp;#39; = \mathbf{r}&amp;#39;&amp;#39; + \mathbf{R}&amp;#39;&amp;#39;(\mathbf{r}&amp;#39; + \mathbf{R}&amp;#39;(\mathbf{r} + \mathbf{R}\mathbf{u}))
$$&lt;/p&gt;

&lt;p&gt;The problem is that rotation is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_map&quot;&gt;linear transformation&lt;/a&gt; while translation is not. However, we can turn translation into a linear transformation, which allows us to replace the cumbersome series of operations above with simply matrix multiplications, resulting in an equation that looks like this:
\begin{equation}
\mathbf{p}&amp;#39;&amp;#39; = \mathbf{T}&amp;#39;&amp;#39; \cdot \mathbf{T}&amp;#39; \cdot \mathbf{T} \cdot \mathbf{p}
\label{eqn:sequence}
\end{equation}
This is easier to look at, use, and debug than the previous sequence of operations.&lt;/p&gt;

&lt;h4 id=&quot;homogeneous-coordinates&quot;&gt;Homogeneous coordinates&lt;/h4&gt;

&lt;p&gt;The key is to use &lt;a href=&quot;https://en.wikipedia.org/wiki/Homogeneous_coordinates&quot;&gt;homogeneous coordinates&lt;/a&gt;. For example, the Cartesian coordinates:
$$
\begin{bmatrix}
x \\
y 
\end{bmatrix}
$$
become
$$
\begin{bmatrix}
x \\
y \\
1
\end{bmatrix}
$$&lt;/p&gt;

&lt;p&gt;The form of the \(\mathbf{T}\) matrices above is now \(3 \times 3\) and represents a two-dimensional rotation about \(\theta\) &lt;em&gt;followed by&lt;/em&gt; a translation by \((t_x,t_y)\):
\begin{equation}
\begin{bmatrix}
\cos{\theta} &amp;amp; -\sin{\theta} &amp;amp; t_x \\
\sin{\theta} &amp;amp; \cos{\theta} &amp;amp; t_y \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
\label{eqn:homogeneous}
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exercise: Apply the homogeneous transformation \(\theta = \pi, t_x = 1, t_y = 1\) to \(\mathbf{p} = \begin{bmatrix} 1 &amp;amp; 1 \end{bmatrix}^{\mathsf{T}}\). Is the result what you expected?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Another way to write this matrix is in &lt;a href=&quot;https://en.wikipedia.org/wiki/Block_matrix&quot;&gt;block form&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;\begin{align}
\mathbf{T} \equiv
\begin{bmatrix}
\mathbf{R} &amp;amp; \mathbf{t} \\
\mathbf{0} &amp;amp; 1
\end{bmatrix}
\label{eqn:homogeneous2}
\end{align}&lt;/p&gt;

&lt;p&gt;where \(\mathbf{R}\) is the \(2 \times 2\) rotation matrix and \(\mathbf{t}\) is the \(2 \times 1\) translation vector.&lt;/p&gt;

&lt;h4 id=&quot;inverting-transformations&quot;&gt;Inverting transformations&lt;/h4&gt;

&lt;p&gt;One of the great things about a rotation matrix is that its orthogonality makes it &lt;a href=&quot;../linear-algebra&quot;&gt;easy and fast to invert&lt;/a&gt;. Unfortunately, a homogeneous transformation matrix is not orthogonal. However, we can still invert the matrix easily using the following formula:
\begin{equation}
\mathbf{T}^{-1} = \begin{bmatrix}
\mathbf{R}^\mathsf{T} &amp;amp; -\mathbf{R}^\mathsf{T}\mathbf{t} \\
\mathbf{0} &amp;amp; 1
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;transforming-points-vs-transforming-vectors&quot;&gt;Transforming points vs. transforming vectors&lt;/h4&gt;

&lt;p&gt;When we transform points (as might compose vertices in &lt;a href=&quot;https://en.wikipedia.org/wiki/Polytope&quot;&gt;polytopic shapes&lt;/a&gt;), the Euclidean norm of those points may
change. When we transform vectors (as might describe the angular velocity
of a body), &lt;em&gt;we wish to preserve lengths&lt;/em&gt;. Consider the figure above where
\(\theta = \frac{\pi}{2}\). The homogeneous transformation matrix corresponding to this figure is:
\begin{equation*}
\ _w\mathbf{T}_s \equiv 
\begin{bmatrix}
0 &amp;amp; -1 &amp;amp; 1 \\
1 &amp;amp; 0 &amp;amp; 1 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
\end{equation*}&lt;/p&gt;

&lt;p&gt;If we define the vector \(\mathbf{v}_s \equiv \begin{bmatrix} 1 &amp;amp; 1 &amp;amp; 1 \end{bmatrix}^{\mathsf{T}}\), then:
\begin{equation*}
\ _w\mathbf{T}_s \mathbf{v}_s = \mathbf{v}_w = \begin{bmatrix} 0 \\ 2 \\ 1 \end{bmatrix}
\end{equation*}
and we notice that the lengths of the two vectors are not equal (ignoring the homogeneous components of course): \(||\mathbf{v}_s|| = \sqrt{2} \neq 2 = ||\mathbf{v}_w||\). If we want to preserve magnitudes, we only apply the rotational part of the matrix:
\begin{equation*}
\ _w\mathbf{R}_s \equiv
\begin{bmatrix}
0 &amp;amp; -1 \\
1 &amp;amp; 1
\end{bmatrix}
\end{equation*}
Then \(||\ _w\mathbf{R}_s \cdot \begin{bmatrix} 1 &amp;amp; 1 \end{bmatrix}^{\mathsf{T}}|| = ||\begin{bmatrix} 1 &amp;amp; 1 \end{bmatrix}^{\mathsf{T}}||\).&lt;/p&gt;

&lt;h4 id=&quot;concatenating-transformations&quot;&gt;Concatenating transformations&lt;/h4&gt;

&lt;p&gt;A sequence of transformations, as might be used for determining the poses
of the links of a robot arm, is applied from right to left: in Equation \ref{eqn:sequence}, \(\mathbf{T}\) is applied first, then \(\mathbf{T}&amp;#39;\), and
finally \(\mathbf{T}&amp;#39;&amp;#39;\). Difficulty remembering the ordering- and the ordering changes if moving from the form of the homogeneous transformation in Equation \ref{eqn:homogeneous} to an OpenGL-style homogeneous transformation!- is one
reason we mark the frame of reference for transformations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; This example shows how we can determine the location of any point on a multi-rigid body or the pose defined with respect to a link on a multi-rigid body using a sequence of transformations. See the depiction of a pendulum below. &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Depiction of frames and variables used to describe the kinematics of a single pendulum.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/pendulum-frames.png&quot; alt=&quot;Depiction of frames and variables used to describe the kinematics of a single pendulum.&quot; width=&quot;400px&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;The transformation from Frame 1 (defined at the endpoint of the pendulum) to Frame 0 (the world frame) is determined like so:&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\ _0\mathbf{T}_1 = \ _0\mathbf{T}_{0&amp;#39;} \cdot \ _{0&amp;#39;}\mathbf{T}_1
\end{equation*}&lt;/p&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;p&gt;\begin{equation*}
_0\mathbf{T}_{0&amp;#39;} \equiv
\begin{bmatrix}
\cos{\theta} &amp;amp; -\sin{\theta} &amp;amp; 0 \\
\sin{\theta} &amp;amp; \cos{\theta} &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
\end{equation*}
and
\begin{equation*}
_{0&amp;#39;}\mathbf{T}_{1} \equiv
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; -\ell \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
\end{equation*}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Make sure you understand why the \(\ell\) in \(\ _{0&amp;#39;}\mathbf{T}_1\)
is translated along -y&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Given the relationship \(\ _0\mathbf{T}_1\), we can transform points from Frame 1 to Frame 0. For instance, if I want to know the position of the pendulum bob in the world frame, I can do this:&lt;/p&gt;

&lt;p&gt;\begin{align}
\mathbf{p}_1 &amp;amp; \equiv \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} \\
\mathbf{p}_0 &amp;amp; = \ _0\mathbf{T}_1 \cdot \mathbf{p}_1
\end{align}&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You should try this yourself using simple values for \(\theta\) and \(\ell\)&lt;/em&gt; (like \(pi\) and \(1\), respectively). You should also practice
converting from points in Frame 0 back to Frame 1.&lt;/p&gt;

&lt;h3 id=&quot;rigid-body-kinematics&quot;&gt;Rigid body kinematics&lt;/h3&gt;

&lt;p&gt;Modeling particles doesn&amp;#39;t require consideration of orientation because particles are infinitesimally small. When bodies have significant dimensions, they can not be modeled as particles and we must move along the spectrum of modeling complexity. If the body can effectively be modeled as rigid, we can completely specify the body&amp;#39;s configuration using only a reference point on the body (often we use the center-of-mass) and the body&amp;#39;s orientation. As we&amp;#39;re still working in only two-dimensions, that means each body&amp;#39;s configuration can be specified using some number of (minimally three) real numbers, termed the &lt;em&gt;generalized coordinates&lt;/em&gt;. The time-derivatives of the generalized coordinates are termed the &lt;em&gt;generalized velocities&lt;/em&gt;. Generalized coordinates and velocities are always defined with respect to a frame of reference. &lt;/p&gt;

&lt;h4 id=&quot;generalized-coordinates-in-the-global-reference-frame&quot;&gt;Generalized coordinates in the global reference frame&lt;/h4&gt;

&lt;p&gt;Let&amp;#39;s first look at the generalized coordinates for a body defined in the global reference frame, \(w\):&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{q}_w = \begin{bmatrix} \bar{x}_w \\ \bar{y}_w \\ \theta_w \end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;where \({ \bar{x}_w, \bar{y}_w }\) is the position of the center-of-mass in frame \(w\). &lt;/p&gt;

&lt;h4 id=&quot;generalized-velocities-in-the-global-reference-frame&quot;&gt;Generalized velocities in the global reference frame&lt;/h4&gt;

&lt;p&gt;Let&amp;#39;s now compute the time-derivatives:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\dot{\mathbf{q}}_w = \begin{bmatrix} \dot{\bar{x}}_w \\ \dot{\bar{y}}_w \\ \dot{\theta}_w \end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;This is just the velocity of the body at its center-of-mass (the &lt;em&gt;linear velocity&lt;/em&gt;) and its rate of angular rotation (the &lt;em&gt;angular velocity&lt;/em&gt;). &lt;/p&gt;

&lt;h3 id=&quot;velocity-of-a-point-on-a-rigid-body&quot;&gt;Velocity of a point on a rigid body&lt;/h3&gt;

&lt;p&gt;How do the kinematic equations change if the generalized coordinates are defined with respect to a point on the rigid body, rather than with respect to the body&amp;#39;s center-of-mass? First of all, we have to define our point such that we can always identify it! Let us call this point of interest \(\mathbf{p}\) and define it in Frame \(w\) as follows:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{p}_w = \begin{bmatrix}\bar{x}_w \\ \bar{y}_w \end{bmatrix} + \ _w\mathbf{R}_i(\theta_w)\cdot \mathbf{r}_i
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(\ _w\mathbf{R}_i\) is the matrix encoding the orientation of the body and \(\mathbf{r}_i\) is a vector from the center-of-mass to the point on the body &lt;em&gt;in the body frame&lt;/em&gt;, which we denote as \(i\): since this vector is defined in the body frame, \(\mathbf{r}\) does not change as the body moves or rotates.  &lt;/p&gt;

&lt;p&gt;We can then derive \(\dot{\mathbf{p}}_w\):
\begin{equation}
\dot{\mathbf{p}}_w = \begin{bmatrix}\dot{\bar{x}}_w \\ \dot{\bar{y}}_w \end{bmatrix} + \ _w\dot{\mathbf{R}}_i(\theta_w, \dot{\theta}_w)\cdot \mathbf{r}_i
\end{equation}
where
\begin{equation}
\ _w\dot{\mathbf{R}}_i \equiv
\dot{\theta}_w 
\begin{bmatrix}
\cos{(\theta_w + \frac{\pi}{2})} &amp;amp; -\sin{(\theta_w + \frac{\pi}{2})} \\
\sin{(\theta_w + \frac{\pi}{2})} &amp;amp; \cos{(\theta_w + \frac{\pi}{2})}
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Problem:&lt;/em&gt; Derive this identity for the time derivative of a rotation matrix
using derivative calculus and trigonometric identities. &lt;/p&gt;
</description>
        
          <description>&lt;p class=&quot;intro&quot;&gt;
The rigid body assumption is a common simplification used to easily and quickly (with respect to computation) model rigid or even semi-rigid bodies. 
&lt;/p&gt;
</description>
        
        <pubDate>Sun, 20 Dec 2015 00:00:00 -0500</pubDate>
        <link>/robotics-course-materials/robotics-course-materials/blog/poses/</link>
        <guid isPermaLink="true">/robotics-course-materials/robotics-course-materials/blog/poses/</guid>
        
        
      </item>
      
    
      
      <item>
        <title>Rigid body poses in 3D</title>
        <description>&lt;p&gt;&lt;p class=&quot;intro&quot;&gt;&lt;/p&gt;

&lt;p&gt;The primary difficulty when beginning to consider three-dimensional kinematics 
is due to the significant difference between two-dimensional and three-dimensional orientations. In two-dimensions, orientation can be represented using a single scalar value. In three-dimensions, a plethora of three-dimensional orientations are conceivable, and at least three values are necessary to represent all 3D rotations. We will not consider translation in three dimensions: it is treated exactly as in 2D (with an additional component to represent location along the \(z\) axis, of course).&lt;/p&gt;

&lt;p&gt;We will again use the convention that frame \(w\) indicates a &amp;quot;world frame&amp;quot; and frame \(s\) indicates an alternative frame of reference.&lt;/p&gt;

&lt;h3 id=&quot;representations-of-3d-orientation&quot;&gt;Representations of 3D orientation&lt;/h3&gt;

&lt;h4 id=&quot;rotation-matrices&quot;&gt;Rotation matrices&lt;/h4&gt;

&lt;p&gt;We can describe the orientation of one reference frame in 3D with respect to 
another using \(3\times 3\) matrices analogously to how we define relative
orientations in 2D using \(2\times 2\) matrices. &lt;/p&gt;

&lt;p&gt;Again consider our alternative frame of reference \(s\), which is collocated with \(w\) but is oriented differently. Assume that the axes of \(w\) are named \(\mathbf{a}_x\), \(\mathbf{a}_y\), and \(\mathbf{a}_z\) and that the respective axes of \(s\) are denoted \(\mathbf{a}&amp;#39;_x, \mathbf{a}&amp;#39;_y\), and \(\mathbf{a}&amp;#39;_z\). All of these vectors are three-dimensional. One choice of the set of vectors \(\mathbf{a}_x, \mathbf{a}_y\), and \(\mathbf{a}_z\) are the vectors \(\begin{bmatrix}1 &amp;amp; 0 &amp;amp; 0\end{bmatrix}^{\mathsf{T}}\), \(\begin{bmatrix}0 &amp;amp; 1 &amp;amp; 0\end{bmatrix}^{\mathsf{T}}\), and \(\begin{bmatrix}0 &amp;amp; 0 &amp;amp; 1\end{bmatrix}^{\mathsf{T}}\), respectively. We can now transform vectors between the reference frames using this matrix:
$$
\ _{w}\mathbf{R}_{s} \triangleq \begin{bmatrix}\mathbf{a}&amp;#39;_{x_1} &amp;amp; \mathbf{a}&amp;#39;_{y_1} &amp;amp; \mathbf{a}&amp;#39;_{z_1} \end{bmatrix} 
$$
Like the \(2 \times 2\) rotation matrices, the matrix above is orthogonal. A nice computational implication of this property is that the inverse of a rotation matrix is equal to its inverse (which makes inversion both fast and numerically stable). If the coordinate frame is &lt;em&gt;right-handed&lt;/em&gt;, then \(\mathbf{a}_x \times \mathbf{a}_y = \mathbf{a}_z\) is a right-handed cross-product and the determinant of \(_w\mathbf{R}_s = 1\). If the coordinate frame is &lt;em&gt;left-handed&lt;/em&gt;, then \(\mathbf{a}_x \times \mathbf{a}_y = \mathbf{a}_z\) is a left-handed cross-product and \(\textrm{det}(\ _w\mathbf{R}_s) = 1\). &lt;/p&gt;

&lt;p&gt;I note that all vectors should be unit vectors and the set of vectors should be mutually orthogonal (hint: you should use this as a debugging check in your coursework). If one or both of these conditions is not satisfied, you will have significant problems!&lt;/p&gt;

&lt;h5 id=&quot;concatenating-rotations&quot;&gt;Concatenating rotations&lt;/h5&gt;

&lt;p&gt;If we have two orientations, \(\ _w\mathbf{R}_s\) and \(\ _s\mathbf{R}_r\), we can concatenate them- effecting one rotation followed by another- using matrix multiplication: \(\ _w\mathbf{R}_r =\ _w\mathbf{R}_s \cdot \ _s\mathbf{R}_r\). Of course, matrix multiplication is commutative, so the order of multiplication is important to get the proper rotation. This warning should sound familiar: use subscripts (or superscripts, or some other system that works for you), to make sure that your operations are correct. Just verifying that your calculations are right by testing one or two cases is dangerous!&lt;/p&gt;

&lt;h4 id=&quot;axis-angle&quot;&gt;Axis-angle&lt;/h4&gt;

&lt;p&gt;The axis-angle representation of orientation appears as the following:
$$
\begin{bmatrix}
u_x &amp;amp; u_y &amp;amp; u_z
\end{bmatrix}^{\mathsf{T}}
\theta
$$
This representation indicates that a rotation about the &lt;em&gt;unit vector&lt;/em&gt; \(\begin{bmatrix} u_x &amp;amp; u_y &amp;amp; u_z \end{bmatrix}^{\mathsf{T}}\) by an angle of \(\theta\).&lt;/p&gt;

&lt;p&gt;The rotation matrix representation corresponds one-to-one to SO(3), the 
&amp;quot;special orthogonal group&amp;quot; of orientations in three-dimensions. However,
the axis-angle representation generally maps two-to-one to SO(3): the values \((\begin{bmatrix} u_x &amp;amp; u_y &amp;amp; u_z \end{bmatrix}^{\mathsf{T}} \theta)\) and \((\begin{bmatrix} -u_x &amp;amp; -u_y &amp;amp; -u_z \end{bmatrix}^{\mathsf{T}} -\theta)\) are identical. The exception is a singularity at \((\begin{bmatrix} u_x &amp;amp; u_y &amp;amp; u_z \end{bmatrix}^{\mathsf{T}} 0)\), where the values of \(u_x\), \(u_y\), and \(u_z\) are arbitrary: all values map to an identity orientation. The singularity makes conversion from a rotation matrix to the axis-angle representation a little tricky, as will be seen below.&lt;/p&gt;

&lt;h5 id=&quot;conversion-to-rotation-matrix&quot;&gt;Conversion to rotation matrix&lt;/h5&gt;

&lt;p&gt;$$
\begin{bmatrix}
u_x^2v + \cos{\theta} &amp;amp; u_x u_y v - u_z\sin{\theta} &amp;amp; u_xu_z v + y\sin{\theta}\\
u_xu_yv + u_z\sin{\theta} &amp;amp; u_y^2v + \cos{\theta} &amp;amp; u_yu_z v - u_x\sin{\theta}\\
u_xu_zv - u_y\sin{\theta} &amp;amp; u_yu_zv - u_x\sin{\theta} &amp;amp; u_z^2v + \cos{\theta}&lt;br&gt;
\end{bmatrix}
$$
where \(v = 1 - \cos{\theta}\) if \(|\theta| &amp;gt; \epsilon\), and \(v = \frac{\sin^2{\theta}}{1+\cos{\theta}}\) otherwise (this formula minimizes cancellation error (see [Ericson 2005], p. 441).&lt;/p&gt;

&lt;p&gt;Note that there exists no straightforward way to concatenate rotations using the axis-angle representation; one must convert to rotation matrices, multiply, and convert back (if so desired).&lt;/p&gt;

&lt;h5 id=&quot;conversion-from-rotation-matrix&quot;&gt;Conversion from rotation matrix&lt;/h5&gt;

&lt;p&gt;\begin{align}
\theta &amp;amp; = \cos^{-1}\Big(\frac{1}{2}(R_{xx} + R_{yy} + R_{zz} - 1)\Big) \\
u_x &amp;amp; = \frac{R_{zy} - R_{yz}}{2 \sin \theta} \\
u_y &amp;amp; = \frac{R_{xz} - R_{zx}}{2 \sin \theta} \\
u_z &amp;amp; = \frac{R_{yx} - R_{xy}}{2 \sin \theta} 
\end{align}&lt;/p&gt;

&lt;h4 id=&quot;unit-quaternions&quot;&gt;Unit quaternions&lt;/h4&gt;

&lt;p&gt;Unit quaternions take the form:
$$
\left.\begin{array}{llll} e_x &amp;amp; e_y &amp;amp; e_z &amp;amp; e_w \end{array}\right.
$$
and are subject to the constraint \(e_x^2 + e_y^2 + e_z^2 + e_w^2 = 1\). The following general text about unit quaternions- also known as &lt;em&gt;rotational quaternions&lt;/em&gt; and &lt;em&gt;Euler parameters&lt;/em&gt; (the latter especially in Mechanics)- was adapted from Wikipedia:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Unit quaternions represent [SO(3)] in a very straightforward way. 
The correspondence between rotations and quaternions can be understood 
by first visualizing the space of rotations itself. In order to 
visualize the space of rotations, it helps to consider a simpler case. 
Any rotation in three dimensions can be described by a rotation by 
some angle about some axis; for our purposes, we will use an axis 
vector to establish handedness for our angle. Consider the special 
case in which the axis of rotation lies in the xy plane. We can then 
specify the axis of one of these rotations by a point on a circle 
through which the vector crosses, and we can select the radius of the 
circle to denote the angle of rotation. Similarly, a rotation whose 
axis of rotation lies in the xy plane can be described as a point on 
a sphere of fixed radius in three dimensions. Beginning at the north 
pole of a sphere in three dimensional space, we specify the point at 
the north pole to be the identity rotation (a zero angle rotation). 
Just as in the case of the identity rotation, no axis of rotation is 
defined, and the angle of rotation (zero) is irrelevant. A rotation 
having a very small rotation angle can be specified by a slice through 
the sphere parallel to the xy plane and very near the north pole. 
The circle defined by this slice will be very small, corresponding to 
the small angle of the rotation. As the rotation angles become larger, 
the slice moves in the -z direction, and the circles become larger 
until the equator of the sphere is reached, which will correspond to a 
rotation angle of pi radians. Continuing southward, the radii of the 
circles now become smaller (corresponding to the absolute value of the 
angle of the rotation considered as a negative number). Finally, as 
the south pole is reached, the circles shrink once more to the 
identity rotation, which is also specified as the point at the 
south pole.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h5 id=&quot;angle-between-two-orientations&quot;&gt;Angle between two orientations&lt;/h5&gt;

&lt;p&gt;One nice property of unit quaternions is that they allow one to compute the distance between two orientations using the &lt;em&gt;geodesic&lt;/em&gt; between the orientations. Think of the geodesic as the shortest path connecting any two orientations in SO(3). The distance between two orientations \(A\) and \(B\) is measured using the angle between them: \(\theta = \cos^{-1}{\mathbf{e}^{A^\mathsf{T}}\mathbf{e}^B}\)
or equivalently (if it disturbs you to perform dot products over unit quaternions:
$$
\theta = \cos^{-1}{(e^A_xe^B_x + e^A_ye^B_y + e^A_ze^B_z + e^A_we^B_w)}
$$&lt;/p&gt;

&lt;h5 id=&quot;conversion-to-rotation-matrices&quot;&gt;Conversion to rotation matrices&lt;/h5&gt;

&lt;p&gt;We can transform unit quaternions to \(3 \times 3\) rotation matrices using the function definition \(R(.)\) below:
$$
R(\mathbf{e}) = 
\begin{bmatrix}
2(e_x^2 + e_w^2) - 1 &amp;amp; 2(e_xe_y - e_ze_w) &amp;amp; 2(e_xe_z + e_ye_w) \\
2(e_xe_y + e_ze_w) &amp;amp; 2(e_y^2 + e_w^2) - 1 &amp;amp; 2(e_ye_z - e_xe_w) \\
2(e_xe_z - e_ye_w) &amp;amp; 2(e_ye_z + e_xe_w) &amp;amp; 2(e_z^2 + e_w^2) - 1
\end{bmatrix}
$$&lt;/p&gt;

&lt;h5 id=&quot;transforming-vectors&quot;&gt;Transforming vectors&lt;/h5&gt;

&lt;p&gt;If we define the two matrices \(\mathbf{G}\) and \(\mathbf{L}\) as follows:
$$
\mathbf{G}(\mathbf{e}) \triangleq \begin{bmatrix} 
-e_x &amp;amp; e_w &amp;amp; -e_z &amp;amp; e_y \\
-e_y &amp;amp; e_z  &amp;amp; e_w &amp;amp; -e_x \\
-e_z &amp;amp; -e_y &amp;amp; e_x &amp;amp; e_w
\end{bmatrix}
$$
$$
\mathbf{L}(\mathbf{e}) \triangleq \begin{bmatrix} 
-e_x &amp;amp; e_w &amp;amp; e_z &amp;amp; -e_y \\
-e_y &amp;amp; -e_z  &amp;amp; e_w &amp;amp; e_x \\
-e_z &amp;amp; e_y &amp;amp; -e_x &amp;amp; e_w
\end{bmatrix}
$$
then you can verify that \(R(\mathbf{e}) = \mathbf{G}(\mathbf{e}){\mathbf{L}(\mathbf{e})}^{\mathsf{T}}\). Then, transforming a vector \(\mathbf{v}\) is most efficient by applying the operation \(\mathbf{G}(\mathbf{e}){\mathbf{L}(\mathbf{e})}^\mathsf{T}\mathbf{v}\).&lt;/p&gt;

&lt;h5 id=&quot;inversion-of-rotation&quot;&gt;Inversion of rotation&lt;/h5&gt;

&lt;p&gt;Inversion of rotation using quaternions is simple using the &lt;em&gt;conjugate&lt;/em&gt;
of a quaternion: simply negate the vector component of the quaternion values to get the conjugate:
$$
R(\mathbf{e})^{-1} = R(-\mathbf{e})
$$&lt;/p&gt;

&lt;h5 id=&quot;concatenation-of-rotations&quot;&gt;Concatenation of rotations&lt;/h5&gt;

&lt;p&gt;We can apply rotation \(A\) (represented by quaternion \(\mathbf{e}^A\)) followed by rotation \(B\) (represented by quaternion \(\mathbf{e}^B\) using the following matrix-vector multiplication operation:
$$
\mathbf{e}^B \otimes \mathbf{e}^A = \begin{bmatrix}
e^A_w &amp;amp; e^A_z &amp;amp; -e^A_y &amp;amp; e^A_x \\ 
-e^A_z &amp;amp; e^A_w &amp;amp; e^A_x &amp;amp; e^A_y \\
e^A_y &amp;amp; -e^A_x &amp;amp; e^A_w &amp;amp; e^A_z \\ 
-e^A_x &amp;amp; -e^A_y &amp;amp; -e^A_z &amp;amp; e^A_w&lt;br&gt;
\end{bmatrix}
\begin{bmatrix}
e^B_x \\
e^B_y \\
e^B_z \\
e^B_w
\end{bmatrix}
$$&lt;/p&gt;

&lt;h4 id=&quot;alternative-representations&quot;&gt;Alternative representations&lt;/h4&gt;

&lt;p&gt;There exist alternative representations of orientations in 3D, including &lt;em&gt;Euler/roll-pitch-yaw/Tait-Bryan&lt;/em&gt; angles, &lt;em&gt;Gibbs vectors&lt;/em&gt;, &lt;em&gt;Pauli spin matrices&lt;/em&gt;, among others. I will not cover these representations in this post.&lt;/p&gt;

&lt;h4 id=&quot;storage-considerations&quot;&gt;Storage considerations&lt;/h4&gt;

&lt;p&gt;Encoding orientations exposes a tradeoff between speed and memory that may not immediately obvious to you. A few options for encoding orientations are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Store the full \(3 \times 3\) matrix&lt;/li&gt;
&lt;li&gt;Store the first two columns of the full \(3 \times 3\) matrix and compute the last column as needed at run-time&lt;/li&gt;
&lt;li&gt;Encode orientation using a three-dimensional orientation representation (Euler angles, roll-pitch-yaw angles)&lt;/li&gt;
&lt;li&gt;Encode orientation using a four-dimensional orientation representation (Quaternions, axis-angle)
The various computational tradeoffs between the choice of representation is shown in the table below.&lt;/li&gt;
&lt;/ol&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;\(3\times 3\) matrix&lt;/th&gt;
&lt;th&gt;\(2 \times 3\) matrix&lt;/th&gt;
&lt;th&gt;Euler-angles&lt;/th&gt;
&lt;th&gt;Quaternions&lt;/th&gt;
&lt;th&gt;Axis-angle&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Storage&lt;/td&gt;
&lt;td&gt;9 values&lt;/td&gt;
&lt;td&gt;6 values&lt;/td&gt;
&lt;td&gt;3 values&lt;/td&gt;
&lt;td&gt;4 values&lt;/td&gt;
&lt;td&gt;4 values&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Conversion to matrix&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;6*, 3+&lt;/td&gt;
&lt;td&gt;6 trig, 16*, 4+&lt;/td&gt;
&lt;td&gt;30*, 12+&lt;/td&gt;
&lt;td&gt;2 trig, 24*, 18+&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Vector transform&lt;/td&gt;
&lt;td&gt;9*, 6+&lt;/td&gt;
&lt;td&gt;15*, 9+&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;24*, 15+&lt;/td&gt;
&lt;td&gt;23*, 16+, 2 trig&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Rotation concatenation&lt;/td&gt;
&lt;td&gt;27*, 18+&lt;/td&gt;
&lt;td&gt;33*, 21+&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;16*, 12+&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Re-normalization cost&lt;/td&gt;
&lt;td&gt;36*, 27+, 3 sqrt&lt;/td&gt;
&lt;td&gt;15*, 11+, 2 sqrt&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;4*, 3+, 1 sqrt&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h3 id=&quot;homogeneous-transformations&quot;&gt;Homogeneous transformations&lt;/h3&gt;

&lt;p&gt;Homogeneous transformations work in 3D exactly like they did in 2D. The 
obvious differences are that homogeneous coordinates are now four dimensional:
$$
\begin{bmatrix}
x \\
y \\
z \\
1
\end{bmatrix}
$$&lt;/p&gt;

&lt;p&gt;and that the transformation matrices are now \(4 \times 4\). The upper left
\(3 \times 3\) block of the matrix is a rotation matrix and the fourth
column is a translation vector. The matrix still represents a rotation followed by a translation. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Block_matrix&quot;&gt;block form of the matrix&lt;/a&gt; still looks like this:
\begin{equation}
\mathbf{T} \equiv
\begin{bmatrix}
\mathbf{R} &amp;amp; \mathbf{t} \\
\mathbf{0} &amp;amp; 1
\end{bmatrix}
\label{eqn:homogeneous}
\end{equation}
but now \(\mathbf{R}\) is the \(3 \times 3\) rotation matrix and \(\mathbf{t}\) is the \(3 \times 1\) translation vector.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Homogeneous transformations can be inverted &lt;a href=&quot;../poses&quot;&gt;exactly as they were in the two-dimensional case&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;angular-velocity-and-acceleration-in-3d&quot;&gt;Angular velocity and acceleration in 3D&lt;/h3&gt;

&lt;p&gt;In order to explore the concept of angular velocity, we&amp;#39;ll examine the velocity of a point on a rigid body in 3D both geometrically and using calculus.
$$
\mathbf{p}_w = \bar{\mathbf{x}}_w + _w\mathbf{R}_i\mathbf{r}_i
$$
where \(\mathbf{p}_w\) is the point in the global frame, \(_w\mathbf{R}_i\) is the rotation matrix from frame \(i\) to the global frame, and \(\mathbf{r}_i\) is the vector from the center-of-mass of the body to the point &lt;em&gt;in the body frame&lt;/em&gt;, \(i\). 
When we take the time-derivative of this equation, we get:
$$
\dot{\mathbf{p}}_w = \dot{\bar{\mathbf{x}}}_w + _w\dot{\mathbf{R}}_i\mathbf{r}_i
$$
The remainder of this section will focus on gleaning insight into the structure of \(_w\mathbf{R}_i\). We start with a geometric interpretation of \(\dot{\mathbf{p}}_w\) using the figure below. If we ignore the body&amp;#39;s linear velocity (by assuming that \(\dot{\bar{\mathbf{x}}}_w = \mathbf{0}\)), then we find that the velocity of \(\dot{\mathbf{p}}_w\) is orthogonal both to the axis of rotation \(\mathbf{\omega}_w/||\mathbf{\omega}_w||\) and to the vector (\(||\mathbf{p} - \bar{\mathbf{x}}||\)). You can confirm geometrically or numerically that the magnitude of \(\dot{\mathbf{p}}_w\) is proportional to both \(||\mathbf{\omega}_w||\) and \(||\mathbf{r}_i||\). &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/frame-geom.png&quot; alt=&quot;A frame&quot;&gt;&lt;/p&gt;

&lt;p&gt;Based on this geometric interpretation of point velocity, it should be clear that \(\dot{\mathbf{p}}_w = \mathbf{\omega}_w \times\ _w\mathbf{R}_i\mathbf{r}_i\) (when the linear velocity at the center-of-mass is not zero, it just adds to \(\dot{\mathbf{p}}_w\)). Accounting for the equation above, we have \(_w\dot{\mathbf{R}}_i\mathbf{r}_i = \mathbf{\omega}_w \times _w\mathbf{R}_i\mathbf{r}_i\), and thus \(_w\dot{\mathbf{R}}_i = \mathbf{\omega}_w \times _w\mathbf{R}_i\). This formula should appear strange to you, because how can we take the cross product between a vector and a matrix? Doing a little matrix arithmetic, we get
\begin{equation}
\label{eqn:skewsymm}
_w\dot{\mathbf{R}}_i\cdot_w{\mathbf{R}_i}^{\mathsf{T}} = \tilde{\mathbf{\omega}}_w
\end{equation}
where we will get to the definition of \(\tilde{\mathbf{\omega}}\) shortly.&lt;/p&gt;

&lt;p&gt;Now, let&amp;#39;s use calculus to examine the time-derivative of a rotation matrix. We start with an identity from orthogonality:
$$
_w\mathbf{R}_i\cdot{_w\mathbf{R}_i}^{\mathsf{T}} - \mathbf{I} = \mathbf{0}
$$
Now computing the time-derivative:
$$
\dot{_w\mathbf{R}_i}\cdot{_w\mathbf{R}_i}^{\mathsf{T}} + _w\mathbf{R}_i\cdot{\dot{_w\mathbf{R}_i}}^{\mathsf{T}} = \mathbf{0}
$$
Using equations above, it should be apparent to you that
$$
\tilde{\mathbf{\omega}}_w + {\tilde{\mathbf{\omega}}_w}^{\mathsf{T}} = \mathbf{0}
$$
This equation indicates that \(\tilde{\mathbf{\omega}}_w\) is a &lt;em&gt;skew symmetric matrix&lt;/em&gt;, which takes the form:
$$
\tilde{\mathbf{\omega}} = \begin{bmatrix} 
0 &amp;amp; -\omega_z &amp;amp; \omega_y \\
\omega_z &amp;amp; 0 &amp;amp; -\omega_x \\
-\omega_y &amp;amp; \omega_x &amp;amp; 0
\end{bmatrix}
$$
from which we can recover the \(3 \times 1\) angular velocity tensor \(\mathbf{\omega}_w = \begin{bmatrix} \omega_x &amp;amp; \omega_y &amp;amp; \omega_z \end{bmatrix}^T\). &lt;/p&gt;

&lt;h4 id=&quot;angular-acceleration&quot;&gt;Angular acceleration&lt;/h4&gt;

&lt;p&gt;Let us now take the time-derivative of \(\dot{\mathbf{p}}_w\) to get:
\begin{equation}
\ddot{\mathbf{p}}_w = \ddot{\bar{\mathbf{x}}}_w + \frac{d}{dt}(\tilde{\mathbf{\omega}}_w\cdot_w\mathbf{R}_i)\mathbf{r}_i = \ddot{\bar{\mathbf{x}}}_w + \dot{\mathbf{\omega}}_w \times _w\mathbf{R}_i \cdot \mathbf{r}_i + \mathbf{\omega}_w \times _w\dot{\mathbf{R}}_i \cdot \mathbf{r}_i
\label{eqn:pddot}
\end{equation}
The \(3 \times 1\) vector \(\dot{\mathbf{\omega}}_w\) (also commonly written \(\mathbf{\alpha}_w\)) is the angular acceleration. The equation above is equivalent to:
\begin{equation}
\ddot{\mathbf{p}}_w = \ddot{\bar{\mathbf{x}}}_w + \dot{\mathbf{\omega}}_w \times _w\mathbf{R}_i \cdot \mathbf{r}_i + \mathbf{\omega}_w \times \mathbf{\omega} \times _w\mathbf{R}_i \cdot \mathbf{r}_i
\end{equation}
The second term on the right above is called the &lt;em&gt;transverse acceleration&lt;/em&gt; and the term to its right is the &lt;em&gt;centripetal acceleration&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Since we&amp;#39;re examining centripetal acceleration, let&amp;#39;s look at the phenomenon of Coriolis acceleration. To do that, we need to compute the acceleration of the point with respect to a point defined in a moving frame (\(s\), which we now assume is moving):
\begin{align}
\mathbf{p}_s &amp;amp; = \ _s\mathbf{R}_w\mathbf{p}_w + \mathbf{x}_w^s \\
\dot{\mathbf{p}}_s &amp;amp; = _s\dot{\mathbf{R}}_w\mathbf{p}_w + \ _s\mathbf{R}_w\dot{\mathbf{p}}_w \\
\ddot{\mathbf{p}}_s &amp;amp; = \dot{\mathbf{\omega}}_s \times \ _s\mathbf{R}_w\mathbf{p}_w + \mathbf{\omega}_s \times \ _s\mathbf{R}_w\dot{\mathbf{p}}_w + \mathbf{\omega}_s \times \ _s\mathbf{R}_w\dot{\mathbf{p}}_w +  \ _s\mathbf{R}_w\ddot{\mathbf{p}}_w \\
&amp;amp; = \dot{\mathbf{\omega}}_s \times \ _s\mathbf{R}_w\mathbf{p}_w + 2\mathbf{\omega}_s \times \ _s\mathbf{R}_w\dot{\mathbf{p}}_w +  \ _s\mathbf{R}_w\ddot{\mathbf{p}}_w
\end{align}
The second term that appears is the &lt;em&gt;Coriolis acceleration&lt;/em&gt;. Notice that this term only appears when observing the acceleration of a point from a moving frame.&lt;/p&gt;

&lt;h4 id=&quot;renormalization&quot;&gt;Renormalization&lt;/h4&gt;

&lt;p&gt;Let us say that you wish to update the rigid body orientation using first-order integration (although the problem we are exploring happens with any integration approach):
\begin{equation}
\mathbf{R}(t+\Delta t) = \mathbf{R}(t) + \Delta t\cdot \dot{\mathbf{R}}(t)
\end{equation}
If the angular velocity is large, then \(\dot{\mathbf{R}}\) will be large as well. As a result, the columns of \(\mathbf{R}(t+\Delta t)\) will generally no longer be either normalized or mutually orthogonal. \(\mathbf{R}(t + \Delta t)\) must then be renormalized using Gram-Schmidt normalization. &lt;/p&gt;

&lt;h4 id=&quot;time-derivatives-of-quaternions&quot;&gt;Time-derivatives of quaternions&lt;/h4&gt;

&lt;p&gt;The first time-derivative of the unit quaternion exhibits the following (identical) relationships with angular velocity: \(\mathbf{\omega}_w = 2\mathbf{G}(\mathbf{e})\dot{\mathbf{e}}\) and \(\dot{\mathbf{e}} = \frac{1}{2}{\mathbf{G}(\mathbf{e})}^{\mathsf{T}}\mathbf{\omega}_w\).
It can also be shown that \(\mathbf{\omega}_i = 2\mathbf{L}(\mathbf{e})\dot{\mathbf{e}}\) and \(\dot{\mathbf{e}} = \frac{1}{2}{\mathbf{L}(\mathbf{e})}^{\mathsf{T}}\mathbf{\omega}_i\), where \(\mathbf{\omega}_i\) is the angular velocity vector in the body&amp;#39;s frame.&lt;/p&gt;

&lt;p&gt;An advantage of unit quaternions is that renormalization is simple using an update:
\begin{align}
\mathbf{e}(t + \Delta t) &amp;amp; = \mathbf{e}(t) + \Delta t \cdot \dot{\mathbf{e}}(t) \\
\mathbf{e}(t + \Delta t) &amp;amp; \leftarrow \frac{\mathbf{e}(t + \Delta t)}{\sqrt{e_x(t + \Delta t) + e_y^2(t + \Delta t) + e_z^2(t + \Delta t) + e_w^2(t + \Delta t)}}
\end{align} &lt;/p&gt;

&lt;p&gt;The second time-derivative of the unit quaternion exhibits the following relationships with angular acceleration: \(\dot{\mathbf{\omega}}_w = 2\mathbf{G}(\mathbf{e})\ddot{\mathbf{e}}\) and \(\ddot{\mathbf{e}} = \frac{1}{2}{\mathbf{G}(\mathbf{e})}^{\mathsf{T}}\dot{\mathbf{\omega}}_w\). It can also be shown that \(\dot{\mathbf{\omega}}_i = 2\mathbf{L}(\mathbf{e})\ddot{\mathbf{e}}\) and \(\ddot{\mathbf{e}} = \frac{1}{2}{\mathbf{L}(\mathbf{e})}^{\mathsf{T}}\dot{\mathbf{\omega}}_i\).&lt;/p&gt;

&lt;h3 id=&quot;generalized-coordinates&quot;&gt;Generalized coordinates&lt;/h3&gt;

&lt;p&gt;Our choices for generalized coordinates, velocities, and accelerations are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Use one representation for everything (position plus unit quaternions). The advantages with this approach are that (1) coordinates/velocities/accelerations can be added to each other and (2) for dynamics, all formulas can be dealt with abstractly as \(\mathbf{f} = \mathbf{M}\mathbf{a}\) (Newton&amp;#39;s second law), \(\mathbf{p} = \mathbf{M}\mathbf{v}\) (generalized momentum), &lt;em&gt;etc.&lt;/em&gt; (one never needs to do any intermediate calculations).&lt;/li&gt;
&lt;li&gt;Use position and multiple representations for orientation, such as unit quaternions and (typically) scaled-axis angle (i.e., \(\mathbf{\omega}, \dot{\mathbf{\omega}}\)). The advantages with this approach are that (1) it&amp;#39;s more compact (generalized velocities and accelerations for rigid bodies require only six parameters, generalized inertia matrices are only \(6 \times 6\) rather than \(7 \times 7\)) and (2)   it&amp;#39;s far simpler: one need not convert generalized inertia, generalized forces, &lt;em&gt;etc.&lt;/em&gt; from their &amp;quot;natural&amp;quot; six-dimensional representation (to be explored at a later time) to a less-intuitive seven-dimensional representation.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;generalized-coordinates-in-the-global-reference-frame&quot;&gt;Generalized coordinates in the global reference frame&lt;/h4&gt;

&lt;p&gt;Let&amp;#39;s first look at the generalized coordinates for a body defined in the global reference frame, \(w\). We will define the generalized coordinates using unit quaternions:
\begin{equation} 
\mathbf{q}_w = \begin{bmatrix} \bar{x}_w \\ \bar{y}_w \\ \bar{z}_w \\ \mathbf{e}_i \end{bmatrix}
\end{equation}
where \({ \bar{x}_w, \bar{y}_w, \bar{z}_w }\) is the position of the center-of-mass in frame \(w\).  \(\ _w\mathbf{e}_i\) gives the orientation of the body relative to the global frame.&lt;/p&gt;

&lt;h4 id=&quot;general-coordinates-in-another-reference-frame&quot;&gt;General coordinates in another reference frame&lt;/h4&gt;

&lt;p&gt;For comparison, let&amp;#39;s look at how the body is defined in another reference frame, \(s\), which is defined relative to frame \(\mathbf{w}\) by a translation of \(\mathbf{x}_w^s\) and by an orientation via the rotation matrix \(_s\mathbf{R}_w\) (and using the equivalent unit quaternion \(\ _s\mathbf{e}_w\)).
The generalized coordinates in frame \(s\) are now:
\begin{equation}
\mathbf{q}_s = \begin{bmatrix}_s\mathbf{R}_w \begin{bmatrix}\bar{x}_w \\ \bar{y}_w \\ \bar{z}_w \end{bmatrix} + \mathbf{x}_w^s  \\ 
_s\mathbf{e}_w \otimes _w\mathbf{e}_i \end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;generalized-velocities-in-the-global-reference-frame&quot;&gt;Generalized velocities in the global reference frame&lt;/h4&gt;

&lt;p&gt;Let&amp;#39;s now compute the time-derivatives:
\begin{equation} 
\dot{\mathbf{q}}_w = \begin{bmatrix} \dot{\bar{x}}_w \\ \dot{\bar{y}}_w \\ \dot{\bar{z}}_w \\ _w\dot{\mathbf{e}}_i\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;generalized-velocities-in-another-reference-frame&quot;&gt;Generalized velocities in another reference frame&lt;/h4&gt;

&lt;p&gt;The generalized velocity in \mbox{frame \(s\)} is:
\begin{equation}
\dot{\mathbf{q}}_s 
= \begin{bmatrix}\tilde{\mathbf{\omega}}_s\cdot \ _s\mathbf{R}_w\begin{bmatrix}\dot{\bar{x}}_w \\ \dot{\bar{y}}_w \\ \dot{\bar{z}}_w \end{bmatrix} \\ _s\mathbf{e}_w \otimes _w\dot{\mathbf{e}}_i \end{bmatrix}
\end{equation}
Here we are assuming that frame \(s\) is fixed (and we already know that frame \(w\) is fixed, since it is the ``global&amp;#39;&amp;#39; frame of reference), because \(\mathbf{x}_w^s\) disappears upon taking the derivative and \(\ _s\mathbf{e}_w\) is constant.&lt;/p&gt;

&lt;h4 id=&quot;generalized-velocities-in-a-moving-reference-frame&quot;&gt;Generalized velocities in a moving reference frame&lt;/h4&gt;

&lt;p&gt;What if frame \(s\) is &lt;em&gt;not&lt;/em&gt; fixed but instead moves relative to frame \(w\) with linear velocity \(\dot{\mathbf{x}}_w^s\) and angular velocity \(\dot{\mathbf{\omega}}_w^s\)? Then: 
\begin{equation}
\dot{\mathbf{q}}_s = \begin{bmatrix}\tilde{\mathbf{\omega}}_s\cdot \ _s\mathbf{R}_w \begin{bmatrix}\dot{\bar{x}}_w \\ \dot{\bar{y}}_w \\ \dot{\bar{z}}_w \end{bmatrix} + \dot{\mathbf{x}}^s_w  \\ _s\dot{\mathbf{e}}_w \otimes _w\mathbf{e}_i + _s\dot{\mathbf{e}}_w \otimes _w\mathbf{e}_i \end{bmatrix}
\end{equation}&lt;/p&gt;
</description>
        
          <description>&lt;p&gt;&lt;p class=&quot;intro&quot;&gt;&lt;/p&gt;
</description>
        
        <pubDate>Sat, 19 Dec 2015 00:00:00 -0500</pubDate>
        <link>/robotics-course-materials/robotics-course-materials/blog/poses3/</link>
        <guid isPermaLink="true">/robotics-course-materials/robotics-course-materials/blog/poses3/</guid>
        
        
      </item>
      
    
      
      <item>
        <title>An engineer&#39;s guide to matrices, vectors, and numerical linear algebra</title>
        <description>&lt;p&gt;This material is not meant to replace a proper course in linear algebra, where
important concepts like vector spaces, eigenvalues/eigenvectors, and
spans are defined. My intent is to give you a practical guide to concepts
in matrix arithmetic and numerical linear algebra that I have found useful. &lt;/p&gt;

&lt;h3 id=&quot;matrix-arithmetic&quot;&gt;Matrix arithmetic&lt;/h3&gt;

&lt;h4 id=&quot;matrix-and-vector-addition-and-subtraction&quot;&gt;Matrix and vector addition and subtraction&lt;/h4&gt;

&lt;p&gt;Matrices and vectors can only be added or subtracted if they are of the
same dimensionality. Then, addition and subtraction are performed
elementwise:&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\begin{bmatrix}
u_1 \\
u_2 \\
u_3 
\end{bmatrix} + 
\begin{bmatrix}
v_1 \\
v_2 \\
v_3
\end{bmatrix} =
\begin{bmatrix}
u_1 + v_1 \\
u_2 + v_2 \\
u_3 + v_3 
\end{bmatrix}
\end{equation*}&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\begin{bmatrix}
m_{11} &amp;amp; m_{12} &amp;amp; m_{13} \\
m_{21} &amp;amp; m_{22} &amp;amp; m_{23} \\
m_{31} &amp;amp; m_{32} &amp;amp; m_{33}
\end{bmatrix} + 
\begin{bmatrix}
n_{11} &amp;amp; n_{12} &amp;amp; n_{13} \\
n_{21} &amp;amp; n_{22} &amp;amp; n_{23} \\
n_{31} &amp;amp; n_{32} &amp;amp; n_{33}
\end{bmatrix} = 
\begin{bmatrix}
m_{11} + n_{11} &amp;amp; m_{12} + n_{12} &amp;amp; m_{13} + n_{13} \\
m_{21} + n_{21} &amp;amp; m_{22} + n_{22} &amp;amp; m_{23} + n_{23} \\
m_{31} + n_{31} &amp;amp; m_{32} + n_{32} &amp;amp; m_{33} + n_{33}
\end{bmatrix}
\end{equation*}&lt;/p&gt;

&lt;h4 id=&quot;matrix-and-vector-scaling&quot;&gt;Matrix and vector scaling&lt;/h4&gt;

&lt;p&gt;Matrix and vectors can be scaled by multiplying every element in the matrix or vector by the scalar, as seen below:&lt;/p&gt;

&lt;p&gt;\begin{align*}
\mathbf{M} &amp;amp; \equiv \begin{bmatrix} 
m_{11} &amp;amp; m_{12} &amp;amp; m_{13} \\
m_{21} &amp;amp; m_{22} &amp;amp; m_{23} \\
m_{31} &amp;amp; m_{32} &amp;amp; m_{33} \end{bmatrix} \\ 
s \mathbf{M} &amp;amp; = \begin{bmatrix}
s m_{11} &amp;amp; sm_{12} &amp;amp; sm_{13} \\
s m_{21} &amp;amp; sm_{22} &amp;amp; sm_{23} \\
s m_{31} &amp;amp; sm_{32} &amp;amp; sm_{33} \end{bmatrix} 
\end{align*}
for \(s \in \mathbb{R}\)&lt;/p&gt;

&lt;h4 id=&quot;special-matrices&quot;&gt;Special matrices&lt;/h4&gt;

&lt;p&gt;I will point out three special types of matrices:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\(\mathbf{0} \in \mathbb{R}^{m \times n}\) (the &amp;quot;zero matrix&amp;quot;): a matrix with every entry set to zero&lt;/li&gt;
&lt;li&gt;Diagonal matrices: a square (\(n \times n\)) matrix with nonzero entries placed only on the diagonal (from upper left to lower right)&lt;/li&gt;
&lt;li&gt;\(\mathbf{I} \in \mathbb{R}^{n \times n}\) (the &amp;quot;identity matrix&amp;quot;): a diagonal matrix with every entry on the diagonal set to 1 &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;the-cross-product&quot;&gt;The cross product&lt;/h4&gt;

&lt;p&gt;The cross product operator (\(\times\)) does not fit neatly into matrix/vector arithmetic
and linear algebra, because it applies only to vectors in \(\mathbb{R}^3\).
For two vectors \(\mathbf{a}, \mathbf{b} \in \mathbb{R}^3\), \(\mathbf{a} \times \mathbf{b}\) yields:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A vector orthogonal to \(\mathbf{a}\) and to \(\mathbf{b}\), assuming that \(\mathbf{a}\) and \(\mathbf{b}\) are linearly independent (otherwise, \(\mathbf{a} \times \mathbf{b} = \mathbf{0}\)) &lt;/li&gt;
&lt;li&gt;The negation of \(\mathbf{b} \times \mathbf{a}\); stated again, \(\mathbf{a} \times \mathbf{b} = -\mathbf{b} \times \mathbf{a}\).&lt;/li&gt;
&lt;li&gt;A different vector depending on whether the cross product is a &lt;em&gt;right handed cross product&lt;/em&gt; or a &lt;em&gt;left handed cross product&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The cross product is distributive over addition:&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\mathbf{a} \times (\mathbf{b} + \mathbf{c}) = \mathbf{a} \times \mathbf{b} + \mathbf{a} \times \mathbf{c}
\end{equation*}&lt;/p&gt;

&lt;h5 id=&quot;computing-the-cross-product&quot;&gt;Computing the cross product&lt;/h5&gt;

&lt;p&gt;For the right handed cross product, 
$$
\mathbf{a} \times \mathbf{b} = \begin{bmatrix}
a_2b_3 - a_3b_2 \\
a_3b_1 - a_1b_3 \\
a_1b_2 - a_2b_1
\end{bmatrix}
$$&lt;/p&gt;

&lt;p&gt;The left handed cross product is just the negation of this vector.&lt;/p&gt;

&lt;h4 id=&quot;inner-products&quot;&gt;Inner products&lt;/h4&gt;

&lt;p&gt;The inner product operation (also called the &lt;em&gt;dot product&lt;/em&gt;) between two vectors \(\mathbf{a}\) and \(\mathbf{b}\) is written \(\lt \mathbf{a}, \mathbf{b}\gt \), \(\mathbf{a}^{\textsf{T}}\mathbf{b}\), or \(\mathbf{a} \cdot \mathbf{b}\). The inner product is only defined for vectors of equal dimension and consists of the sum of the products of the corresponding elements from the two vectors. An example is given below:&lt;/p&gt;

&lt;p&gt;\begin{align*}
\mathbf{a} &amp;amp; \equiv \begin{bmatrix} a_1 \\ a_2 \\ a_3 \\ a_4 \end{bmatrix} \\
\mathbf{b} &amp;amp; \equiv \begin{bmatrix} b_1 \\ b_2 \\ b_3 \\ b_4 \end{bmatrix} \\
\mathbf{a}^\textsf{T}\mathbf{b} &amp;amp; = a_1 b_1 + a_2 b_2 + a_3 b_3 + a_4 b_4
\end{align*}&lt;/p&gt;

&lt;p&gt;Some properties of the inner product follow, for real vectors \(\mathbf{a}, \mathbf{b}, \mathbf{c}\):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The dot product is commutative: \(\mathbf{a} \cdot \mathbf{b} = \mathbf{b} \cdot \mathbf{a}\)&lt;/li&gt;
&lt;li&gt;The dot product is distributive over vector addition: \(\mathbf{a} \cdot (\mathbf{b} + \mathbf{c}) = \mathbf{a} \cdot \mathbf{b} + \mathbf{a} \cdot \mathbf{c}\)&lt;/li&gt;
&lt;li&gt;If \(\mathbf{a}, \mathbf{b} \ne \mathbf{0}\), \(\mathbf{a} \cdot \mathbf{b} = 0 \) if and only if \(\mathbf{a}\) and \(\mathbf{b}\) are orthogonal&lt;/li&gt;
&lt;li&gt;\(\mathbf{a} \times (\mathbf{b} \times \mathbf{c}) = \mathbf{b}(\mathbf{a} \cdot \mathbf{c}) - \mathbf{c}(\mathbf{a} \cdot \mathbf{b})\)&lt;/li&gt;
&lt;li&gt;\(\mathbf{a} \cdot (\mathbf{b} \times \mathbf{c}) = \mathbf{b} \cdot (\mathbf{c} \times \mathbf{a}) = \mathbf{c} \cdot (\mathbf{a} \times \mathbf{b})\) &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;matrix-vector-multiplication&quot;&gt;Matrix/vector multiplication&lt;/h4&gt;

&lt;p&gt;Matrix/vector multiplication is identical to multiple inner (dot) product
operations over the rows of \(\mathbf{A}\). Assume we define matrix \(\mathbf{A}\) as follows:&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\mathbf{A} \equiv \begin{bmatrix} \mathbf{a}_1 \\ \vdots \\ \mathbf{a}_m \end{bmatrix}
\end{equation*}&lt;/p&gt;

&lt;p&gt;We can then compute the matix vector product \(\mathbf{Av}\) using \(m\) inner products:&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\mathbf{Av} = \begin{bmatrix} \mathbf{a}_1\mathbf{v} \\ \vdots \\ \mathbf{a}_m\mathbf{v} \end{bmatrix}
\end{equation*} &lt;/p&gt;

&lt;p&gt;As a concrete example, we define:&lt;/p&gt;

&lt;p&gt;\begin{align*}
\mathbf{A} &amp;amp; \equiv \begin{bmatrix} a &amp;amp; b &amp;amp; c \\ d &amp;amp; e &amp;amp; f \end{bmatrix} \\
\mathbf{v} &amp;amp; \equiv \begin{bmatrix} g \\ h \\ i \end{bmatrix} \\
\mathbf{Av} &amp;amp; = \begin{bmatrix} ag + bh + ci \\ dg + eh + fi \end{bmatrix}
\end{align*}&lt;/p&gt;

&lt;h4 id=&quot;matrix-matrix-multiplication&quot;&gt;Matrix/matrix multiplication&lt;/h4&gt;

&lt;p&gt;Matrix/matrix multiplication proceeds the same as matrix-vector multiplication
over multiple columns:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{MN} \equiv \mathbf{M}\begin{bmatrix} \mathbf{n}_1 &amp;amp; \ldots &amp;amp; \mathbf{n}_n \end{bmatrix} \equiv \begin{bmatrix} \mathbf{Mn}_1 &amp;amp; \ldots &amp;amp; \mathbf{Mn}_n \end{bmatrix} 
\end{equation}&lt;/p&gt;

&lt;p&gt;As a concrete example:&lt;/p&gt;

&lt;p&gt;\begin{align*}
\mathbf{M} &amp;amp; \equiv 
\begin{bmatrix} a &amp;amp; b \\ c &amp;amp; d \\ e &amp;amp; f \end{bmatrix} \\
\mathbf{n}_1 &amp;amp; \equiv \begin{bmatrix} g \\ j \end{bmatrix}  \\
\mathbf{n}_2 &amp;amp; \equiv \begin{bmatrix} h \\ k \end{bmatrix}  \\
\mathbf{n}_3 &amp;amp; \equiv \begin{bmatrix} i \\ l \end{bmatrix}  \\
\mathbf{N} &amp;amp; \equiv \begin{bmatrix} \mathbf{n}_1 &amp;amp; \mathbf{n}_2 &amp;amp; \mathbf{n}_3 \end{bmatrix} \\
\mathbf{Mn}_1 &amp;amp; \equiv \begin{bmatrix} ag + bj \\ cg + dj \\ eg + fj \end{bmatrix} \\
\mathbf{Mn}_2 &amp;amp; \equiv \begin{bmatrix} ah + bk \\ ch + dk \\ eh + fk \end{bmatrix} \\
\mathbf{Mn}_3 &amp;amp; \equiv \begin{bmatrix} ai + bl \\ ci + dl \\ ei + fl \end{bmatrix} \\
\mathbf{MN} &amp;amp; \equiv \begin{bmatrix} ag+bj &amp;amp; ah+bk &amp;amp; ai+bl \\ cg+dj &amp;amp; ch+dk &amp;amp; ci + dl\\ eg+fj &amp;amp; eh+fk &amp;amp; ei+fl \end{bmatrix}
\end{align*}&lt;/p&gt;

&lt;p&gt;Note that &lt;strong&gt;matrix multiplication is not commutative&lt;/strong&gt;: \(\mathbf{AB} \ne \mathbf{BA}\) (generally), even when the dimensions are compatible.&lt;/p&gt;

&lt;h4 id=&quot;outer-products&quot;&gt;Outer products&lt;/h4&gt;

&lt;p&gt;The &lt;em&gt;outer product&lt;/em&gt; \(\mathbf{ab}^\mathsf{T}\) of two vectors \(\mathbf{a} \in \mathbb{R}^m\) and \(\mathbf{b} \in \mathbb{R}^n\) is always defined and represents a matrix/matrix multiplication operation between a \(m \times 1\) and a \(1 \times n\) matrix; in other words, normal matrix/matrix multiplication rules apply.&lt;/p&gt;

&lt;h4 id=&quot;matrix-transposition&quot;&gt;Matrix transposition&lt;/h4&gt;

&lt;p&gt;The transpose of a matrix is defined as follows:&lt;/p&gt;

&lt;p&gt;$$
A_{ij}^\mathsf{T} = A_{ji}
$$&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;A depiction of the matrix transpose operation.&lt;/caption&gt;
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/e/e4/Matrix_transpose.gif&quot; alt=&quot;A depiction of the matrix transpose operation.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;where the operator \(^\mathsf{T}\) indicates transposition. This definition implies that an \(m \times n\) matrix becomes an \(n \times m\) matrix. If \(\mathbf{A} = \mathbf{A}^\textsf{T}\) we say that \(\mathbf{A}\) is a &lt;em&gt;symmetric matrix&lt;/em&gt;. &lt;/p&gt;

&lt;p&gt;The following properties apply to matrix transposition for matrices \(\mathbf{A}\) and \(\mathbf{B}\):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\((\mathbf{A}^\textsf{T})^\mathsf{T} = \mathbf{A}\)&lt;/li&gt;
&lt;li&gt;\((\mathbf{A}+\mathbf{B})^\mathsf{T} = \mathbf{A}^\mathsf{T} + \mathbf{B}^\mathsf{T}\)&lt;/li&gt;
&lt;li&gt;\((\mathbf{AB})^\mathsf{T} = \mathbf{B}^\mathsf{T}\mathbf{A}^\mathsf{T}\)&lt;/li&gt;
&lt;li&gt;If \(\mathbf{A}\) has only real entries, then \(\mathbf{A}^\textsf{T}\mathbf{A}\) is a positive semi-definite matrix (see below).&lt;/li&gt;
&lt;li&gt;\(\mathbf{A}\mathbf{A}^\textsf{T}\) is a symmetric matrix.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;matrix-inversion&quot;&gt;Matrix inversion&lt;/h4&gt;

&lt;p&gt;The inverse of a matrix \(\mathbf{A}\), written \(\mathbf{A}^{-1}\), is characterized by the following properties:&lt;/p&gt;

&lt;p&gt;\begin{align}
\mathbf{AA}^{-1} &amp;amp; = \mathbf{I} \\
\mathbf{A}^{-1}\mathbf{A} &amp;amp; = \mathbf{I}
\end{align}&lt;/p&gt;

&lt;p&gt;The inverse exists only if \(\mathbf{A}\) is square and is &lt;em&gt;non-singular&lt;/em&gt;. Singularity can be determined multiple ways (only two are listed below):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If the determinant of the matrix is zero, the matrix is singular. The determinant can be computed using LU factorization (see below).&lt;/li&gt;
&lt;li&gt;If one or more of the singular values of the matrix is zero, the matrix is singular. The singular values can be computed using the singular value decomposition (see below).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following properties apply to matrix inversion for matrices \(\mathbf{A}\) and \(\mathbf{B}\):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\((\mathbf{A}^\textsf{-1})^\mathsf{-1} = \mathbf{A}\)&lt;/li&gt;
&lt;li&gt;\((\mathbf{A}^\textsf{T})^{-1} = (\mathbf{A}^{-1})^\textsf{T}\)&lt;/li&gt;
&lt;li&gt;\((\mathbf{AB})^{-1} = \mathbf{B}^{-1}\mathbf{A}^{-1}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In numerical linear algebra, you almost never need to explicitly form the
inverse of a matrix and &lt;strong&gt;you should avoid explicitly forming the inverse
whenever possible&lt;/strong&gt;: &lt;em&gt;the solution obtained by computing \(\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}\) is considerably slower than back/forward substitution-based methods&lt;/em&gt; (using, e.g., Cholesky factorization, LU factorization, etc.) for solving \(\mathbf{Ax} = \mathbf{b}\).&lt;/p&gt;

&lt;h3 id=&quot;empty-matrices-and-vectors&quot;&gt;Empty matrices and vectors&lt;/h3&gt;

&lt;p&gt;Empty matrices (matrices with one or more dimensions equal to zero) are often useful. They allow formulas,
optimization, etc. to be used without breaking even when the inputs to the
problem are empty: it does not become necessary to use special logic to
handle such corner cases. &lt;/p&gt;

&lt;p&gt;Given scalar \(s \in \mathbb{R}\), empty matrix \(\mathbf{M} \in \mathbb{R}^{m \times n}\) (with one of \(m,n\) equal to zero), empty matrix \(\mathbf{E} \in \mathbb{R}^{0 \times m}\), and empty matrix \(\mathbf{F} \in \mathbb{R}^{n \times 0}\), we have the following rules:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;\(s \mathbf{M} = \mathbf{M}\)&lt;/li&gt;
&lt;li&gt;\(\mathbf{M} + \mathbf{M} = \mathbf{M} \)&lt;/li&gt;
&lt;li&gt;\(\mathbf{EM} = \mathbf{F}^\mathsf{T}\)&lt;/li&gt;
&lt;li&gt;\(\mathbf{MF} = \mathbf{E}^\mathsf{T}\)&lt;/li&gt;
&lt;li&gt;\(\mathbf{E}\mathbf{F}^\mathsf{T} = \mathbf{0}\)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;computational-considerations-for-matrix-multiplication-associativity&quot;&gt;Computational considerations for matrix-multiplication associativity&lt;/h3&gt;

&lt;p&gt;Matrix multiplication &lt;em&gt;is&lt;/em&gt; associative: \((\mathbf{AB})\mathbf{C} = \mathbf{A}(\mathbf{BC})\). The amount of computation required can be very different in the two cases.
Assume that \(\mathbf{A} \in \mathbb{R}^{i \times j}, \mathbf{B} \in \mathbb{R}^{j \times k}, \mathbf{C} \in \mathbb{R}^{k \times m}\). Depending on the order of operation, two very different flop counts are possible:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;\((\mathbf{A}\mathbf{B})\mathbf{C} = O(ijk) + O(ikm)\)&lt;/li&gt;
&lt;li&gt;\(\mathbf{A}(\mathbf{B}\mathbf{C}) = O(jkm) + O(ijm)\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now consider the following variable instances: \(i = 1, j = 2, k = 3, m = 4 \). The asymptotic number of operations in Case (1) will be on the order of 15 flops and in Case (2) will be 32 flops. Takeaway: consider your multiplication ordering. &lt;/p&gt;

&lt;p&gt;Note that in the case of multiplying a chain of matrices and then a vector:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{ABv}
\end{equation}&lt;/p&gt;

&lt;p&gt;One always wants to do the vector multiplication first:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{ABv} = \mathbf{A}(\mathbf{Bv})
\end{equation}&lt;/p&gt;

&lt;p&gt;In many applications, only a few matrices may be multiplied at
one time, meaning that a little logic can be used to determine the order of operations. For longer chains of matrices, one likely wants to use &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_chain_multiplication&quot;&gt;dynamic programming to determine the optimal multiplication order&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;linear-algebra&quot;&gt;Linear algebra&lt;/h3&gt;

&lt;h4 id=&quot;orthogonality&quot;&gt;Orthogonality&lt;/h4&gt;

&lt;p&gt;Vectors \(\mathbf{u}\) and \(\mathbf{v}\) are orthogonal if their dot (inner) product is zero.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Orthogonal&lt;/em&gt; matrices are very convenient because they possess the property that their inverse is equal to their transpose:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{A}^\mathsf{T} = \mathbf{A}^{-1} \textrm{ if } \mathbf{A} \textrm{ orthogonal}
\end{equation}&lt;/p&gt;

&lt;p&gt;Computationally, this means that the inverse can be computed quickly and robustly. An orthogonal matrix has the following properties:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The determinant of the matrix is \(\pm 1\)&lt;/li&gt;
&lt;li&gt;The dot product of any two rows \(i \ne j\) of the matrix is zero&lt;/li&gt;
&lt;li&gt;The dot product of any two columns \(i \ne j\) of the matrix is zero&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;positive-and-negative-definiteness&quot;&gt;Positive and negative definiteness&lt;/h4&gt;

&lt;p&gt;A symmetric matrix \(\mathbf{A}\) is &lt;em&gt;positive definite&lt;/em&gt; if:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{x}^\mathsf{T}\mathbf{A}\mathbf{x} \gt 0
\end{equation}&lt;/p&gt;

&lt;p&gt;for any \(\mathbf{x} \in \mathbb{R}^n\) such that \(\mathbf{x} \ne \mathbf{0}\). This condition is equivalent to saying that a matrix is positive definite if all of its &lt;a href=&quot;https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors&quot;&gt;eigenvalues&lt;/a&gt; are positive; eigenvalues are readily computable with GNU Octave/Matlab (using &lt;code&gt;eig&lt;/code&gt;) and with most libraries for numerical linear algebra. If \(\mathbf{A}\) is not positive definite and instead,&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{x}^\mathsf{T}\mathbf{A}\mathbf{x} \ge 0
\end{equation}&lt;/p&gt;

&lt;p&gt;for any \(\mathbf{x} \in \mathbb{R}^n\) such that \(\mathbf{x} \ne \mathbf{0}\), we say that the matrix is &lt;em&gt;positive semi-definite&lt;/em&gt;. This condition is equivalent to saying that a matrix is positive semi-definite if all of its eigenvalues are
non-negative. &lt;/p&gt;

&lt;p&gt;Similarly, a matrix is &lt;em&gt;negative definite&lt;/em&gt; if all of its eigenvalues are
strictly negative and &lt;em&gt;negative semi-definite&lt;/em&gt; if all of its eigenvalues
are non-positive. If none of these conditions hold- \(\mathbf{A}\) has both positive and negative eigenvalues- we say that the matrix is &lt;em&gt;indefinite&lt;/em&gt;.&lt;/p&gt;

&lt;h5 id=&quot;checking-positive-definiteness&quot;&gt;Checking positive-definiteness&lt;/h5&gt;

&lt;p&gt;The fastest general way to check for positive-definiteness is using the
Cholesky factorization. If the Cholesky factorization succeeds, the matrix
is positive definite. This approach for checking positive definiteness
is a (significant) constant factor faster than approaches that compute eigenvalues.&lt;/p&gt;

&lt;h5 id=&quot;applications-of-definiteness&quot;&gt;Applications of definiteness&lt;/h5&gt;

&lt;p&gt;Definite matrices have many applications in engineering applications.
As one example, if the Hessian matrix of an objective function is 
positive semi-definite, the function is convex and admits solution
via robust convex optimization codes. As another example, Lyapunov
stability analysis requires negative definiteness of the time derivative
of a Lyapunov function candidate. &lt;/p&gt;

&lt;p&gt;We often wish to avoid indefinite matrices. For example, quadratic programming
with indefinite matrices is NP-hard, while it is polynomial time solvable
with definite matrices. &lt;/p&gt;

&lt;h4 id=&quot;factorizations&quot;&gt;Factorizations&lt;/h4&gt;

&lt;p&gt;I like to consider matrix factorizations in ascending order of computational expense. Correlated with computational expense is numerical robustness. A list of the factorizations follows:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Factorization&lt;/th&gt;
&lt;th&gt;Flops&lt;/th&gt;
&lt;th&gt;Applicability&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Cholesky factorization&lt;/td&gt;
&lt;td&gt;\(n^3/3\)&lt;/td&gt;
&lt;td&gt;Positive-definite matrices only&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LDL\(^\mathsf{T}\) factorization&lt;/td&gt;
&lt;td&gt;\(n^3/2 + O(n^2)\)&lt;/td&gt;
&lt;td&gt;Symmetric matrices only&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LU factorization&lt;/td&gt;
&lt;td&gt;\(2n^3/3\)&lt;/td&gt;
&lt;td&gt;Non-singular matrices (no least squares)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;QR factorization&lt;/td&gt;
&lt;td&gt;\(4n^3/3\)&lt;/td&gt;
&lt;td&gt;Singular and non-singular matrices (least squares ok)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Singular value decomposition&lt;/td&gt;
&lt;td&gt;\(8n^3/3\)&lt;/td&gt;
&lt;td&gt;Singular and non-singular matrices (least squares ok)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4 id=&quot;nullspace&quot;&gt;Nullspace&lt;/h4&gt;

&lt;p&gt;The nullspace \(\mathbf{R}\) of matrix \(\mathbf{A}\) is a nonzero matrix
such that: &lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{AR} = \mathbf{0}
\end{equation}&lt;/p&gt;

&lt;p&gt;The nullspace of a matrix \(\mathbf{A}\) can be determined using the 
rightmost columns of \(\mathbf{V}\) (from the singular value decomposition 
of \(\mathbf{A}\)) that correspond to the zero singular values from 
\(\mathbf{\Sigma}\) (also from the SVD of \(\mathbf{A}\)).&lt;/p&gt;

&lt;p&gt;Nullspaces are particularly good for optimization and least squares problems.
For example, the nullspace allows optimizing along multiple criteria in
hierarchical fashion.&lt;/p&gt;

&lt;h3 id=&quot;matrix-calculus&quot;&gt;Matrix calculus&lt;/h3&gt;

&lt;p&gt;If \(\mathbf{a}, \mathbf{b}\) are functions, then the derivative of (denoted by prime &amp;#39;) the matrix multiplication operation is: \(\mathbf{a}^\mathsf{T}\mathbf{b} = {\mathbf{a}&amp;#39;}^{\mathsf{T}}\mathbf{b} + \mathbf{a}^\mathsf{T} \cdot \mathbf{b}&amp;#39;\)&lt;/p&gt;

&lt;h4 id=&quot;gradient&quot;&gt;Gradient&lt;/h4&gt;

&lt;p&gt;The gradient of a function \(f(\mathbf{x})\), where \(\mathbf{x} \in \mathbb{R}^n\) is the \(n\)-dimensional vector:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\nabla f_{\mathbf{x}} \equiv
\begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\vdots \\
\frac{\partial f}{\partial x_n} \\
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;hessian&quot;&gt;Hessian&lt;/h4&gt;

&lt;p&gt;The Hessian matrix of the same function is the \(n \times n\) matrix of second order partial derivatives:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\nabla f_{\mathbf{xx}} \equiv 
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} &amp;amp; \ldots &amp;amp; \frac{\partial^2 f}{\partial x_1 \partial x_n}\\
\vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} &amp;amp; \ldots &amp;amp; \frac{\partial^2 f}{\partial x_n^2}\\
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;Given that \(\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}\), the Hessian matrix is symmetric (this helps with debugging and can reduce computation when constructing the matrix).&lt;/p&gt;

&lt;h4 id=&quot;jacobian&quot;&gt;Jacobian&lt;/h4&gt;

&lt;p&gt;The Jacobian matrix of a function with vector outputs is the partial derivative
of each function output dimension taken with respect to the partial derivative of each function input dimension. Let us examine a contrived function, \(f : \mathbb{R}^3 \to \mathbb{R}^2\); \(f(.)\) might represent a vector flow for
points in three dimensional Cartesian space. The Jacobian of \(f(.)\) is then:&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\frac{\partial f}{\partial \mathbf{x}} = \begin{bmatrix} 
\frac{\partial f_1}{\partial x_1} &amp;amp; \frac{\partial f_1}{\partial x_2} &amp;amp; \frac{\partial f_1}{\partial x_3} \\
\frac{\partial f_2}{\partial x_1} &amp;amp; \frac{\partial f_2}{\partial x_2} &amp;amp; \frac{\partial f_2}{\partial x_3} 
\end{bmatrix} 
\end{equation*}&lt;/p&gt;

&lt;p&gt;Just like the standard derivative, the Jacobian matrix gives the instantaneous change in \(f(.)\) at \(\mathbf{x}\). &lt;/p&gt;

&lt;h3 id=&quot;least-squares-problems&quot;&gt;Least squares problems&lt;/h3&gt;

&lt;p&gt;Least squares problems are ubiquitous in science and engineering applications.
Solving a least squares problem finds a line (or plane or hyperplane,
in higher dimensions) that minimizes the sum of the squared distance from a
set of points to the line/plane/hyperplane. Another way of saying this is
that least squares seeks to minimize the residual error, i.e., \(||\mathbf{A}\mathbf{x} - \mathbf{b}||\). &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;A depiction of least squares as a mechanical device. Springs are attached from each point to a rod. The amount of force increases quadratically with the distance of each point to the rod.&lt;/caption&gt;
&lt;img src=&quot;http://www.datavis.ca/papers/koln/figs/grsp2.gif&quot; alt=&quot;A depiction of least squares as a mechanical device. Springs are attached from each point to a rod. The amount of force increases quadratically with the distance of each point to the rod.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;Clearly, if \(\mathbf{A}\) is square and non-singular, the solution is \(\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}\). What if \(\mathbf{A} \in \mathbb{R}^{m \times n}\), where \(m \neq n\)? Assume that each row of \(\mathbf{A}\) is linearly independent (i.e., \(\mathbf{A}\) has full row rank) for now. If \(m \gt n\), then there are more
equations than variables and the problem is &lt;em&gt;overdetermined&lt;/em&gt;; we expect \(||\mathbf{Ax} - \mathbf{b}||\) to be nonzero. If \(m \lt n\), then there are more
variables than unknowns and the problem is &lt;em&gt;underdetermined&lt;/em&gt;; we expect there
to be multiple (infinite) assignments to \(\mathbf{x}\) that make \(||\mathbf{Ax} - \mathbf{b}|| = 0\).&lt;/p&gt;

&lt;h5 id=&quot;the-moore-penrose-pseudo-inverse&quot;&gt;The Moore-Penrose pseudo-inverse&lt;/h5&gt;

&lt;p&gt;&lt;em&gt;Two&lt;/em&gt; least squares problems become evident: (1) select \(\mathbf{x}\) such that  \(||\mathbf{Ax} - \mathbf{b}||\) is minimized. If \(||\mathbf{Ax} - \mathbf{b}|| = 0\), then (2) select the solution that minimizes \(||\mathbf{x}||\). Both solutions can be obtained using the Moore-Penrose pseudo-inverse (which is denoted using the operator \(\ ^+\), which has some of the properties of the inverse (I&amp;#39;ll only list a few below):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;\(\mathbf{A}\mathbf{A}^+ = \mathbf{I}\)&lt;/li&gt;
&lt;li&gt;\(\mathbf{A}^+\mathbf{A} = \mathbf{I}\)&lt;/li&gt;
&lt;li&gt;If \(\mathbf{A}\) is invertible, then \(\mathbf{A}^+ = \mathbf{A}^{-1}\).&lt;/li&gt;
&lt;li&gt;\(\mathbf{0}^{-1} = \mathbf{0}^\mathsf{T}\).&lt;/li&gt;
&lt;li&gt;\((\mathbf{A}^+)^+ = \mathbf{A}\).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The &lt;em&gt;left pseudo-inverse&lt;/em&gt; follows:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{A}^+ = {(\mathbf{A}^\mathsf{T}\mathbf{A})}^{-1}\mathbf{A}^\mathsf{T}
\end{equation}&lt;/p&gt;

&lt;p&gt;This pseudo-inverse constitutes a &lt;em&gt;left inverse&lt;/em&gt; (because \(\mathbf{A}^+\mathbf{A} = \mathbf{I}\)), while the &lt;em&gt;right pseudo-inverse&lt;/em&gt; (below)
constitutes a &lt;em&gt;right inverse&lt;/em&gt; (because \(\mathbf{A}\mathbf{A}^+ = \mathbf{I}\)). See &lt;a href=&quot;https://en.wikipedia.org/wiki/Inverse_function#Left_and_right_inverses&quot;&gt;Wikipedia&lt;/a&gt; for an accessible description of left and right inverses. &lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{A}^+ = \mathbf{A}^\mathsf{T}{(\mathbf{A}\mathbf{A}^\textsf{T})}^{-1}
\end{equation}&lt;/p&gt;

&lt;h5 id=&quot;solving-least-squares-problems-using-the-singular-value-decomposition&quot;&gt;Solving least squares problems using the singular value decomposition&lt;/h5&gt;

&lt;p&gt;Still working under the assumption that \(\mathbf{A}\) is of full row rank,
we can also use the singular value decomposition to solve least squares 
problems.&lt;/p&gt;

&lt;p&gt;Recall that the singular value decomposition of \(\mathbf{A} = \mathbf{U\Sigma}mathbf{V}^{\mathsf{T}}\). \(\mathbf{U}\) and \(\mathbf{V}\) are both orthogonal, which means that \(\mathbf{U}^{-1} = \mathbf{U}^{\mathsf{T}}\) and \(\mathbf{V}^{-1} = \mathbf{V}^{\mathsf{T}}\). From the identity above, \(\mathbf{A}^{-1} = \mathbf{V}\Sigma^{-1}\mathbf{U}^{\mathsf{T}}\). For non-square \(\mathbf{A}\), \(\mathbf{A}^+ = \mathbf{V}\mathbf{\Sigma}^+{\mathbf{U}}^\mathsf{T}\), where \(\Sigma^+\) will be defined as follows:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\Sigma_{ij}^+ \leftarrow \begin{cases}\frac{1}{\Sigma_{ij}} &amp;amp; \textrm{if } i = j, \\
0 &amp;amp; \textrm{if } i \ne j. \end{cases}
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;regularization&quot;&gt;Regularization&lt;/h4&gt;

&lt;p&gt;It&amp;#39;s too much to ask that \(\mathbf{A}\), \(\mathbf{A}^\mathsf{T}\mathbf{A}\), or \(\mathbf{A}\mathbf{A}^\textsf{T}\) be always invertible. Even if you 
&lt;em&gt;know&lt;/em&gt; the matrix is invertible (based on some theoretical properties), this does not mean that \(\mathbf{A}\) et al. will be well conditioned. And singular
matrices arise in least squares applications when two or more equations are 
linearly dependent (in other words, regularly).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Regularization&lt;/em&gt; allows one to compute least squares solutions with numerical
robustness.&lt;/p&gt;

&lt;h5 id=&quot;using-regularization-with-the-singular-value-decomposition&quot;&gt;Using regularization with the singular value decomposition&lt;/h5&gt;

&lt;p&gt;The formula for computing \(\mathbf{\Sigma}^+\) can be updated for a numerically robust solution:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\Sigma_{ij}^+ \leftarrow \begin{cases}\frac{1}{\Sigma_{ij}} &amp;amp; \textrm{if } i = j \textrm{ and } \Sigma_{ij} &amp;gt; \epsilon, \\
0 &amp;amp; \textrm{if } i \ne j \textrm{ or } \Sigma_{ij} \le \epsilon. \end{cases}
\end{equation}&lt;/p&gt;

&lt;p&gt;\(\epsilon\) can be set using &lt;em&gt;a priori&lt;/em&gt; knowledge or using the strategy
used in &lt;a href=&quot;http://www.netlib.org/lapack/&quot;&gt;LAPACK&lt;/a&gt;&amp;#39;s library:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\epsilon = \epsilon_{\textrm{mach}} \cdot \max{(m,n)} \cdot \max_{i,j} \Sigma_{ij} 
\end{equation} &lt;/p&gt;

&lt;h5 id=&quot;tikhonov-regularization&quot;&gt;Tikhonov regularization&lt;/h5&gt;

&lt;p&gt;A very simple form of regularization is known as &lt;em&gt;Tikhonov Regularization&lt;/em&gt;
and, in statistics applications, as &lt;em&gt;Ridge Regression&lt;/em&gt;. Consider the system of linear equations:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{A}\mathbf{x} = \mathbf{b}
\end{equation}&lt;/p&gt;

&lt;p&gt;for \(\mathbf{A} \in \mathbb{R}^{n \times n}\) and \(\mathbf{x}, \mathbf{b} \in \mathbb{R}^{n \times 1}\). The
matrix \(\mathbf{A} + \epsilon\mathbf{I}\), where \(\mathbf{I}\) is the identity matrix will always be invertible for sufficiently large \(\epsilon \ge 0\). Tikhonov Regularization solves the nearby system \((\mathbf{A} + \epsilon\mathbf{I})\mathbf{x} = \mathbf{b}\) for \(\mathbf{x}\).&lt;/p&gt;

&lt;p&gt;The obvious question at this point: what is the optimal value of \(\epsilon\)? If \(\epsilon\) is too small, the relevant factorization algorithm (e.g., Cholesky factorization, LU factorization) will fail with an error (at best). If \(epsilon\) is too large, the residual error \(||\mathbf{Ax} - \mathbf{b}||\) will be greater than is necessary. The optimal value of \(\epsilon\) can be
computed with a singular value decomposition, but- one might as well just use the result from the SVD to compute a regularization solution (as described above)- if one goes down this computationally expensive route.&lt;/p&gt;

&lt;p&gt;An effective \(\epsilon\) will keep the condition number (the ratio of
largest to smallest singular values) relatively small: \(10^6\) or so. An 
example of a quick and dirty way to compute \(\epsilon\) is:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\epsilon = \epsilon_{\textrm{mach}} \cdot \max{(m,n)} \cdot ||\mathbf{A}||_\infty
\end{equation} &lt;/p&gt;

&lt;p&gt;where \(\epsilon_{\textrm{mach}}\) is machine epsilon and&lt;/p&gt;

&lt;p&gt;\begin{equation}
||\mathbf{A}||_\infty = \max_{i \in m, j \in n} |A_{ij}|
\end{equation} &lt;/p&gt;
</description>
        
          <description>&lt;p&gt;This material is not meant to replace a proper course in linear algebra, where
important concepts like vector spaces, eigenvalues/eigenvectors, and
spans are defined. My intent is to give you a practical guide to concepts
in matrix arithmetic and numerical linear algebra that I have found useful. &lt;/p&gt;
</description>
        
        <pubDate>Fri, 18 Dec 2015 00:00:00 -0500</pubDate>
        <link>/robotics-course-materials/robotics-course-materials/blog/linear-algebra/</link>
        <guid isPermaLink="true">/robotics-course-materials/robotics-course-materials/blog/linear-algebra/</guid>
        
        
      </item>
      
    
      
      <item>
        <title>C++ overview and OpenSceneGraph introduction</title>
        <description>&lt;p&gt;&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;&lt;/span&gt;
This learning module will provide an overview of C++, targeted toward
those with some background in Java or C.  &lt;/p&gt;

&lt;h2 id=&quot;overview-of-c&quot;&gt;Overview of C++&lt;/h2&gt;

&lt;h3 id=&quot;c-data-types&quot;&gt;C++ data types&lt;/h3&gt;

&lt;p&gt;See &lt;a href=&quot;http://en.cppreference.com/w/cpp/language/types&quot;&gt;this page&lt;/a&gt; for a
description of all C++ primitive data types, including concepts like minimum and maximum numbers, not-a-number,
and infinity.  The most commonly used types are
&lt;code&gt;void&lt;/code&gt;, &lt;code&gt;bool&lt;/code&gt;, &lt;code&gt;char&lt;/code&gt;, &lt;code&gt;int&lt;/code&gt;, &lt;code&gt;unsigned&lt;/code&gt;, &lt;code&gt;long&lt;/code&gt;, &lt;code&gt;float&lt;/code&gt;, and &lt;code&gt;double&lt;/code&gt;.
Since C++ is &amp;quot;close to the metal&amp;quot;, like C, it can help you to know the
number of bits used for each representation, &lt;em&gt;which can change depending on
machine architecture&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;some-differences-between-c-and-java&quot;&gt;Some differences between C++ and Java&lt;/h3&gt;

&lt;p&gt;Notes below will be useful even to those programmers without a background in
Java.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;C++ uses &lt;code&gt;bool&lt;/code&gt; for a Boolean type (Java calls this &lt;code&gt;boolean&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Java uses &lt;code&gt;System.out.println&lt;/code&gt; for output to &lt;code&gt;stdout&lt;/code&gt;. C++ uses &lt;code&gt;std::cout&lt;/code&gt;, the &lt;code&gt;&amp;lt;&amp;lt;&lt;/code&gt; operator, and &lt;code&gt;std::endl&lt;/code&gt;. The Java statement &lt;code&gt;System.out.println(&amp;quot;Hello world!&amp;quot;);&lt;/code&gt; would be &lt;code&gt;std::cout &amp;lt;&amp;lt; &amp;quot;Hello world!&amp;quot; &amp;lt;&amp;lt; std::endl;&lt;/code&gt; in C++.&lt;/li&gt;
&lt;li&gt;Java forces you to allocate non-primitive types on the heap, where C++ allows you to allocate non-primitive types on the stack (the latter is faster and more amenable to real-time performance). &lt;/li&gt;
&lt;li&gt;Java automatically de-allocates memory (using relatively slow garbage collection).&lt;/li&gt;
&lt;li&gt;Array allocation is slightly different. Arrays are allocated in Java like &lt;code&gt;int[] array = new int[20]&lt;/code&gt;. Arrays are allocated in C++ like &lt;code&gt;int* array = new int[20]&lt;/code&gt;. &lt;/li&gt;
&lt;li&gt;In Java, a member function is defined like &lt;code&gt;public void tabulateScores()&lt;/code&gt; while the function would be declared in C++ like &lt;code&gt;public: void tabulateScores()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;All primitive types (&lt;code&gt;int&lt;/code&gt;, &lt;code&gt;float&lt;/code&gt;, etc.) are &lt;a href=&quot;http://courses.washington.edu/css342/zander/css332/passby.html&quot;&gt;passed by value&lt;/a&gt; to functions and all non-primitive types are &lt;a href=&quot;http://courses.washington.edu/css342/zander/css332/passby.html&quot;&gt;passed by reference&lt;/a&gt;. C++ gives the option to pass any type by reference or by value to a function.&lt;/li&gt;
&lt;li&gt;C++ requires you to &lt;a href=&quot;http://stackoverflow.com/questions/4757565/c-forward-declaration&quot;&gt;declare&lt;/a&gt; function prototypes and classes when you refer to them (before they have been &lt;em&gt;defined&lt;/em&gt;- fleshed out). If you refer to a class before it has been defined, C++ requires you to do a &lt;a href=&quot;http://stackoverflow.com/questions/4757565/c-forward-declaration&quot;&gt;forward declaration&lt;/a&gt;. Java was smart to avoid declarations, in my opinion. &lt;/li&gt;
&lt;li&gt;C++ does not have &lt;em&gt;interfaces&lt;/em&gt; but it does have &lt;a href=&quot;https://en.wikipedia.org/wiki/Virtual_function&quot;&gt;pure virtual functions&lt;/a&gt;,
which serve an identical purpose.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some resources for Java programmers to learn C++:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.horstmann.com/ccj2/ccjapp3.html&quot;&gt;Moving from Java to C++&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://cs.brown.edu/courses/cs123/docs/java_to_cpp.shtml&quot;&gt;Java to C++ Transition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;object-oriented-programming-in-c&quot;&gt;Object-oriented programming in C++&lt;/h3&gt;

&lt;p&gt;Object oriented programming (OOP) is a programming model centered around data and
the functions used to operate on that data rather than &lt;em&gt;procedural programming languages&lt;/em&gt; (like C) that focus on decomposing a task into subroutines (procedures). A tutorial to OOP in C++ can be found &lt;a href=&quot;http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-088-introduction-to-c-memory-management-and-c-object-oriented-programming-january-iap-2010/lecture-notes/MIT6_088IAP10_lec04.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;memory-allocation-and-shared-pointers&quot;&gt;Memory allocation and shared pointers&lt;/h3&gt;

&lt;p&gt;One price you pay for the additional speed and control that C++ offers is
the need to manage heap memory allocation and deallocation. Memory is allocated
from the heap using the &lt;code&gt;new&lt;/code&gt; operator:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;int* x;       // x is a pointer
x = new int;  // allocated memory for x on the heap
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and memory must be deallocated from the heap using the &lt;code&gt;delete&lt;/code&gt; operator:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;delete [] x;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For every &lt;code&gt;new&lt;/code&gt; in your code, there should be a matching &lt;code&gt;delete&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&quot;shared-pointers&quot;&gt;Shared pointers&lt;/h4&gt;

&lt;p&gt;Shared pointers provide automatic memory deallocation. &lt;em&gt;I suggest using them
instead of regular pointers for memory allocation/deallocation.&lt;/em&gt; The idea
is simple- when no more references point to a block of memory, the block 
is deallocated- though a few caveats exist. Shared pointers work like this:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;shared_ptr&amp;lt;int&amp;gt; x;              // x is a shared pointer to an int
x = shared_ptr&amp;lt;int&amp;gt;(new int);   // allocated memory
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;There no longer needs to be a matching &lt;code&gt;delete&lt;/code&gt; statement. The advantages
of shared pointers over garbage collection are that the former is considerably
faster and that memory is reclaimed as soon as possible. The disadvantage is
that circular pointer references must be explicitly managed by the programmer
or memory leaks will occur.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Using the following example class definitions will result in a memory leak&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;class B; // the forward reference is necessary

class A
{
  shared_ptr&amp;lt;B&amp;gt; b;
};

class B
{
  shared_ptr&amp;lt;A&amp;gt; a;
};
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This situation is fixable using a &lt;a href=&quot;http://www.boost.org/doc/libs/1_58_0/libs/smart_ptr/weak_ptr.htm&quot;&gt;weak pointer&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;class B; // the forward reference is necessary

class A
{
  shared_ptr&amp;lt;B&amp;gt; b;
};

class B
{
  weak_ptr&amp;lt;A&amp;gt; a;  // the weak pointer breaks the circular reference
};
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&quot;passing-by-reference-and-passing-by-value&quot;&gt;Passing by reference and passing by value&lt;/h3&gt;

&lt;p&gt;(&lt;em&gt;ED: I have seen this advice somewhere but cannot locate it at the present moment. I will cite my source when I find it again.&lt;/em&gt;)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pass variables by reference when the function is to modify the variable. 
As a matter of fact, indicating that the variable is passed by reference
&lt;em&gt;and without the &lt;code&gt;const&lt;/code&gt;&lt;/em&gt; keyword indicates to the caller that the function is &lt;em&gt;expected&lt;/em&gt; to modify the variable.&lt;/li&gt;
&lt;li&gt;Else, for primitive types, pass by value &lt;/li&gt;
&lt;li&gt;Pass objects by reference and use the &lt;code&gt;const&lt;/code&gt; modifier when the object uses more memory than a pointer (64-bits on most systems) and the function is not expected to change the object (e.g., &lt;code&gt;void sum_inertias(const SpatialRBInertiad&amp;amp; J)&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Pass objects by value when the object uses less memory than a pointer and the function is not expected to modify the object &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;compiling-linking-c-on-unix-type-systems&quot;&gt;Compiling/linking C++ on Unix-type systems&lt;/h3&gt;

&lt;p&gt;Whether producing an executable file or a software library, C++ requires
two processes: &lt;em&gt;compiling&lt;/em&gt; the C++ source code into machine code 
(&amp;quot;object files&amp;quot;) and &lt;em&gt;linking&lt;/em&gt; the object files together (which resolves 
symbolic references to functions and data). A description of the compilation
and linking processes is &lt;a href=&quot;http://stackoverflow.com/questions/6264249/how-does-the-compilation-linking-process-work&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As a very simple example,&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;g++ -c hello.cpp -o hello.o
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;compiles &lt;code&gt;hello.cpp&lt;/code&gt; to produce &lt;code&gt;hello.o&lt;/code&gt; and&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;g++ hello.o -o hello
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;links &lt;code&gt;hello.o&lt;/code&gt; with the C++ standard libraries to produce the executable &lt;code&gt;hello&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I recommend getting and learning &lt;a href=&quot;http://cmake.org&quot;&gt;CMake&lt;/a&gt; to build your projects, which can take care of the compiling and linking process for you automatically. Otherwise, you have to compile your source files manually, forcing you to remember all of the arcane command line options, and then manually link your objects together. One warning: &lt;a href=&quot;http://stackoverflow.com/questions/45135/why-does-the-order-in-which-libraries-are-linked-sometimes-cause-errors-in-gcc&quot;&gt;the linker (g++) is sensitive to the order that libraries and object files are specified on the command line on Linux systems&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;newer-language-features&quot;&gt;Newer language features&lt;/h3&gt;

&lt;p&gt;C++ continues to support more and more features over time. The language&amp;#39;s evolution reminds me of this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://images.knifecenter.com/knifecenter/wenger/images/WR16999a.jpg&quot; alt=&quot;Big swiss army knife&quot;&gt;&lt;/p&gt;

&lt;p&gt;because few language features are ever removed. The 
&lt;a href=&quot;http://www.cplusplus.com/reference/stl/&quot;&gt;C++ standard template library&lt;/a&gt; contains a number of useful data structures- including vectors, linked lists, queues, stacks, sets, and maps- and algorithms (finding maximum elements, binary search, sorting, and more). You will be able to increase your programming proficiency in C++ many fold when you understand the concept of &lt;a href=&quot;http://www.cs.northwestern.edu/%7Eriesbeck/programming/c++/stl-iterators.html&quot;&gt;iterators&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A staging ground for many C++ algorithms that often make their way into
the language is &lt;a href=&quot;http://www.boost.org&quot;&gt;Boost&lt;/a&gt;. This functionality goes part
of the way toward replicating the utility of other languages&amp;#39; standard 
libraries (Python and Java in particular). &lt;/p&gt;

&lt;h3 id=&quot;templates&quot;&gt;Templates&lt;/h3&gt;

&lt;p&gt;Templates allow us to avoid code like this:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;void swap(int&amp;amp; x, int&amp;amp; y)
{
  int tmp = x;
  x = y;
  y = tmp;
}

void swap(float&amp;amp; x, float&amp;amp; y)
{
  float tmp = x;
  x = y;
  y = tmp;
}

.
.
.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can do this instead:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;template &amp;lt;typename T&amp;gt;
void swap(T&amp;amp; x, T&amp;amp; y)
{
  T tmp = x;
  x = y;
  y = tmp;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This saves typing and, more importantly, reduces possibility of bugs from
copy and paste (a great way to introduce bugs in programming). On the downside,
templates make code a little harder to read, make it slower to compile,
and tends to generate really hard to read compiler error messages for syntax errors (&lt;a href=&quot;https://isocpp.org/wiki/faq/templates#template-error-msgs&quot;&gt;see this part of the C++ FAQ for a fix&lt;/a&gt;). Learning templates well will help you understand the
Boost, the STL, and will give you the ability to read the majority of C++ code.&lt;/p&gt;

&lt;h3 id=&quot;exceptions&quot;&gt;Exceptions&lt;/h3&gt;

&lt;p&gt;Before exceptions, programmers would check for errors like this:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;FILE* fp = fopen(&amp;quot;/tmp/dat&amp;quot;, &amp;quot;w&amp;quot;);
if (!fp)
{
  std::cerr &amp;lt;&amp;lt; &amp;quot;Unable to open file!&amp;quot; &amp;lt;&amp;lt; std::endl;
  return false;
}

...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using exceptions we check for errors like this:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;try
{
  fp = open(&amp;quot;/tmp/dat&amp;quot;);
}
catch (IOException e)
{
  std::cerr &amp;lt;&amp;lt; &amp;quot;Unable to open file!&amp;quot; &amp;lt;&amp;lt; std::endl;
  return false;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;One advantage is that if we don&amp;#39;t care about the error at this level- it&amp;#39;s
apparent that we already signal to the calling function that there was a 
problem by 
the &lt;code&gt;return false&lt;/code&gt; statement- then we can keep our code very neat by doing
this instead:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;fp = open(&amp;quot;/tmp/dat&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now if we do not &amp;quot;catch&amp;quot; the exception, the function above is responsible
for catching it, on up the &lt;a href=&quot;https://en.wikipedia.org/wiki/Call_stack&quot;&gt;call stack&lt;/a&gt;, until- if the &lt;code&gt;main&lt;/code&gt; function does not catch it- the exception will cause
the program to terminate with an error.&lt;/p&gt;

&lt;p&gt;A commentor on &lt;a href=&quot;http://stackoverflow.com/questions/196522/in-c-what-are-the-benefits-of-using-exceptions-and-try-catch-instead-of-just&quot;&gt;Stack overflow&lt;/a&gt; indicates two benefits:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;They can&amp;#39;t be ignored: you must deal with them at some level or they will terminate your program. If you do not explicitly check for the error code, it is lost.&lt;/li&gt;
&lt;li&gt;They &lt;em&gt;can&lt;/em&gt; be ignored: if you explicitly wish to ignore an exception, it
will propagate up to higher levels until some piece of code does handle it.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This same Stack overflow thread has many more viewpoints on why exceptions are useful. No commentor argues that checking for error codes is a better solution.&lt;/p&gt;

&lt;h3 id=&quot;programming-debugging-advice&quot;&gt;Programming / debugging advice&lt;/h3&gt;

&lt;p&gt;Some general programming advice (beyond C++):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;readability&lt;/strong&gt;: One of your primary goals when programming is to carefully 
guide another programmer through your code. Even if you expect to be the
only person to ever see your code, you will be that other programmer in six months.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;minimize cognitive load&lt;/strong&gt;: Toward keeping your code readable, minimize the
cognitive load. Name variables and functions descriptively (&lt;code&gt;num_iterations&lt;/code&gt; instead of &lt;code&gt;n&lt;/code&gt;, &lt;code&gt;calc_inertias(.)&lt;/code&gt; instead of &lt;code&gt;compute(.)&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;use STL containers instead of arrays&lt;/strong&gt;: Arrays do no range checking and
the correct size must be allocated at runtime; accidentally overwriting memory
outside of the array is a common bug &lt;a href=&quot;https://en.wikipedia.org/wiki/Buffer_overflow&quot;&gt;and is a common vector for security attacks&lt;/a&gt;. I prefer the &lt;a href=&quot;http://www.cplusplus.com/reference/vector/vector/&quot;&gt;STL vector&lt;/a&gt;, which can be accessed like an array (e.g., &lt;code&gt;x[5] = 3&lt;/code&gt;), can be queried for its size, automatically deallocates memory when the variable goes out of scope, performs range checking, and can increase its capacity automatically. &lt;a href=&quot;http://cs.brown.edu/%7Ejak/proglang/cpp/stltut/tut.html&quot;&gt;Here&lt;/a&gt; is a nice tutorial on the STL (Standard Template Library). &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;put reusable code in functions and keep functions small&lt;/strong&gt;: longer
functions are more likely to have defects (see a dissenting viewpoint plus several that backup my point of view &lt;a href=&quot;http://c2.com/cgi/wiki?LongFunctionHeresy&quot;&gt;here&lt;/a&gt;). The longer your function is, than say 50 lines of code, the more you should consider breaking it into multiple functions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;http://stackoverflow.com/questions/14041453/why-are-preprocessor-macros-evil-and-what-are-the-alternatives&quot;&gt;beware of macros&lt;/a&gt;&lt;/strong&gt; &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;write the comments first&lt;/strong&gt;: This is a strategy I use when programming. Writing the comments first helps you focus on organizing the logic. Filling in the code from the comments is pretty easy when you know the language syntax.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;address the first compiler errors first&lt;/strong&gt;: Many errors found by the C++ compiler will disappear after you correct the first in a list of errors.&lt;br&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;fix all compiler warnings&lt;/strong&gt;: C++ compilers tend to generate warnings in places where compilers for other languages would generate errors. Take compiler warnings seriously- treat them as errors. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;write &lt;a href=&quot;https://en.wikipedia.org/wiki/Unit_testing&quot;&gt;unit tests&lt;/a&gt;&lt;/strong&gt;: Unit tests allow you to catch problems in a function while you remember the ins and outs of that function as opposed to six months down the road when you locate a bug in the function.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;use a debugger&lt;/strong&gt;: see below&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;c-tools&quot;&gt;C++ tools&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;git / version control&lt;/strong&gt;: While not a C++ tool &lt;em&gt;per se&lt;/em&gt;, use version control to track your changes. Advanced features of version control even allow you to, as examples: run unit tests, run regression tests, and build binary releases
upon committing code.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;gdb / lldb&lt;/strong&gt;: Debugging using &lt;code&gt;printf&lt;/code&gt; (or its variants among programming languages) is usually an order of magnitude faster than using a debugger. Learn at least the main features of a debugger. A good tutorial on gdb is found &lt;a href=&quot;http://www.unknownroad.com/rtfm/gdbtut/&quot;&gt;here&lt;/a&gt;. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;valgrind&lt;/strong&gt;: If you have a bug that you are having difficulty locating using gdb, &lt;a href=&quot;http://valgrind.org&quot;&gt;valgrind&lt;/a&gt; should be your next stop. Valgrind can locate problems like illegal memory reads and writes that gdb will not catch. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;performance tools&lt;/strong&gt;: Do not &lt;a href=&quot;https://shreevatsa.wordpress.com/2008/05/16/premature-optimization-is-the-root-of-all-evil/&quot;&gt;prematurely optimize&lt;/a&gt;: you will find that your intuition about the time sinks in your software are often wrong anyway. Use a &lt;em&gt;profiler&lt;/em&gt;, my favorite on Linux is currently &lt;a href=&quot;https://github.com/gperftools/gperftools&quot;&gt;google-perftools&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;additional-reference-materials&quot;&gt;Additional reference materials&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.parashift.com/c++-faq/&quot;&gt;C++ FAQ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Google&lt;/li&gt;
&lt;/ul&gt;

&lt;hr&gt;

&lt;h2 id=&quot;overview-of-openscenegraph&quot;&gt;Overview of OpenSceneGraph&lt;/h2&gt;

&lt;p&gt;You have two clear options to program in 3D: &lt;a href=&quot;https://en.wikipedia.org/wiki/OpenGL&quot;&gt;OpenGL&lt;/a&gt;, which is a &lt;em&gt;state system&lt;/em&gt; (the rendering is completed determined
by state variables), and &lt;a href=&quot;https://en.wikipedia.org/wiki/Scene_graph&quot;&gt;scene graph-based systems&lt;/a&gt;, like &lt;a href=&quot;https://en.wikipedia.org/wiki/OpenSceneGraph&quot;&gt;OpenSceneGraph&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Open_Inventor&quot;&gt;Open Inventor&lt;/a&gt;, and &lt;a href=&quot;http://www.oracle.com/technetwork/articles/javase/index-jsp-138252.html&quot;&gt;Java 3D&lt;/a&gt;. The
earliest technology for viewing 3D content on the web, &lt;a href=&quot;https://en.wikipedia.org/wiki/VRML97&quot;&gt;VRML&lt;/a&gt;, is based on a scene graph representation (and this is a pretty good file format too).&lt;/p&gt;

&lt;p&gt;I will discuss the scene graph representation because it is intuitive to
understand- it fits well into the object-oriented paradigm, in particular-
and 3D rendering can be achieved with very little code. For example, this tiny bit of code renders many 3D models that you can view using mouse controls:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;// simple.cpp (Evan Drumwright)
#include &amp;lt;osgDB/ReadFile&amp;gt;
#include &amp;lt;osgViewer/Viewer&amp;gt;

int main(int argc, char** argv)
{
  if (argc &amp;lt; 2)
  {
    std::cerr &amp;lt;&amp;lt; &amp;quot;syntax: simple &amp;lt;filename&amp;gt;&amp;quot; &amp;lt;&amp;lt; std::endl;
    return -1;
  }
  osgViewer::Viewer viewer;
  viewer.setSceneData(osgDB::readNodeFile(argv[1]));
  return viewer.run();
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can build this program using &lt;a href=&quot;../../assets/other/simpleosg/CMakeLists.txt&quot;&gt;this&lt;/a&gt; CMake build file. You can then run the program on many 3D files. One example is this &lt;a href=&quot;http://scv.bu.edu/documentation/software-help/graphics-programming/osg_examples/materials/cessna.osg&quot;&gt;cessna airplane&lt;/a&gt;. Once you build the program, you run it like this: &lt;code&gt;simple cessna.osg&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;the-scene-graph&quot;&gt;The scene graph&lt;/h3&gt;

&lt;p&gt;A scene graph is a collection of nodes in a tree (or, more generally, a graph) structure. A node in the tree may have many children but only a single parent, with the effect of a parent applied to all its child nodes. An operation performed on a group automatically propagates its effect to all of its members. &lt;/p&gt;

&lt;p&gt;Associating a geometrical transformation matrix (which I will describe in a future learning module) at a node will apply the transformation (rotation, translation, scaling) to all nodes below it. Materials are applied The scene graph paradigm is 
particularly good for rendering and animating animals, humans, and robots.&lt;/p&gt;

&lt;p&gt;(Adapted from &lt;a href=&quot;https://en.wikipedia.org/wiki/Scene_graph&quot;&gt;this page&lt;/a&gt;). &lt;/p&gt;

&lt;p&gt;An example scene graph for a virtual human is depicted below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/scene_graph.png&quot; alt=&quot;example virtual human scene graph&quot;&gt;&lt;/p&gt;

&lt;p&gt;The types of nodes in the graph are described below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://trac.openscenegraph.org/documentation/OpenSceneGraphReferenceDocs/a00910.html&quot;&gt;Transform&lt;/a&gt;: A group node for which all children are transformed by a 4x4 (homogeneous) &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformation_matrix&quot;&gt;transformation matrix&lt;/a&gt;- again, I will discuss this in a future learning module.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://trac.openscenegraph.org/documentation/OpenSceneGraphReferenceDocs/a00357.html&quot;&gt;Group&lt;/a&gt;: A generic node for grouping children together&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://trac.openscenegraph.org/documentation/OpenSceneGraphReferenceDocs/a00775.html&quot;&gt;Sphere&lt;/a&gt;: A geometric primitive node for rendering a sphere&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://trac.openscenegraph.org/documentation/OpenSceneGraphReferenceDocs/a00479.html&quot;&gt;Material&lt;/a&gt;: An object for setting the color properties (color, shininess, transparency) of an object&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;simple-animation&quot;&gt;Simple animation&lt;/h3&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;#include &amp;lt;osgDB/ReadFile&amp;gt;
#include &amp;lt;osgViewer/Viewer&amp;gt;
#include &amp;lt;osg/MatrixTransform&amp;gt;
#include &amp;lt;osgGA/TrackballManipulator&amp;gt;
#include &amp;lt;osgGA/StateSetManipulator&amp;gt;
#include &amp;lt;unistd.h&amp;gt;

int main(int argc, char** argv)
{
  if (argc &amp;lt; 2)
  {
    std::cerr &amp;lt;&amp;lt; &amp;quot;syntax: anim &amp;lt;filename&amp;gt;&amp;quot; &amp;lt;&amp;lt; std::endl;
    return -1;
  }

  // create the viewer, as before, but now we need to add 
  // a trackball manipulator
  osgViewer::Viewer viewer;

  // create a transform
  osg::MatrixTransform* group = new osg::MatrixTransform;
  viewer.setCameraManipulator(new osgGA::TrackballManipulator());

  // read the file and add it to the transform group
  group-&amp;gt;addChild(osgDB::readNodeFile(argv[1]));

  // point the viewer to the scene graph
  viewer.setSceneData(group);
  viewer.realize();

  // set the angle (in radians)
  const double ANGLE = M_PI/180.0;
  unsigned i = 0;

  // loop until done
  while (true)
  {
    if (viewer.done()) break;

    // render a frame
    viewer.frame();

    // update the transform to do a rotation around axis .577 .577 .577
    osg::Matrixd T;
    T.makeRotate(ANGLE*i, 0.57735, 0.57735, 0.57735);
    group-&amp;gt;setMatrix(T);
    i++;

    // sleep a little (10000 microseconds = 10ms = 100 frames per second)
    usleep(10000);
  }

  return 0;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This code fragment covers 90% of animation cases: simply update a matrix transform
and then render a frame (using &lt;code&gt;frame()&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;You can build this program using &lt;a href=&quot;../../assets/other/animosg/CMakeLists.txt&quot;&gt;this&lt;/a&gt; CMake build file. Again, you can then run the program on many 3D files. Once you build the program, you run it like this: &lt;code&gt;anim cessna.osg&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;One important note about animation&lt;/strong&gt;: if your code between calls to &lt;code&gt;frame()&lt;/code&gt; takes too long, then the frame rate will naturally suffer.&lt;/p&gt;

&lt;h3 id=&quot;3d-file-formats-and-tools&quot;&gt;3D file formats and tools&lt;/h3&gt;

&lt;p&gt;To do anything cool with 3D, you need models, and models require considerable
time and expertise to create. You can search for models using Google (try
&amp;quot;3D model spaceship&amp;quot;, for example), convert between models using tools, or
even try building your own or modifying someone else&amp;#39;s. Some useful tools are linked to below: &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Wavefront_.obj_file&quot;&gt;Wavefont OBJ&lt;/a&gt; is a file format that is extremely simple both to parse and to write. I prefer it less
than other file formats when colors should be applied, because these &amp;quot;materials&amp;quot; are stored outside of the file (all data cannot be stored in a single file). The file extension is &amp;quot;.obj&amp;quot;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/VRML&quot;&gt;VRML&lt;/a&gt; comes in two formats, both still
popular, &lt;a href=&quot;http://www.martinreddy.net/gfx/3d/VRML.spec&quot;&gt;VRML 1.0&lt;/a&gt; and &lt;a href=&quot;http://gun.teipir.gr/VRML-amgem/spec/index.html&quot;&gt;VRML 97 (also known as VRML 2.0)&lt;/a&gt;. The VRML 1.0 file extension is &amp;quot;.iv&amp;quot;; the VRML 2.0 file extensions are &amp;quot;.wrl&amp;quot; and
&amp;quot;.vrml&amp;quot; (less common). VRML 1.0 is easy to parse and write to; VRML 2.0 is
easier to write to. There exist tools for converting between VRML 1.0 and 2.0, but your mileage will vary.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.blender.org&quot;&gt;Blender&lt;/a&gt; is free, professional (or near professional grade) 3D modeling and rendering software. It can help you edit 3D models
and convert between various representations. The only problems: its interface is not very intuitive, the interface has changed multiple times in the 10+ years that I&amp;#39;ve used it, and the documentation has historically been poor.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;learning-more&quot;&gt;Learning more&lt;/h3&gt;

&lt;p&gt;There are a number of tutorials available for OpenSceneGraph &lt;a href=&quot;http://trac.openscenegraph.org/projects/osg//wiki/Support/Tutorials&quot;&gt;here&lt;/a&gt;. API documentation for OpenSceneGraph is located &lt;a href=&quot;http://trac.openscenegraph.org/documentation/OpenSceneGraphReferenceDocs/&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;
</description>
        
          <description>&lt;p&gt;&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;&lt;/span&gt;
This learning module will provide an overview of C++, targeted toward
those with some background in Java or C.  &lt;/p&gt;
</description>
        
        <pubDate>Fri, 18 Dec 2015 00:00:00 -0500</pubDate>
        <link>/robotics-course-materials/robotics-course-materials/blog/C++/</link>
        <guid isPermaLink="true">/robotics-course-materials/robotics-course-materials/blog/C++/</guid>
        
        
      </item>
      
    
      
      <item>
        <title>Forward kinematics</title>
        <description>&lt;p class=&quot;intro&quot;&gt;
Forward kinematics is the process of determining the location and orientation of a designated point on a robot as a function of the robot&#39;s configuration.
&lt;/p&gt;

&lt;h2 id=&quot;constraining-the-movement-of-rigid-bodies&quot;&gt;Constraining the movement of rigid bodies&lt;/h2&gt;

&lt;p&gt;In a previous learning module, we learned that rigid bodies have six
degrees of freedom: three for translation and three for rotation. Rigid
robots are composed of a number of these rigid bodies (which we call &amp;quot;links&amp;quot;)
and some constraints on the links&amp;#39; motion (called &amp;quot;joints&amp;quot;). Some common joints are depicted below: &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;A weld (or &#39;fixed joint&#39;) prevents both relative linear and angular motion between two rigid bodies.&lt;/caption&gt;
&lt;img src=&quot;https://www.accessiblesystems.com/images/bul/weld.jpg&quot; alt=&quot;A weld (or &#39;fixed joint&#39;) prevents both relative linear and angular motion between two rigid bodies.&quot; width=&quot;600&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;A ball and socket joint prevents relative linear motion between two bodies.&lt;/caption&gt;
&lt;img src=&quot;https://sjeyr7pe.wikispaces.com/file/view/BALLSOCK.JPG.jpeg/240766305/BALLSOCK.JPG.jpeg&quot; alt=&quot;A ball and socket joint prevents relative linear motion between two bodies.&quot; width=&quot;600&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Revolute joints prevents relative linear motion between two bodies (like a ball and socket joint) and allow relative rotational motion around only one axis. Revolute joints are common in robotics applications, particularly for robots that emulate biological organisms.&lt;/caption&gt;
&lt;img src=&quot;http://200.126.14.82/web/help/toolbox/physmod/mech/mech_building18.gif&quot; alt=&quot;Revolute joints prevents relative linear motion between two bodies (like a ball and socket joint) and allow relative rotational motion around only one axis. Revolute joints are common in robotics applications, particularly for robots that emulate biological organisms.&quot; width=&quot;600&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Universal joints prevent relative linear motion between two bodies (like a ball and socket joint) and allow relative rotational motion around two axes.&lt;/caption&gt;
&lt;img src=&quot;http://www.flamingriver.com/sysimg/34-dd-x-34-dd-forged-u-joint-fr2644.jpg&quot; alt=&quot;Universal joints prevent relative linear motion between two bodies (like a ball and socket joint) and allow relative rotational motion around two axes.&quot; width=&quot;600&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Prismatic joints eliminate relative rotational motion between two bodies and allow relative linear motion along one axis between the two bodies. Prismatic joints are particularly common in industrial robots.&lt;/caption&gt;
&lt;img src=&quot;http://ode-wiki.org/wiki/images/5/5f/PistonJoint.jpg&quot; alt=&quot;Prismatic joints eliminate relative rotational motion between two bodies and allow relative linear motion along one axis between the two bodies. Prismatic joints are particularly common in industrial robots.&quot; width=&quot;600&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h3 id=&quot;an-example-of-constrained-motion&quot;&gt;An example of constrained motion&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Foucault_pendulum&quot;&gt;The Foucault pendulum&lt;/a&gt; was
devised as an experiment to demonstrate the rotation of the Earth. It consists
of a bob suspended from the ceiling by a long wire.&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Animation of the Foucault pendulum. The Earth&#39;s rotation causes the trajectory of the pendulum to change over time, causing a spiral pattern to emerge on the floor.&lt;/caption&gt;
&lt;img src=&quot;http://i.imgur.com/2UoaPJe.gif&quot; alt=&quot;Animation of the Foucault pendulum. The Earth&#39;s rotation causes the trajectory of the pendulum to change over time, causing a spiral pattern to emerge on the floor.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;If we assume that the wire is massless, then the only rigid body is the
pendulum bob (since the ceiling is massive and effectively still, we do not
consider it). If the ceiling is located at the origin of the world \(\begin{bmatrix}0 &amp;amp; 0 &amp;amp; 0\end{bmatrix}^\mathsf{T}\), then the pendulum must satisfy the &lt;a href=&quot;https://en.wikipedia.org/wiki/Implicit_function&quot;&gt;implicit equation&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{x} + \ _w\mathbf{R}_i \mathbf{u}_i = \mathbf{0} \label{eqn:ball-and-socket}
\end{equation} &lt;/p&gt;

&lt;p&gt;where \(\mathbf{x}\) is the center-of-mass of the bob, \(_w\mathbf{R}_i\) is the orientation of the bob, and \(\mathbf{u}_i\) is a vector from the center-of-mass of the bob to the origin of the world. These variables are depicted in the figure below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/foucault-diagram.png&quot; alt=&quot;Depiction of variables for Foucault pendulum&quot;&gt;&lt;/p&gt;

&lt;h2 id=&quot;precise-definition-of-forward-kinematics&quot;&gt;Precise definition of forward kinematics&lt;/h2&gt;

&lt;p&gt;Forward kinematics can take multiple forms, depending on the output sought:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{f}(\mathbf{q}) \to \begin{cases} 
\mathbb{R}^2 &amp;amp; \textrm{ for Cartesian position in 2D} \\
\mathbb{R}^3 &amp;amp; \textrm{ for Cartesian position in 3D} \\
SO(2) &amp;amp; \textrm{ for planar orientation in 2D} \\
SO(3) &amp;amp; \textrm{ for 3D orientation} \\
SE(2) &amp;amp; \textrm{ for Cartesian position in 2D and planar orientation} \\
SE(3) &amp;amp; \textrm{ for Cartesian position in 2D and 3D orientation}
\end{cases}
\end{equation} &lt;/p&gt;

&lt;p&gt;The output of this function is in what is known as &lt;em&gt;operational space&lt;/em&gt;: we
use this generic term because of all of the possible mappings described above.
To make these mappings more concrete, the planar orientation SO(2) is just an angle.
Similarly, SE(2) is two real numbers plus an angle. These numbers are generally stacked into a vector like this:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{bmatrix}
x\\
y\\
\theta
\end{bmatrix}
\end{equation} &lt;/p&gt;

&lt;p&gt;&lt;em&gt;though they are equally valid stacked like this&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\begin{bmatrix}
\theta\\
x\\
y
\end{bmatrix}
\end{equation*} &lt;/p&gt;

&lt;p&gt;&lt;em&gt;or even like this&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\begin{bmatrix}
x\\
\theta\\
y
\end{bmatrix}
\end{equation*} &lt;/p&gt;

&lt;p&gt;Another possibility is using a \(3 \times 3\) homogeneous transformation
matrix:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{bmatrix}
c_{\theta} &amp;amp; -s_{\theta} &amp;amp; x \\
s_{\theta} &amp;amp; c_{\theta} &amp;amp; y \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(c_{\theta}\) means \(\cos{\theta}\) and \(s_{\theta}\) means \(\sin{\theta}\).&lt;/p&gt;

&lt;p&gt;We will see shortly how this representation can be useful.&lt;/p&gt;

&lt;p&gt;3D orientation can be described using any of the representations in &lt;a href=&quot;../poses3&quot;&gt;the learning module on 3D poses&lt;/a&gt;, include Euler angles, axis-angle, unit quaternions, or rotation matrices. The \(4 \times 4\) homogeneous transformation 
matrix is common:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{bmatrix}
r_{11} &amp;amp; r_{12} &amp;amp; r_{13} &amp;amp; x \\
r_{21} &amp;amp; r_{22} &amp;amp; r_{23} &amp;amp; y \\
r_{31} &amp;amp; r_{32} &amp;amp; r_{33} &amp;amp; z \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;an-example&quot;&gt;An example&lt;/h3&gt;

&lt;p&gt;Consider a double pendulum in 2D with two joint angles, \(q_1\) and \(q_2\), and link lengths \(\ell_1\) and \(\ell_2\). We will focus on the pendulum&amp;#39;s endpoint location and orientation. Because the pendulum is in 2D, its forward kinematics function maps to SE(2).&lt;/p&gt;

&lt;p&gt;The configuration of the pendulum in SE(2), using a \(3 \times 3\)
homogeneous transformation matrix, when \(q_1 = 0, q_2 = 0\) is:&lt;/p&gt;

&lt;p&gt;\begin{equation*} 
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; \ell_1 + \ell_2 \\
0 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
\end{equation*}&lt;/p&gt;

&lt;p&gt;What is the operational space configuration of the endpoint at generalized configuration \(q_1 = \frac{\pi}{2}, q_2 = \frac{-\pi}{2}\)?&lt;/p&gt;

&lt;p&gt;We will use the following reference frames.&lt;/p&gt;

&lt;p&gt;\begin{align}
_w\mathbf{T}_1 &amp;amp; \equiv 
\begin{bmatrix} 
c_1 &amp;amp; -s_1 &amp;amp; 0 \\
s_1 &amp;amp; c_1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}\\ 
_1\mathbf{T}_{1&amp;#39;} &amp;amp; \equiv
\begin{bmatrix} 
1 &amp;amp; 0 &amp;amp; \ell_1 \\
0 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}\\
_{1&amp;#39;}\mathbf{T}_{2} &amp;amp; \equiv
\begin{bmatrix} 
c_2 &amp;amp; -s_2 &amp;amp; 0 \\
s_2 &amp;amp; c_2 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}\\ 
_2\mathbf{T}_{2&amp;#39;} &amp;amp; \equiv
\begin{bmatrix} 
1 &amp;amp; 0 &amp;amp; \ell_2 \\
0 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
\end{align}&lt;/p&gt;

&lt;p&gt;where \(c_1, s_1, c_2, s_2\) denote \(\cos{q_1}, \sin{q_1}, \cos{q_2}, \sin{q_2}\), respectively.&lt;/p&gt;

&lt;p&gt;When we substitute \(q_1 = \frac{\pi}{2}, q_2 = \frac{-\pi}{2}\) and multiply \(_w\mathbf{T}_1 \cdot\ _1\mathbf{T}_{1&amp;#39;} \cdot _{1&amp;#39;}\mathbf{T}_2 \cdot _{2}\mathbf{T}_{2&amp;#39;}\), we arrive at:&lt;/p&gt;

&lt;p&gt;\begin{equation}
_w\mathbf{T}_{2&amp;#39;} = \begin{bmatrix}
c_{1}c_2 - s_1s_2 &amp;amp; -c_2s_1 - c_1s_2 &amp;amp; l_1 c_1 + l_2 (c_{1}c_2 - s_1s_2) \\
c_2s_1 + c_1s_2 &amp;amp; c_1 c_2 - s_1s_2 &amp;amp; l_1 s_1 + l_2 (c_2s_1 + c_1s_2) \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;which simplifies to (I used Macsyma to do the simplification):&lt;/p&gt;

&lt;p&gt;\begin{equation}
_w\mathbf{T}_{2&amp;#39;} = \begin{bmatrix}
c_{1+2} &amp;amp; -s_{1+2} &amp;amp; l_1 c_1 + l_2 c_{1+2} \\
s_{1+2} &amp;amp; c_{1+2} &amp;amp; l_1 s_1 + l_2 s_{1+2} \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;The upper left \(2 \times 2\) part of this matrix gives the orientation of the second link. The upper right \(2 \times 1\) part of this matrix gives the position of the end point of the second link.&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Depiction of the various frames in the example above for q1 = pi/4, q2 = -pi/4.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/two-link-frames.png&quot; alt=&quot;Depiction of the various frames in the example above for q1 = pi/4, q2 = -pi/4.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h3 id=&quot;denavit-hartenberg-parameters&quot;&gt;Denavit-Hartenberg parameters&lt;/h3&gt;

&lt;p&gt;If a robot manufacturer draws a figure providing the data above, you might
make a mistake computing the forward kinematics function (particularly as
we move from two joints to, say, seven). The &lt;a href=&quot;https://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters&quot;&gt;Denavit-Hartenberg parameters&lt;/a&gt; provide a more
compact, less error prone encoding using four parameters per reference frame.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=rA9tm0gTln8&quot;&gt;Here&lt;/a&gt; is a nice video depicting
Denavit-Hartenberg. &lt;/p&gt;

&lt;p&gt;&lt;em&gt;The nice part of D-H parameters&lt;/em&gt; is that a simple algorithm can compute
the forward kinematics for a robot given a small table of parameters. &lt;em&gt;I
personally skip using D-H parameters&lt;/em&gt; because I find that determining them is
error prone. D-H parameters transfer the likelihood of making an error from 
the forward kinematics programmer to the robotics manufacturer. &lt;/p&gt;

&lt;h2 id=&quot;determining-a-robot-39-s-number-of-degrees-of-freedom&quot;&gt;Determining a robot&amp;#39;s number of degrees-of-freedom&lt;/h2&gt;

&lt;p&gt;For a robot affixed to its environment, the robot&amp;#39;s degrees of freedom (DoF) is
equal to its number of joint variables (if the robot has kinematic loops, its
DOF is equal to its &lt;em&gt;independent&lt;/em&gt; joint variables). &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;A robot with kinematic loops.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/veloce-frontal.jpg&quot; alt=&quot;A robot with kinematic loops.&quot; width=&quot;600&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;A graph depicting the kinematic structure of the veloce robot depicted previously.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/veloce.png&quot; alt=&quot;A graph depicting the kinematic structure of the veloce robot depicted previously.&quot; width=&quot;600&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;A graph depicting the kinematic structure of a human. Note that the graph is a tree.&lt;/caption&gt;
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/2/2c/Modele_cinematique_corps_humain.svg&quot; alt=&quot;A graph depicting the kinematic structure of a human. Note that the graph is a tree.&quot; width=&quot;400&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;If a robot is not affixed to its environment, we say that its base is
&amp;quot;floating&amp;quot;, and that robot has six degrees of freedom plus its number of
independent joint variables. In fact, a minimum (there may be more than one) 
set of variables used to specify a robot&amp;#39;s configuration is known as
&lt;em&gt;minimum coordinates&lt;/em&gt; or &lt;em&gt;independent coordinates&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;As a quick aside, there may not be an actuator at every joint. If the number
of actuators is fewer than the robot&amp;#39;s DoF, we say that the robot is
&lt;em&gt;underactuated&lt;/em&gt;. Such robots are particularly challenging to control.
Legged robots, for instance, are underactuated, as are quadrotors. &lt;/p&gt;

&lt;h3 id=&quot;an-alternative-approach-to-determining-the-number-of-degrees-of-freedom&quot;&gt;An alternative approach to determining the number of degrees of freedom&lt;/h3&gt;

&lt;p&gt;Each rigid body in three dimensions has six degrees of freedom (three translation, three rotation). So, multiply the number of robot links by six, and then subtract the total number of constraint equations.&lt;/p&gt;

&lt;p&gt;For the example of the Foucault pendulum, there is one rigid body (the body) and three constraint equations (Equation \ref{eqn:ball-and-socket}; note the vector form of the equation). The total number of degrees-of-freedom is therefore \(6 - 3 = 3\).&lt;/p&gt;
</description>
        
          <description>&lt;p class=&quot;intro&quot;&gt;
Forward kinematics is the process of determining the location and orientation of a designated point on a robot as a function of the robot&#39;s configuration.
&lt;/p&gt;
</description>
        
        <pubDate>Sat, 12 Dec 2015 00:00:00 -0500</pubDate>
        <link>/robotics-course-materials/robotics-course-materials/blog/forward-kinematics/</link>
        <guid isPermaLink="true">/robotics-course-materials/robotics-course-materials/blog/forward-kinematics/</guid>
        
        
      </item>
      
    
      
      <item>
        <title>Differential kinematics and inverse kinematics</title>
        <description>&lt;p&gt;This learning module covers the relationship between movement in the 
generalized coordinates
and corresponding movement at a designated point on the robot. You may find
it helpful to consider an industrial robot, in which case you can substitute
the term &amp;quot;joint positions&amp;quot; or &amp;quot;joint angles&amp;quot; for &amp;quot;generalized coordinates and
the term &amp;quot;joint velocities&amp;quot; for &amp;quot;generalized velocities&amp;quot;, 
rather than more general robots (like humanoids). &lt;/p&gt;

&lt;h3 id=&quot;analytical-inverse-kinematics&quot;&gt;Analytical inverse kinematics&lt;/h3&gt;

&lt;p&gt;Recall that the forward kinematics equation is:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{x} = \mathbf{f}^p(\mathbf{q})
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(p\) is a frame attached to a rigid link of the robot,
\(\mathbf{q}\) are the \(n\) generalized coordinates of the robot, and \(\mathbf{x}\) is the pose of \(p\) in another frame (typically the world frame). If \(\mathbf{x}\) is represented by \(m\) real numbers, then \(\mathbf{f} : \mathbb{R}^n \to \mathbb{R}^m \).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We will use \(p\) throughout this learning module to refer to
a pose attached to a rigid link on the robot.&lt;/strong&gt; &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Depiction of Frame p, which is defined with respect to one of the robot&#39;s links. While forward kinematics seeks to determine how p is positioned and oriented with respect to the global frame, differential kinematics seek to determine how quickly p changes as a function of the robot&#39;s current configuration.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/frame-p-on-robot.png&quot; alt=&quot;Depiction of Frame p, which is defined with respect to one of the robot&#39;s links. While forward kinematics seeks to determine how p is positioned and oriented with respect to the global frame, differential kinematics seek to determine how quickly p changes as a function of the robot&#39;s current configuration.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h4 id=&quot;inverting-mathbf-f&quot;&gt;Inverting \(\mathbf{f}(.)\)&lt;/h4&gt;

&lt;p&gt;Inverting \(\mathbf{f}(.)\) means determining \(\mathbf{f}^{-1}(\mathbf{x}) = \mathbf{q}\), where \(\mathbf{x}\) is one of the possible outputs of the forward kinematics function (SO(2), SO(3), \(\mathbb{R}^2, \mathbb{R}^3\), SE(2), SE(3)). This inversion is generally challenging because \(\mathbf{f}(.)\) is a nonlinear function. Additional challenges are that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\(\mathbf{f}^{-1}(\mathbf{x})\) may have no solutions&lt;/li&gt;
&lt;li&gt;\(\mathbf{f}^{-1}(\mathbf{x}\)) may have multiple solutions&lt;/li&gt;
&lt;li&gt;Computation of \(\mathbf{f}^{-1}(.)\) generally recommends the use of a symbolic mathematics package (like &lt;a href=&quot;http://maxima.sourceforge.net&quot;&gt;Macsyma&lt;/a&gt;), which means that changing the kinematics of the robot means re-derivation of the inverse kinematics function&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Analytical inverse kinematics requires that the number of constraints be
equal to the robot&amp;#39;s number of generalized coordinates. Artificial constraints
may be introduced to satisfy this requirement. For example, a &amp;quot;typical&amp;quot;
anthropomorphic arm provides seven degrees of freedom, while controlling
position and orientation in 3D presents six constraints. Analytical inverse
kinematics solutions for such arms typically leave the elbow position as
an open parameter for the user to adjust, reducing the controllable degrees
of freedom of the arm to six. &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;An example of analytical inverse kinematics, using a two link planar arm. Image cribbed from Stefan Schaal&#39;s course notes.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/ik.png&quot; alt=&quot;An example of analytical inverse kinematics, using a two link planar arm. Image cribbed from Stefan Schaal&#39;s course notes.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h3 id=&quot;differential-kinematics&quot;&gt;Differential kinematics&lt;/h3&gt;

&lt;p&gt;Another way to do inverse kinematics is using &lt;em&gt;differential kinematics&lt;/em&gt;. Differential kinematics examines the change in \(\mathbf{x}\) given small
changes in \(\mathbf{q}\). &lt;a href=&quot;../forward-kinematics&quot;&gt;Recall that \(\mathbf{x}\) can represent position and/or orientation in 2D or 3D&lt;/a&gt;, so the aforementioned &amp;quot;small changes&amp;quot; refer to small changes in position and/or orientation.&lt;/p&gt;

&lt;p&gt;\begin{equation}
\dot{\mathbf{x}} = \begin{bmatrix} 
\frac{\partial f_1}{\partial q_1} &amp;amp; \ldots &amp;amp; \frac{\partial f_1}{\partial q_n} \\
&amp;amp; \vdots &amp;amp; \\
\frac{\partial f_m}{\partial q_1} &amp;amp; \ldots &amp;amp; \frac{\partial f_m}{\partial q_n}
\end{bmatrix}
\begin{bmatrix}
\dot{q}_1 \\
\vdots \\
\dot{q}_n
\end{bmatrix} \label{eqn:Jacobians-full}
\end{equation}&lt;/p&gt;

&lt;p&gt;The first matrix is the &lt;em&gt;Jacobian of pose \(\mathbf{p}\) with respect to
the generalized configuration&lt;/em&gt;. Refer back to the &lt;a href=&quot;../linear-algebra/&quot;&gt;material on linear algebra&lt;/a&gt; if you need a refresher on Jacobian matrices. The second matrix is just the generalized
velocity, \(\dot{\mathbf{q}}\). So, we can write Equation \ref{eqn:Jacobians-full} as:
\begin{equation}
\dot{\mathbf{x}} = \mathbf{J}\dot{\mathbf{q}} \label{eqn:Jacobians}
\end{equation}
&lt;em&gt;You will frequently see this equation in robotic manipulation&lt;/em&gt;.&lt;/p&gt;

&lt;h4 id=&quot;an-example-analytically-computing-the-jacobian-of-a-double-pendulum&quot;&gt;An example: analytically computing the Jacobian of a double pendulum&lt;/h4&gt;

&lt;p&gt;Consider the &lt;a href=&quot;../forward-kinematics&quot;&gt;double pendulum from the learning module on forward kinematics&lt;/a&gt;. At this point in time, assume that we wish only to
compute the Jacobian matrix of the double pendulum&amp;#39;s endpoint. Stated another way, \(\mathbf{f}(\mathbf{q}) \to \mathbb{R}^2\) and is defined as:
\begin{equation}
\begin{bmatrix}
l_1 c_1 + l_2 c_{1+2} \\
l_1 s_1 + l_2 s_{1+2}
\end{bmatrix}
\label{eqn:fkin-example}
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Before reading further, make sure you know (a) the dimension of \(\mathbf{q}\), (b) the dimension of \(\mathbf{f}(.)\), and the dimension of the Jacobian matrix that will be produced.&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;The Jacobian matrix for Equation \ref{eqn:fkin-example} will be:
\begin{equation}
\begin{bmatrix}
\frac{\partial f_1}{\partial q_1} &amp;amp; \frac{\partial f_1}{\partial q_2} \\
\frac{\partial f_2}{\partial q_1} &amp;amp; \frac{\partial f_2}{\partial q_2}
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;When I determine the derivatives, I get this:
\begin{equation}
\begin{bmatrix}
-l_1 s_1 - l_2 s_{1+2} &amp;amp; -l_2 s_{1+2} \\
l_1 c_1  + l_2 c_{1+2} &amp;amp; l_2 c_{1+2}
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;consideration-for-orientation&quot;&gt;Consideration for orientation&lt;/h4&gt;

&lt;p&gt;What about when the \(\mathbf{f}(.)\) mapping contains orientation components (i.e., SO(2), SO(3), SE(2), SE(3))? If we use the \(3 \times 3\) homogeneous 
transformation matrix from &lt;a href=&quot;../forward-kinematics&quot;&gt;the forard kinematics module&lt;/a&gt;, then the size of the Jacobian matrix will be \(3 \times 3 \times 2\).
Such higher dimensional matrices are not fun to deal with! So it makes sense
for us to use a non-matrix representation of orientation in this case. The orientation of the endpoint (or any point, for that matter) of the second link in the double pendulum is:
\begin{equation}
\theta_1 + \theta_2 \label{eqn:dp-orientation}
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Practice computing the Jacobian matrix for Equation \ref{eqn:dp-orientation}. Then make sure you know what relationship this Jacobian is computing in Equation \ref{eqn:Jacobians}.&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;what-does-the-jacobian-tell-us&quot;&gt;What does the Jacobian tell us?&lt;/h4&gt;

&lt;p&gt;Column \(i\) of the Jacobian gives us the scale of joint \(i\)&amp;#39;s 
instantaneous contribution to the 
movement in all dimensions of operational space. Row \(j\) of the Jacobian 
gives us instantaneous contribution of all joints to dimension \(j\) of
operational space.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note that the contribution of joint \(i\) to \(\mathbf{p}\) is zero if joint \(i\) does not affect \(\mathbf{p}\)&amp;#39;s movement&lt;/strong&gt; (and therefore, column \(i\) of the Jacobian will be a zero vector).&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Axis six does not contribute to movement of Frame p. The column of the Jacobian matrix corresponding to Axis six should therefore be zero.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/frame-p-on-robot.png&quot; alt=&quot;Axis six does not contribute to movement of Frame p. The column of the Jacobian matrix corresponding to Axis six should therefore be zero.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h4 id=&quot;jacobians-and-statics&quot;&gt;Jacobians and statics&lt;/h4&gt;

&lt;p&gt;The velocity of a point \(\mathbf{p}\) on a rigid body due to angular velocity is:
\begin{equation}
\dot{\mathbf{p}} = \dot{\mathbf{x}} + \mathbf{\omega} \times (\mathbf{p} - \mathbf{x})
\end{equation}
where \(\mathbf{x}\) and \(\dot{\mathbf{x}}\) are the position and velocity of the center of mass of the body and \(\mathbf{\omega}\) is the body&amp;#39;s angular velocity.&lt;/p&gt;

&lt;p&gt;Note the similarity with the relationship with the &lt;a href=&quot;https://www.quora.com/What-is-the-difference-between-torque-and-moment-3&quot;&gt;couple&lt;/a&gt; \(\hat{\mathbf{\tau}}\) that results from the addition of a torque, \(\mathbf{\tau}\), and a &lt;a href=&quot;https://www.quora.com/What-is-the-difference-between-torque-and-moment-3&quot;&gt;moment&lt;/a&gt; that results from applying a force \(\mathbf{f}\) on the body at point \(\mathbf{p}\):
\begin{equation}
\hat{\mathbf{\tau}} = \mathbf{\tau} + (\mathbf{p} - \mathbf{x}) \times \mathbf{f}
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;These equations represent a key relationship in mechanics and robotics, between force and motion&lt;/strong&gt;. &lt;em&gt;Moving from single rigid bodies to robots now&lt;/em&gt;, we can add the following rule in addition to Equation \ref{eqn:Jacobians}:&lt;br&gt;
\begin{equation}
\hat{\mathbf{\tau}} = \mathbf{J}^{\mathsf{T}}\begin{bmatrix} \mathbf{f} \\ \mathbf{\tau} \end{bmatrix} \label{eqn:Jacobians-torque}
\end{equation}&lt;/p&gt;

&lt;p&gt;I cannot overstate the importance of Equations \ref{eqn:Jacobians} and \ref{eqn:Jacobians-torque}. These equations tell us how fast a point on the robot is moving in operational space as a function of its joint speeds (Equation \ref{eqn:Jacobians}) and how much torque acts at a robot&amp;#39;s joints as a function of a force applied to a point on the robot (Equation \ref{eqn:Jacobians-torque}).&lt;/p&gt;

&lt;h3 id=&quot;numerical-inverse-kinematics&quot;&gt;Numerical inverse kinematics&lt;/h3&gt;

&lt;p&gt;State of the art numerical approaches for inverse kinematics use a &lt;a href=&quot;https://en.wikipedia.org/wiki/Newton%27s_method&quot;&gt;Newton-Raphson based process for finding roots of nonlinear systems of equations&lt;/a&gt;. These algorithms compute \(\mathbf{f}^{-1}(\mathbf{x}) = \mathbf{q}\) by finding the roots \(\mathbf{q}\) that satisfy \(\mathbf{f}(\mathbf{q}) - \mathbf{x} = \mathbf{0}\).&lt;/p&gt;

&lt;p&gt;Such algorithms are susceptible to failing to find a solution,
even when one or more solutions is known to exist. I will give an overview
of the basic approach, but extensions to this approach can yield significantly 
greater solvability.  &lt;/p&gt;

&lt;p&gt;The idea behind these approaches follows. While we can&amp;#39;t necessarily compute \(\mathbf{f}^{-1}(.)\), we &lt;em&gt;can invert \(\mathbf{f}(.)\) in a small neighborhood around a generalized configuration&lt;/em&gt;. So 
the &lt;em&gt;resolved motion rate control (RMRC)&lt;/em&gt; Algorithm (to be described below) repeatedly (1) inverts \(\mathbf{f}(.)\) around a 
generalized configuration and (2) uses that inverse to determine how to alter 
the generalized coordinates to move toward the goal. The hope is that the
process converges to an answer.&lt;/p&gt;

&lt;h4 id=&quot;the-resolved-motion-rate-control-rmrc-algorithm&quot;&gt;The resolved motion rate control (RMRC) algorithm&lt;/h4&gt;

&lt;p&gt;Given \(\mathbf{q}_{\textrm{init}}\), do:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;\(\mathbf{q} \leftarrow \mathbf{q}_{\textrm{init}}\)&lt;/li&gt;
&lt;li&gt;Compute \(\Delta\mathbf{x} = \mathbf{x}_{\textrm{des}} - f(\mathbf{q}\))&lt;/li&gt;
&lt;li&gt;If \(||\Delta \mathbf{x}|| &amp;lt; \epsilon \) &lt;strong&gt;return&lt;/strong&gt; &lt;em&gt;success&lt;/em&gt; &lt;/li&gt;
&lt;li&gt;Compute Jacobian (\(\mathbf{J}\)), evaluated at \(\mathbf{q}\)&lt;/li&gt;
&lt;li&gt;Solve least squares problem, \(\min_{\mathbf{q}} ||\mathbf{J}\Delta \mathbf{q} - \Delta \mathbf{x}||\) &lt;/li&gt;
&lt;li&gt;Determine \(t \le 1\) such that \(||\mathbf{x}_{\textrm{des}} - \mathbf{f}(\mathbf{q + t\Delta \mathbf{q}}\)) is minimized&lt;/li&gt;
&lt;li&gt;Update \(\mathbf{q}\): \(\mathbf{q} \leftarrow \mathbf{q} + t\Delta \mathbf{q}\)&lt;/li&gt;
&lt;li&gt;Repeat (2) until maximum iterations exceeded&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;return&lt;/strong&gt; &lt;em&gt;failure&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;minimizing-mathbf-j-delta-mathbf-q-delta-mathbf-x&quot;&gt;Minimizing  \(||\mathbf{J}\Delta \mathbf{q} - \Delta \mathbf{x}||\)&lt;/h5&gt;

&lt;p&gt;Due to unit mixing (angular vs. metric), a least squares solution is not 
necessarily desirable when attempting to find
joint angles that minimize residual error in desired position &lt;em&gt;and&lt;/em&gt;
orientation simultaneously. One can attempt to use scaling factors to bias
the least squares solution, but I work to avoid such hacking.&lt;/p&gt;

&lt;p&gt;I present the least squares options without further comment on this issue.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Jacobian transpose&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Least squares problems cannot usually be approached using a matrix transpose
approach: \(\mathbf{A}^\mathsf{T}\mathbf{b}\) does not generally yield a 
&lt;em&gt;descent direction&lt;/em&gt; (a change in \(\mathbf{x}\)) that reduces \(||\mathbf{Ax} - \mathbf{b}||\). The Jacobian transpose method is able to leverage rigid body dynamics&amp;#39; duality between force and
velocity:&lt;/p&gt;

&lt;p&gt;\begin{align*}
\mathbf{J}\dot{\mathbf{q}} &amp;amp; = \dot{\mathbf{x}} \\
\mathbf{J}^\mathsf{T}\mathbf{f} &amp;amp; = \mathbf{\tau}
\end{align*}&lt;/p&gt;

&lt;p&gt;The Jacobian transpose approach functions as if there were a spring attached 
between frame \(p\) and \(\mathbf{x}_{\textrm{des}}\). Proof
that the Jacobian transpose approach yields a descent direction follows.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: \((\mathbf{JJ}^\mathsf{T}\Delta \mathbf{x})^\mathsf{T}\Delta \mathbf{x} \ge 0\)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof&lt;/em&gt;: \((\mathbf{JJ}^\mathsf{T}\Delta \mathbf{x})^\mathsf{T}\Delta \mathbf{x} = (\mathbf{J}^\textsf{T}\Delta \mathbf{x})^\mathsf{T}(\mathbf{J}^\textsf{T}\Delta \mathbf{x}) = ||\mathbf{J}^\textsf{T}\Delta \mathbf{x}||^2\)&lt;/p&gt;

&lt;p&gt;Properties of the Jacobian transpose method:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Slow (linear) convergence to a solution (but requires \(O(n^2)\) operations, in place of expensive \(O(n^3)\) operations)&lt;/li&gt;
&lt;li&gt;No numerical problems; robust to singularities and near singularities in the Jacobian matrix&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Unregularized pseudo-inverse-based least squares&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Alternatively, we can use the right pseudo inverse to solve the least squares problem.&lt;/p&gt;

&lt;p&gt;\begin{align}
\Delta \mathbf{q} &amp;amp; = \mathbf{J}^+\Delta \mathbf{x} \\
 &amp;amp; = \mathbf{J}^\mathsf{T}(\mathbf{JJ}^\mathsf{T})^{-1}\Delta \mathbf{x}
\end{align}&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Fast (quadratic) convergence to a solution (but requires expensive \(O(n^3)\) operations)&lt;/li&gt;
&lt;li&gt;Numerical problems will occur near singularities in the Jacobian matrix; 
if the matrix is &lt;em&gt;nearly singular&lt;/em&gt;, it is possible that the factorization of \(\mathbf{JJ}^\mathsf{T}\) does not report that the matrix is singular  and yet \(\Delta \mathbf{q}\) not be a descent direction.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;(QR/SVD)-based least squares&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Any standard numerical approach for solving least squares problems, including
QR factorization and singular value decomposition (as described in the &lt;a href=&quot;../linear-algebra/&quot;&gt;linear algebra material&lt;/a&gt;) can be applied to this problem.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Fast (quadratic) convergence to a solution (but requires very expensive \(O(n^3)\) operations)&lt;/li&gt;
&lt;li&gt;Numerical problems may occur near singularities in the Jacobian matrix, but
failures will be less frequent than with unregularized least squares &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Least squares with nullspace&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;If the robot has more degrees of freedom available than there are position
and orientation constraints, we say that the kinematics are &lt;em&gt;redundant&lt;/em&gt;.
Redundancy can be exploited when the number of linearly independent columns 
in the Jacobian matrix is greater than the number of linearly dependent rows.
The nullspace of the Jacobian matrix can then be used to find a solution \(\Delta \mathbf{q}\) that reduces the residual error in the least squares problem &lt;em&gt;while
simultaneously satisfying secondary goals&lt;/em&gt;. Such secondary goals have included
selecting (1) the solution that minimizes distance from a desired joint 
configuration, (2) the solution that maximizes distance from obstacles, and
(3) the solution that maximizes manipulability (minimizes the new Jacobian&amp;#39;s condition number).&lt;/p&gt;

&lt;p&gt;If the singular value decomposition is used to compute the least squares
solution to \(||\mathbf{J}\Delta \mathbf{q} - \Delta \mathbf{x}||\), the nullspace of the Jacobian is provided &lt;a href=&quot;../linear-algebra/&quot;&gt;as a free byproduct&lt;/a&gt;. Otherwise, &lt;a href=&quot;http://www.springer.com/us/book/9781846286414&quot;&gt;the nullspace of the Jacobian is given by the matrix&lt;/a&gt; (page 125):&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{R} \equiv (\mathbf{I} - \mathbf{J}^+\mathbf{J})
\end{equation}&lt;/p&gt;

&lt;p&gt;If \(\Delta \mathbf{q}\) minimizes the residual \(||\mathbf{J\Delta q} = \mathbf{\Delta x}||\), then \(\mathbf{\Delta q} + \mathbf{Ry}\), where \(\mathbf{R} \in \mathbb{R}^{n \times m}, \mathbf{y} \in \mathbb{R}^m\) also minimizes the residual.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof&lt;/em&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Let \(\mathbf{e} \equiv \mathbf{J}\mathbf{\Delta}\mathbf{q} - \mathbf{\Delta x}\)&lt;/li&gt;
&lt;li&gt;Then \(\mathbf{J}(\Delta \mathbf{q} + \mathbf{Ry}) - \mathbf{\Delta x} = \mathbf{e}\) because, by definition of the nullspace, \(\mathbf{JRy} = \mathbf{0}\).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The secondary goal(s) is expressed as one or more objective functions \(\mathbf{g}(\mathbf{q})\) that attains its minimum at \(\mathbf{g}(\mathbf{\Delta q} + \mathbf{Ry}) = \mathbf{0}\).&lt;/p&gt;

&lt;p&gt;Typical objective functions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Maximizing a manipulability measure: \(g(\mathbf{q}) \equiv \sqrt{\textrm{det}(\mathbf{J(q)J}^\mathsf{T}(\mathbf{q})}\). Moves the robot away from singular configurations.&lt;/li&gt;
&lt;li&gt;Maximizing distance from joint limits: \(g(\mathbf{q}) \equiv \sum_{i=1}^n (q^{i^u} - q^i)^2 + (q^i - q^{i^l})^2\)&lt;/li&gt;
&lt;li&gt;Maximizing distance to an objstacle: \(g(\mathbf{q}) \equiv \min_{\mathbf{p}, \mathbf{o}} ||\mathbf{p}(\mathbf{q}) - \mathbf{o}||\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, satisfying a hierarchy of task goals is possible using multiple nullspace projections.&lt;/p&gt;

&lt;h5 id=&quot;computing-delta-mathbf-x&quot;&gt;Computing \(\Delta \mathbf{x}\)&lt;/h5&gt;

&lt;p&gt;When \(\mathbf{x}_{\textrm{des}}\) represents a target in Cartesian space (we do not care about \(p\)&amp;#39;s orientation), \(\Delta \mathbf{x}\) is computed using only a simple vector subtraction operation.&lt;/p&gt;

&lt;p&gt;Similarly, when \(\mathbf{x}_{\textrm{des}}\) represents a target &lt;em&gt;2D&lt;/em&gt; orientation, \(\Delta \mathbf{x}\) can be computed using &lt;em&gt;nearly&lt;/em&gt; a simple scalar subtraction operation. Why &amp;quot;nearly&amp;quot;? Consider the example where the current orientation is \(\theta \equiv \frac{\pi}{15}\) and target orientation is \(\theta_{\textrm{des}} \equiv \frac{29\pi}{15}\). Why is the simple subtraction \(\theta_{\textrm{des}} - \theta\) not recommended?&lt;/p&gt;

&lt;p&gt;When \(\mathbf{x}_{\textrm{des}}\) represents a target &lt;em&gt;3D&lt;/em&gt; orientation, 
matters become more complicated. In such acases, \(\mathbf{x}_{\textrm{des}}\) and \(\Delta \mathbf{x}\) will take a different form (why they must is a question for you to answer). We assume that \(\mathbf{x}_{\textrm{des}}\) is given as a \(3 \times 3\) rotation matrix and that \(\Delta \mathbf{x} \in \mathbb{R}^3\). &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;../poses3&quot;&gt;Recall that&lt;/a&gt;:
\begin{equation}
_w\dot{\mathbf{R}}_i\cdot\ _w{\mathbf{R}_i}^{\mathsf{T}} = \tilde{\mathbf{\omega}}_w
\end{equation}&lt;/p&gt;

&lt;p&gt;We can use &lt;a href=&quot;../dynamical-systems&quot;&gt;Euler integration &lt;/a&gt; and this equation to solve for \(\tilde{\mathbf{\omega}}\) given (1) the current orientation of the body and (2) the desired orientation of the body:
\begin{equation}
_w\mathbf{R}_i + \Delta t \tilde{\mathbf{\omega}}\ _w\mathbf{R}_i = \mathbf{x}_{\textrm{des}}
\end{equation}
Assume that \(\Delta t = 1\)- it doesn&amp;#39;t matter what we set \(\Delta t\) to since we will not be considering how much time it requires to move between the two orientations. We want \(\mathbf{\omega}\): 
\begin{align}
\tilde{\mathbf{\omega}}\ _w\mathbf{R}_i &amp;amp; = \mathbf{x}_{\textrm{des}} - \ _w\mathbf{R}_i \\
\tilde{\mathbf{\omega}} &amp;amp; = (\mathbf{x}_{\textrm{des}} - \ _w\mathbf{R}_i)\ _w\mathbf{R}_i^\mathsf{T}
\end{align}&lt;/p&gt;

&lt;p&gt;Recall that we desire for \(\Delta \mathbf{x}\) to be a three dimensional vector, while \(\tilde{\mathbf{\omega}}\) is a \(3 \times 3\) matrix. In fact, 
\(\tilde{\mathbf{\omega}}\) &lt;em&gt;should&lt;/em&gt; be a skew symmetric matrix (&lt;a href=&quot;../poses3&quot;&gt;as you hopefully recall&lt;/a&gt;) but the
first order approximation and lack of re-orthogonalization mean that it
generally will not be. So, to get \(\omega\), we use the skew-symmetric form of \(\mathbf{\omega} \times\) (again, &lt;a href=&quot;../poses3&quot;&gt;as you should recall&lt;/a&gt;) resulting in:
\begin{equation}
\Delta \mathbf{x} = \frac{1}{2} \begin{bmatrix}
\tilde{\omega}_{32} - \tilde{\omega}_{23} \\
\tilde{\omega}_{13} - \tilde{\omega}_{31} \\
\tilde{\omega}_{21} - \tilde{\omega}_{12}
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;h5 id=&quot;ray-search&quot;&gt;Ray search&lt;/h5&gt;

&lt;p&gt;As with any linearization, the approximation of \(\dot{\mathbf{f}}(.)\) using \(\mathbf{J}\dot{\mathbf{q}}\) becomes less accurate the farther we move
from the generalized coordinates where \(\mathbf{J}\) is evaluated. This means that we do not generally want to update the generalized configuration using:
\begin{equation}
\mathbf{q}_{i+1} \leftarrow \mathbf{q}_i + \Delta \mathbf{q}
\end{equation}
because \(\mathbf{q}_{i+1}\) might be farther from the goal! Instead, we do the update like this:
\begin{equation}
\mathbf{q}_{i+1} \leftarrow \mathbf{q}_i + \alpha \Delta \mathbf{q}
\end{equation}
This is called a &lt;em&gt;ray search&lt;/em&gt; or (less accurately) a line search, because we search along the ray \( 0 &amp;lt; \alpha &amp;lt; \infty \). Best practice is to constrain the search to \(\alpha &amp;lt; 1\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;As an aside, the ray/line search process is used in optimization, and therefore in machine learning, so you may see these formulas in the future.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There are a few options for the ray search:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Set \(\alpha\) to some small value (say 0.001), that you have found
works well empirically for the robot and task. This approach is clearly
not adaptive- it takes small steps even when big stops might be possible.&lt;/li&gt;
&lt;li&gt;Use a univariate optimization method like &lt;a href=&quot;http://fedc.wiwi.hu-berlin.de/xplore/tutorials/xegbohtmlnode62.html&quot;&gt;Brent&amp;#39;s Method&lt;/a&gt; to establish
a (local) optimum \(\alpha\). This approach is computationally expensive.&lt;/li&gt;
&lt;li&gt;Use &lt;a href=&quot;https://en.wikipedia.org/wiki/Backtracking_line_search&quot;&gt;backtracking line search&lt;/a&gt;, which requires a little information, but
is adaptive, like (2), but much faster.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Use Option (1) when your problem does not vary much, and you need to prototype
a solution quickly. Otherwise, use Option (3).&lt;/p&gt;

&lt;h4 id=&quot;manipulability&quot;&gt;Manipulability&lt;/h4&gt;

&lt;p&gt;In most cases in robotics, the &lt;em&gt;rank&lt;/em&gt; of the Jacobian matrix will be determined
by the number of independent rows, as this number is usually much smaller than 
the number of independent columns. If the Jacobian does not have full row
rank (meaning that one or more rows of the Jacobian matrix are linearly
dependent), the robot will lack the ability to move frame \(p\)
in every possible direction (for example, the robot may be unable to move \(p\) vertically). &lt;/p&gt;

&lt;p&gt;This loss of rank can occur for the following reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;workspace singularity&lt;/em&gt;: the robot&amp;#39;s arm is fully outstretched. There is no joint movement that could cause the arm to move further in the outstretched direction. Workspace singularities often occur on anthropomorphic robots when effecting pointing or when a leg is fully extended (as when standing with ``knees&amp;#39;&amp;#39; locked).&lt;/li&gt;
&lt;li&gt;&lt;em&gt;internal singularity&lt;/em&gt;: two or more rotational degrees of freedom have aligned such that movement in one is equivalent to movement in the other. This phenomenon is known as &lt;a href=&quot;https://www.youtube.com/watch?v=zc8b2Jo7mno&quot;&gt;Gimbal lock&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Manipulability can be determined by examining the condition number of the
Jacobian matrix at configuration \(\mathbf{q}\). Recall from the &lt;a href=&quot;../linear-algebra/&quot;&gt;lecture material on linear algebra&lt;/a&gt; that the condition number of a matrix is the ratio of the largest
to the smallest singular values. The closer the condition number is to infinity, the less manipulability that the robot possesses at configuration \(\mathbf{q}\).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;It is wise to avoid moving the robot to a configuration where manipulability
is reduced, even though computing the condition number requires a (relatively)
computationally expensive singular value decomposition.&lt;/em&gt; &lt;/p&gt;

&lt;h5 id=&quot;state-of-the-art-approaches-for-numerical-ik&quot;&gt;State of the art approaches for numerical IK&lt;/h5&gt;

&lt;p&gt;&lt;a href=&quot;https://www.researchgate.net/publication/282852814_TRAC-IK_An_Open-Source_Library_for_Improved_Solving_of_Generic_Inverse_Kinematics&quot;&gt;State of the art IK approaches&lt;/a&gt; use &lt;a href=&quot;https://en.wikipedia.org/wiki/Quasi-Newton_method&quot;&gt;quasi-Newton methods&lt;/a&gt; for nonlinear programming
with inequality constraints to compute inverse kinematics with joint limits.
These are generic (albeit carefully crafted) approaches applied to a standard
nonlinear programming description of the problem. With all numerical IK
approaches, &lt;em&gt;convergence is heavily
dependent upon the starting configuration&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;computing-the-jacobian-matrix&quot;&gt;Computing the Jacobian matrix&lt;/h3&gt;

&lt;p&gt;We will only consider computing the Jacobian matrix for revolute and prismatic
joints, as these are the most common powered joints, and I will only discuss
robots with bases affixed to their environment (like industrial robots). 
Therefore, we can assume that column \(i\) of the
Jacobian matrix will correspond to joint \(i\) of the robot, &lt;em&gt;where
the numbering is arbitrary&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;I will designate the axis of joint \(i\) to point along \(\hat{\mathbf{z}}_i\) (the hat indicates that the vector is normalized) and the current
location of joint \(i\) as \(\mathbf{j}_i\).  &lt;/p&gt;

&lt;p&gt;&lt;em&gt;One of the biggest challenges with programming robots that manipulate is
that matrices (like Jacobians) lose their meaning&lt;/em&gt;: they are simply a
collection of numbers. If the matrix is computed incorrectly, the effect
may not be obvious.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To help with this problem, you should always consider the frame of 
reference in which you are computing such quantities.&lt;/strong&gt; The frame of reference
for the Jacobian matrix that we compute below is the global reference frame. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bonus question: how would the Jacobian matrix change for a robot with a floating base?&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;computing-the-jacobian-matrix-for-translational-motion-only-2d&quot;&gt;Computing the Jacobian matrix for translational motion only (2D)&lt;/h4&gt;

&lt;p&gt;Translational motion in 2D will yield a Jacobian matrix with two rows. For a revolute joint, we assume that the joint&amp;#39;s rotation is about the positive &lt;em&gt;z&lt;/em&gt;-axis. &lt;/p&gt;

&lt;p&gt;Therefore, a column of the Jacobian matrix takes the following form for translational motion with a revolute joint:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{bmatrix}
j_{i_2} - p^o_2\\
p^o_1 - j_{i_1}
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;This equation is a cross product operation, as will be seen when we compute the Jacobian matrix for translational motion in 3D. We denote \(\mathbf{p}^o\) as the point under consideration located on the robot, defined in the global frame. &lt;/p&gt;

&lt;p&gt;A prismatic joint&amp;#39;s contribution to translational motion is just the joint axis:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{bmatrix}
\hat{\mathbf{z}}_i
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;computing-the-jacobian-matrix-for-translational-motion-only-3d&quot;&gt;Computing the Jacobian matrix for translational motion only (3D)&lt;/h4&gt;

&lt;p&gt;Translational motion in 3D will yield a Jacobian matrix with three rows.&lt;/p&gt;

&lt;p&gt;A column of the Jacobian matrix takes the following form for translational motion with a revolute joint:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{bmatrix}
\hat{\mathbf{z}}_i \times (\mathbf{p}^o - \mathbf{j}_i)
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;This equation can be intuited using the figure below:
&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Intuition behind the translational motion due to a revolute joint. As the link turns counter-clockwise about the joint, the linear contribution to the motion will be proportional to the rate of movement and the distance between p and the joint.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/geometric-Jacobian.png&quot; alt=&quot;Intuition behind the translational motion due to a revolute joint. As the link turns counter-clockwise about the joint, the linear contribution to the motion will be proportional to the rate of movement and the distance between p and the joint.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;&lt;/p&gt;

&lt;p&gt;As in 2D, a prismatic joint&amp;#39;s contribution to translational motion is just the joint axis:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{bmatrix}
\hat{\mathbf{z}}_i
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;computing-the-jacobian-matrix-for-rotational-motion-only-2d&quot;&gt;Computing the Jacobian matrix for rotational motion only (2D)&lt;/h4&gt;

&lt;p&gt;Rotational motion in 2D will yield a Jacobian matrix with a single row. For rotational motion with a revolute joint, a column of the Jacobian matrix takes the form:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{bmatrix}
1
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;For rotational motion, a prismatic joint makes no contribution, so the column of the Jacobian matrix takes the form:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{bmatrix}
0
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;computing-the-jacobian-matrix-for-rotational-motion-only-3d&quot;&gt;Computing the Jacobian matrix for rotational motion only (3D)&lt;/h4&gt;

&lt;p&gt;Rotational motion in 3D will yield a Jacobian matrix with three rows. For rotational motion with a revolute joint, a column of the Jacobian matrix takes the form:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{bmatrix}
\hat{\mathbf{z}}_i
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;For rotational motion, a prismatic joint makes no contribution, so the column of the Jacobian matrix takes the form:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{bmatrix}
\mathbf{0}
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;computing-the-jacobian-matrix-for-translational-and-rotational-motion&quot;&gt;Computing the Jacobian matrix for translational and rotational motion&lt;/h4&gt;

&lt;p&gt;Combining translational and rotational motion entails simply stacking the
Jacobian rows corresponding to translation on top of the rows corresponding
to rotation. The ordering- linear components on top, angular on bottom, for
example- is arbitrary, but must be consistent: \(\Delta \mathbf{x}\)
must follow the same convention. You can see why the ordering is arbitrary
in the equation below:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{bmatrix}
\dot{\overrightarrow{\mathbf{x}}} \\
\mathbf{\omega}
\end{bmatrix} = 
\begin{bmatrix}
\mathbf{J}_{\overrightarrow{\mathbf{x}}} \\
\mathbf{J}_{\mathbf{\omega}}
\end{bmatrix}
\dot{\mathbf{q}}
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(\dot{\overrightarrow{\mathbf{x}}}\) is the linear motion of a point
on the robot, \(\mathbf{J}_{\overrightarrow{\mathbf{x}}}\) represents the
components of the Jacobian matrix that contribute to linear motion, and \(\mathbf{J}_{\mathbf{\omega}}\) represents the components of the Jacobian that contribute to angular motion. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Verify that, if I permute rows of the Jacobian, I will get the same output for \(\dot{\mathbf{x}}\) (under the same permutation).&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Similarly, I can permute columns of the Jacobian as long as I permute the
same entries in \(\dot{\mathbf{q}}\).&lt;/p&gt;

&lt;h4 id=&quot;an-example-computing-the-jacobian-matrix-for-the-double-pendulum&quot;&gt;An example: computing the Jacobian matrix for the double pendulum&lt;/h4&gt;

&lt;p&gt;We again use the double pendulum example from &lt;a href=&quot;../forward-kinematics&quot;&gt;the forward kinematics module&lt;/a&gt;. We need the following pieces of information:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The endpoint of the mechanism (&lt;a href=&quot;../forward-kinematics&quot;&gt;already determined&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;The location of each joint (\(\mathbf{j}_i)\)&lt;/li&gt;
&lt;li&gt;Each joint&amp;#39;s axis: this always points along the \(z\)-axis, since this example is in 2D&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Finally, we also have to determine the form of the Jacobian that we want: I&amp;#39;ll say that we want a \(3 \times 2\) Jacobian matrix so that \(\dot{\mathbf{x}}\) corresponds to:
\begin{equation}
\begin{bmatrix}
\dot{x}\\
\dot{y}\\
\dot{\theta}
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;The only piece of information that we lack is the locations of the joints.&lt;br&gt;
The first joint is located at the origin (0,0). The origin of the
second joint is located at Frame 1&amp;#39; &lt;a href=&quot;../forward-kinematics&quot;&gt;in the example&lt;/a&gt;, which
is:&lt;/p&gt;

&lt;p&gt;\begin{align}
_w\mathbf{T}_1 \cdot\ _1\mathbf{T}_{1&amp;#39;} = 
\begin{bmatrix} 
c_1 &amp;amp; -s_1 &amp;amp; 0 \\
s_1 &amp;amp; c_1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix} \cdot
\begin{bmatrix} 
1 &amp;amp; 0 &amp;amp; \ell_1 \\
0 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix} = 
\begin{bmatrix} 
c_1 &amp;amp; -s_1 &amp;amp; l_1 c_1 \\
s_1 &amp;amp; c_1 &amp;amp; l_1 s_1 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
\end{align}&lt;/p&gt;

&lt;p&gt;Summarizing:
\begin{align}
\mathbf{j}_1 &amp;amp; = \begin{bmatrix} 0 \\ 0 \end{bmatrix} \\
\mathbf{j}_2 &amp;amp; = \begin{bmatrix} l_1 c_1 \\ l_1 s_1 \end{bmatrix}
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{bmatrix}
-(l_1 s_1 + l_2 s_{1+2}) &amp;amp; -l_2 s_{12}  \\
l_1 c_1 + l_2 c_{1+2} &amp;amp; l_2 c_{12} \\
1 &amp;amp; 1
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;Note that this is equivalent to the 3D Jacobian:
\begin{equation}
\begin{bmatrix}
-(l_1 s_1 + l_2 s_{1+2}) &amp;amp; -l_2 s_{12}  \\
l_1 c_1 + l_2 c_{1+2} &amp;amp; l_2 c_{12} \\
0 &amp;amp; 0 \\
0 &amp;amp; 0 \\
0 &amp;amp; 0 \\
1 &amp;amp; 1
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;where I have designated the top three components to be linear motion and
the bottom three components to be angular motion.&lt;/p&gt;

&lt;h4 id=&quot;computing-the-jacobian-matrix-numerically-using-a-finite-difference-approach&quot;&gt;Computing the Jacobian matrix numerically (using a finite difference approach)&lt;/h4&gt;

&lt;p&gt;Recall that the Jacobian matrix is computed at a generalized configuration, \(\mathbf{q} \in \mathbb{R}^n\). &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Compute \(\mathbf{x} \leftarrow f(\mathbf{q})\)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;for&lt;/strong&gt; \(i=1,\ldots,n\):

&lt;ol&gt;
&lt;li&gt;Set \(q_i \leftarrow q_i + \epsilon\)&lt;/li&gt;
&lt;li&gt;Compute \(\mathbf{x}&amp;#39; \leftarrow f(\mathbf{q})\)&lt;/li&gt;
&lt;li&gt;Set column i of the Jacobian matrix to \((\mathbf{x}&amp;#39; - \mathbf{x})/\epsilon\)&lt;/li&gt;
&lt;li&gt;Set \(q_i \leftarrow q_i - \epsilon\)&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Step 2.3 requires computing the differential between two operational
space configurations (i.e., positions, orientations, or mixed position and 
orientation), as described above. &lt;/p&gt;
</description>
        
          <description>&lt;p&gt;This learning module covers the relationship between movement in the 
generalized coordinates
and corresponding movement at a designated point on the robot. You may find
it helpful to consider an industrial robot, in which case you can substitute
the term &amp;quot;joint positions&amp;quot; or &amp;quot;joint angles&amp;quot; for &amp;quot;generalized coordinates and
the term &amp;quot;joint velocities&amp;quot; for &amp;quot;generalized velocities&amp;quot;, 
rather than more general robots (like humanoids). &lt;/p&gt;
</description>
        
        <pubDate>Fri, 11 Dec 2015 00:00:00 -0500</pubDate>
        <link>/robotics-course-materials/robotics-course-materials/blog/differential-kinematics/</link>
        <guid isPermaLink="true">/robotics-course-materials/robotics-course-materials/blog/differential-kinematics/</guid>
        
        
      </item>
      
    
      
      <item>
        <title>Controlling robot motion</title>
        <description>&lt;p&gt;This learning module focuses on approaches for regulating the &lt;em&gt;motion&lt;/em&gt; of a 
robot. Concepts covered include model-based (feedforward) and model-free (error feedback) approaches, composite approaches, and stability.&lt;/p&gt;

&lt;p&gt;A caveat: even if we could control the motion of a robot with perfect accuracy
does not mean that it&amp;#39;s a good idea. A robot that moves exactly as commanded, minimizing 
the effects of all disturbances on its motion, is a robot that will crush
humans, damage the environment, or destroy itself. To avoid those problems, 
one should consider alternative schemes like &lt;a href=&quot;http://users.softlab.ntua.gr/%7Ektzaf/Courses/literature/07_Force_Control.pdf&quot;&gt;force control&lt;/a&gt;, which regulates
the force applied to the environment, and
&lt;a href=&quot;https://en.wikipedia.org/wiki/Impedance_control&quot;&gt;impedance control&lt;/a&gt;, which regulates 
the relationship between force and motion. Such schemes should be 
considered for uncontrolled environments or when the accuracy necessary
to perform a task is higher than the accuracy of motion and/or modeling. &lt;/p&gt;

&lt;p&gt;Some definitions of basic concepts in control:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Controller&lt;/strong&gt;: sends inputs to the dynamical system that may change its state&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plant&lt;/strong&gt;: another name for the dynamical system that the controller can change &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reference signal&lt;/strong&gt;: the value (state or a function of state) that the system is to attain. The reference signal may be fixed (a thermostat setting is effectively fixed) or changing over time&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Error signal&lt;/strong&gt;: the difference between the reference signal and the system&amp;#39;s output&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An example that we will use for illustrative purposes is a cruise control
system in a car. In this example, the car is the plant, the cruise control
is the controller, the driver&amp;#39;s set speed is the reference signal, and the
control input is the throttle (the gas pedal).&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;A depiction of a negative feedback controller for controlling a robot. Where do the desireds (qd and dqd/dt) come from?&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/nfb.png&quot; alt=&quot;A depiction of a negative feedback controller for controlling a robot. Where do the desireds (qd and dqd/dt) come from?&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h2 id=&quot;inverse-dynamics-control&quot;&gt;Inverse dynamics control&lt;/h2&gt;

&lt;p&gt;Without loss of generality, let&amp;#39;s consider the &lt;em&gt;linear&lt;/em&gt; dynamical system:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\dot{\mathbf{x}} = \mathbf{A}\mathbf{x} + \mathbf{B}\mathbf{u}
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(\mathbf{u}\) are the controls that we can apply. Assume
that \(\mathbf{A}, \mathbf{B} \in \mathbb{R}^{n \times n}\) and that
\(\mathbf{B}\) is a full rank matrix. Then if we want to determine
controls \(\mathbf{u}\) to drive the system to change in state \(\dot{\mathbf{x}}_{\textrm{des}}\), algebra yields:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{u} = \mathbf{B}^{-1}(\dot{\mathbf{x}}_{\textrm{des}} - \mathbf{A}\mathbf{x})
\end{equation}&lt;/p&gt;

&lt;p&gt;Controlling a dynamical system in this way is termed &lt;em&gt;inverse dynamics 
control&lt;/em&gt;. This is straightforward- so why is this a full learning module?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The control system designer may not possess a mathematical model (\(\dot{\mathbf{x}} = \mathbf{A}\mathbf{x} + \mathbf{Bu}\) in the case above) of the dynamical system, so alternative control techniques may be necessary. With respect to robotics, obtaining accurate and rapidly computable mathematical models of robots with compliant elements is challenging. &lt;/li&gt;
&lt;li&gt;&lt;em&gt;A mathematical model is just a model&lt;/em&gt;. There will usually be 
effects- &lt;a href=&quot;https://en.wikipedia.org/wiki/Backlash_(engineering)&quot;&gt;transmission backlash&lt;/a&gt;, imperfectly rigid links, &lt;a href=&quot;https://en.wikipedia.org/wiki/Counter-electromotive_force&quot;&gt;counter-electromotive forces&lt;/a&gt;, joint friction, and other
influences- that the model will not account for, so the inverse dynamics 
control will generally not influence the system exactly as desired. Some form
of error feedback control is usually necessary as well.&lt;/li&gt;
&lt;li&gt;In tandem with the previous point, &lt;a href=&quot;https://en.wikipedia.org/wiki/System_identification&quot;&gt;system identification&lt;/a&gt; is necessary to identify modeling parameters. For example, if our dynamics model incorporates Coulomb and viscous friction models at the joints, then both Coulomb and viscous friction parameters must be determined. Determining these parameters is often a tedious process that requires extensive physical experimentation and can change as the robot is operated (as it enters its normal operating temperature, leaks hydraulic fluid, etc.) or manipulates or carries objects. &lt;/li&gt;
&lt;li&gt;Even if we possess a perfect mathematical model, we generally&lt;br&gt;
do not know the contact forces acting on the robot, which implies that we cannot
invert the model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Inverse dynamics control is a &lt;em&gt;feedforward&lt;/em&gt; control scheme, which is 
distinguished from an &lt;em&gt;open loop&lt;/em&gt; control scheme in that the latter does
not use a mathematical model of the system. &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;The Three types of Control System (a) Open Loop (b) Feed-forward (c) Feedback (Closed Loop) (image and description from Wikipedia)&lt;/caption&gt;
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/en/c/c7/Control_Systems.png&quot; alt=&quot;The Three types of Control System (a) Open Loop (b) Feed-forward (c) Feedback (Closed Loop) (image and description from Wikipedia)&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h2 id=&quot;error-feedback-control&quot;&gt;Error feedback control&lt;/h2&gt;

&lt;p&gt;Error feedback control is a simple alternative that uses the error signal
to determine controls. Error feedback control requires no mathematical
model of the system, which is a double edged sword: error feedback control
does not anticipate the system&amp;#39;s behavior (if there is no error, no controls
are applied). &lt;/p&gt;

&lt;p&gt;The main challenges with error feedback control are (1) setting the rate
at which error is corrected (this process is called &lt;em&gt;tuning controller gains&lt;/em&gt;)
and (2) ensuring that there is little lag from changes in the system
state to actuation.&lt;/p&gt;

&lt;p&gt;The latter &lt;em&gt;control lag&lt;/em&gt; problem can occur because the sensing device has
low bandwidth relative to the speed with which the robot can move or because
filtering (by averaging) is necessary to combat sensory noise. Some plumbing
systems can exhibit significant control lag, for example: water temperature
may lag five to ten seconds behind adjustments to the taps.&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;The concept of the feedback loop to control the dynamic behavior of the system: this is negative feedback, because the sensed value is subtracted from the desired value to create the error signal, which is amplified by the controller (image and description from Wikipedia)&lt;/caption&gt;
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/2/24/Feedback_loop_with_descriptions.svg&quot; alt=&quot;The concept of the feedback loop to control the dynamic behavior of the system: this is negative feedback, because the sensed value is subtracted from the desired value to create the error signal, which is amplified by the controller (image and description from Wikipedia)&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h3 id=&quot;proportional-control&quot;&gt;Proportional control&lt;/h3&gt;

&lt;p&gt;One of the simplest error feedback controllers is the &lt;em&gt;proportional controller&lt;/em&gt;, which applies the control signal proportionally to the error signal. The
cruise control system in many cars uses proportional control (also called &amp;quot;P control&amp;quot;): the throttle is applied proportionally to the difference between the current speed and the set speed. &lt;/p&gt;

&lt;p&gt;The control for a P-controller is computed using the following function.&lt;/p&gt;

&lt;p&gt;\begin{equation}
u = k_p (x_{\textrm{des}} - x)
\end{equation} &lt;/p&gt;

&lt;p&gt;\(u\) is the control, \(x_{\textrm{des}}\) is the desired state, and
\(x\) is the system state. \(k_p \ge 0\) is known as the &amp;quot;proportional
gain&amp;quot; or &amp;quot;P gain&amp;quot;. &lt;/p&gt;

&lt;h3 id=&quot;proportional-derivative-control&quot;&gt;Proportional-derivative control&lt;/h3&gt;

&lt;p&gt;If many cars operated using cruise control were to suddenly begin climbing a large hill, the speed will drop precipitously, causing the accelerator to be
applied abruptly. Alternatively, the accelerator may be applied as the car is
at the top of a steep grade, causing the car to greatly exceed the desired
speed.&lt;/p&gt;

&lt;p&gt;A &lt;em&gt;derivative&lt;/em&gt; controller can help solve this problem by accounting for the
rate at which error is changing: control is modified depending on whether
error is increasing or decreasing. &lt;/p&gt;

&lt;p&gt;\begin{equation}
u = k_d (\dot{x}_{\textrm{des}} - \dot{x})
\end{equation} &lt;/p&gt;

&lt;p&gt;As in the case above, \(k_d \ge 0\) is the &amp;quot;derivative gain&amp;quot; or &amp;quot;D gain&amp;quot;.
It is generally the case that proportional and derivative control are combined
to make a &amp;quot;proportional-derivative&amp;quot; or &amp;quot;PD controller&amp;quot;. This controller is
represented by the equation below: &lt;/p&gt;

&lt;p&gt;\begin{equation}
u = k_p (x_{\textrm{des}} - x) + k_d (\dot{x}_{\textrm{des}} - \dot{x})
\end{equation} &lt;/p&gt;

&lt;p&gt;In the cruise control example, adding derivative control would add &lt;em&gt;more&lt;/em&gt; throttle (compared to just a proportional control)
if the car is decelerating when it is below speed and would decrease (or would limit the increase) the amount of throttle if
the car is accelerating when it is above speed.&lt;/p&gt;

&lt;h3 id=&quot;proportional-integrative-derivative-control&quot;&gt;Proportional-integrative-derivative control&lt;/h3&gt;

&lt;p&gt;PD control is not proficient at addressing &lt;em&gt;steady state error&lt;/em&gt;, which (informally) is error that remains after the system&amp;#39;s behavior has settled. Steady
state error is relevant for only fixed reference signals. A PD controller is 
often unable to eliminate steady state error without excessively large gains. &lt;/p&gt;

&lt;p&gt;Continuing with our cruise control example, imagine that our 
car is driving along the highway while towing a large trailor. The cruise
control will stay at some speed \(y\) below the set speed. If the speed
falls below \(y\), the positional error will be sufficiently large such 
that the proportional control component can increase the speed back to \(y\).
The combination of the proportional gain and positional error are insufficient to further decrease error for speeds above \(y\).  &lt;/p&gt;

&lt;p&gt;Steady state error can be eliminated using even a small gain by accumulating 
(integrating) the error over time. This &lt;em&gt;integrative&lt;/em&gt; term results in a
proportional-integrative-derivative (PID) controller.&lt;/p&gt;

&lt;p&gt;\begin{equation}
u = k_p (x_{\textrm{des}} - x) + k_d (\dot{x}_{\textrm{des}} - \dot{x}) + k_i \int_{t_0}^{t_c} (x_{\textrm{des}}(t) - x(t))\textrm{ d}t
\end{equation} &lt;/p&gt;

&lt;p&gt;where \(t_0\) and \(t_c\) are the initial and current system times,
respectively. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Disadvantages of PID control&lt;/strong&gt; are that the controller can saturate the 
actuators when error is not reduced, &lt;a href=&quot;https://en.wikipedia.org/wiki/Integral_windup&quot;&gt;integral windup&lt;/a&gt;- which causes the integral term to rapidly accumulate 
when the setpoint is changed- can occur, and another \(k\) term must be
tuned.&lt;/p&gt;

&lt;h3 id=&quot;pidd-control&quot;&gt;PIDD control&lt;/h3&gt;

&lt;p&gt;PIDD or PID\(^2\) control, which incorporates a second derivative, is
not commonly used in robotics applications: few sensors are able to provide
clean acceleration signals.&lt;/p&gt;

&lt;h2 id=&quot;decentralized-control&quot;&gt;Decentralized control&lt;/h2&gt;

&lt;p&gt;PD and PID schemes are commonly used to control robots, in which case a
separate controller is applied at each joint: \(2n\) or \(3n\) gains
must then be tuned. This is known as &lt;em&gt;decentralized control&lt;/em&gt;, because each joint
is controlled as if it is independent of the others. A torque applied to
one joint creates dynamic effects on the other joints; these dynamic effects
are treated as disturbances to be eliminated. A centralized control scheme
(like inverse dynamics control) computes controls for each joint 
simultaneously and anticipating effects.&lt;/p&gt;

&lt;h2 id=&quot;combining-feedforward-and-feedback-control&quot;&gt;Combining feedforward and feedback control&lt;/h2&gt;

&lt;p&gt;The best of both worlds is a hybrid control scheme that combines the
best aspects of feedforward and feedback control. Two options are to
sum the outputs from the two control schemes and feed the output from the&lt;br&gt;
error feedback controller into the feedforward controller; only the latter
is truly a centralized controller.&lt;/p&gt;

&lt;p&gt;Inverse dynamics is the most obvious option for feedforward control. Another
possibility for feedforward control is use a gravity compensation block.&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;A control block diagram of a composite (negative feedback + inverse dynamics controller) for controlling a robot. A good inverse dynamics model will allow error feedback gains to be really small.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/composite.png&quot; alt=&quot;A control block diagram of a composite (negative feedback + inverse dynamics controller) for controlling a robot. A good inverse dynamics model will allow error feedback gains to be really small.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;A control block diagram of an error feedback plus gravity compensation scheme for controlling a robot. Gravity compensation is less dependent on having a good model than inverse dynamics and also requires less computation, but tracking accuracy is potentially much lower.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/gravity.png&quot; alt=&quot;A control block diagram of an error feedback plus gravity compensation scheme for controlling a robot. Gravity compensation is less dependent on having a good model than inverse dynamics and also requires less computation, but tracking accuracy is potentially much lower.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h2 id=&quot;stability-of-a-control-system&quot;&gt;Stability of a control system&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;../dynamical-systems&quot;&gt;We have already looked at stability of dynamical systems&lt;/a&gt;. Simplifying the issue, there we sought to find whether a dynamical system
would converge to an equilibrium point from points in the neighborhood of the
equilibrium point. For controlled systems (like robots), we seek guarantees 
that applying the controller to the system will result in bounded errors
(recall the definition of the error signal above), &lt;em&gt;even if the uncontrolled 
system is unstable&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;For example, if a controller causes a system&amp;#39;s error to be describable by the linear 
differential equation&lt;/p&gt;

&lt;p&gt;\begin{equation}
\dot{\mathbf{e}} = -\mathbf{K}\mathbf{e}
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(\mathbf{K} \in \mathbb{R}^n\), then the controlled system is stable
if all real eigenvalue components of \(-\mathbf{K}\) are negative (and
marginally stable if all real eigenvalue components are non-positive). &lt;/p&gt;

&lt;p&gt;Unfortunately, the combined differential equations of the plant and the 
controller are usually nonlinear and not generally amenable to such simple
analysis. Lyapunov stability analysis is often the go-to approach.&lt;/p&gt;

&lt;h3 id=&quot;system-response&quot;&gt;System response&lt;/h3&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Figure depicting rise time, overshoot, delay time, and peak time. Image from Wikipedia.&lt;/caption&gt;
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/7/71/Second_order_under-damped_response.svg&quot; alt=&quot;Figure depicting rise time, overshoot, delay time, and peak time. Image from Wikipedia.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;In the figure above, \(t_d\) is the delay time, \(t_r\) is the rise
time, \(t_p\) is the peak time, and \(M_p\) is the overshoot. Settling
time and steady state error are not depicted.&lt;/p&gt;

&lt;p&gt;These definitions are adapted from &lt;a href=&quot;https://en.wikipedia.org/wiki/Transient_response&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;rise time&lt;/strong&gt;: the time required for the system to move from a specified low value to a specified high value &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;overshoot&lt;/strong&gt;: occurs when the system exceeds its set point (followed by a period of oscillation if the system is &lt;a href=&quot;../dynamical-systems&quot;&gt;underdamped&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;settling time&lt;/strong&gt;: the time that the system enters and remains within a specified error band around the set point&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;delay time&lt;/strong&gt;: the time required for the system to move halfway from the
specified low value to the specified high value&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peak time&lt;/strong&gt;: the time required for the system to hit the first overshoot peak&lt;/li&gt;
&lt;/ul&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Animation depicting effect of different PID gains on rise time, overshoot, and settling time. Image from Wikipedia.&lt;/caption&gt;
&lt;img src=&quot;http://upload.wikimedia.org/wikipedia/commons/3/33/PID_Compensation_Animated.gif&quot; alt=&quot;Animation depicting effect of different PID gains on rise time, overshoot, and settling time. Image from Wikipedia.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;
</description>
        
          <description>&lt;p&gt;This learning module focuses on approaches for regulating the &lt;em&gt;motion&lt;/em&gt; of a 
robot. Concepts covered include model-based (feedforward) and model-free (error feedback) approaches, composite approaches, and stability.&lt;/p&gt;
</description>
        
        <pubDate>Wed, 09 Dec 2015 00:00:00 -0500</pubDate>
        <link>/robotics-course-materials/robotics-course-materials/blog/motion-control/</link>
        <guid isPermaLink="true">/robotics-course-materials/robotics-course-materials/blog/motion-control/</guid>
        
        
      </item>
      
    
  </channel>
</rss>
