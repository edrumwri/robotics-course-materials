<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Robotics</title>
    <description>Robotics</description>
    <link>http://positronicslab.github.io/</link>
    <atom:link href="http://positronicslab.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 12 Jan 2016 23:44:32 -0500</pubDate>
    <lastBuildDate>Tue, 12 Jan 2016 23:44:32 -0500</lastBuildDate>
    <generator>Jekyll v2.4.0</generator>
    
      
      <item>
        <title>Programming autonomous robots</title>
        <description>&lt;p&gt;This module will discuss some topics for those new to robot programming. 
I&amp;#39;ll start with programming general real-time systems, talk next about
an architecture for a typical robot system- including &lt;em&gt;planners&lt;/em&gt; and 
&lt;em&gt;controllers&lt;/em&gt;, and conclude with many thoughts on the open problem of 
combining reactive and deliberative behaviors for dextrous robots.&lt;/p&gt;

&lt;h2 id=&quot;programming-real-time-systems&quot;&gt;Programming real-time systems&lt;/h2&gt;

&lt;p&gt;Real-time systems are generally best controlled by a real time operating 
system, which provide specific process scheduling policies and minimal
interrupt latency. The key feature of a real time operating system is its
predictability.&lt;/p&gt;

&lt;p&gt;For controlling robots, the user selects a control frequency (e.g., 100 Hz),
and the real time operating system ensures that this process is run 
regularly- every 10 ms (plus or minus some small amount of error). &lt;em&gt;It is the
programmer&amp;#39;s responsibility to ensure that their process does not overrun the
process&amp;#39; allotted time&lt;/em&gt; (10 ms in the example above). If such an overrun
occurs, the real time operating system will typically generate an exception.
Programmers typically avoid I/O and memory allocation in real-time control
loops, as these procedures are often unpredictable. &lt;/p&gt;

&lt;h2 id=&quot;architecture-of-a-robotic-system&quot;&gt;Architecture of a robotic system&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Autonomous&lt;/em&gt; robotic systems- as opposed to those robots situated in controlled
environments (like factories)- are typically driven using a small number of 
&lt;em&gt;behaviors&lt;/em&gt;, modular components that focus on getting the robot to perform a single task (like avoiding obstacles, homing to a light source, or following a
wall). Many robot architectures are built on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Finite-state_machine&quot;&gt;finite state machine&lt;/a&gt; model, with behaviors representing the states and transitions between behaviors occuring in response to events. &lt;/p&gt;

&lt;p&gt;The reason these interacting behaviors are used, instead of the previously
dominant &lt;em&gt;sense-plan-act&lt;/em&gt; approach, is depicted in &lt;a href=&quot;https://www.youtube.com/watch?v=qXdn6ynwpiI&quot;&gt;this video of an early robot, Shakey&lt;/a&gt;: the robot moves too
slowly to act and react in uncontrolled environments.&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Data flow and computation using the sense-plan-act approach to robot architecture.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/SPA.png&quot; alt=&quot;Data flow and computation using the sense-plan-act approach to robot architecture.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;For example, here is a finite state machine-based architecture for a simple
foraging robot:&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Architecture for a simple foraging robot. Graph nodes represent machine states and transitions represent events and sensory signals. Image from Bristol Robotics Laboratory.&lt;/caption&gt;
&lt;img src=&quot;http://www.brl.ac.uk/images/finite-state-machine.png&quot; alt=&quot;Architecture for a simple foraging robot. Graph nodes represent machine states and transitions represent events and sensory signals. Image from Bristol Robotics Laboratory.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;The figure below shows the architecture of a more sophisticated autonomous 
robot capable of manipulating its environment. The depicted architecture does 
not represent a single task, but rather depicts the&lt;br&gt;
flow of data between software and hardware and real-time and non-real-time
software components. &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;An example of a software architecture for an autonomous robot, the Willow Garage PR2 mobile manipulator.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/robot-software-architecture.png&quot; alt=&quot;An example of a software architecture for an autonomous robot, the Willow Garage PR2 mobile manipulator.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h3 id=&quot;important-components&quot;&gt;Important components&lt;/h3&gt;

&lt;h4 id=&quot;interprocess-communication&quot;&gt;Interprocess communication&lt;/h4&gt;

&lt;p&gt;As can be seen from the figure, communication between multiple computational
processes running simultaneously is important. The particular mechanism
chosen for &lt;em&gt;interprocess communication&lt;/em&gt; (IPC) is critical: latency between
one process transmitting data and the other receiving it should be minimized.
The IPC mechanism depicted in this figure is &lt;em&gt;shared memory&lt;/em&gt;, which is not
necessarily friendly to program with, but is quite fast.&lt;/p&gt;

&lt;h4 id=&quot;sensors&quot;&gt;Sensors&lt;/h4&gt;

&lt;p&gt;The robot depicted in this example possesses several types of sensors:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;LIDAR&lt;/strong&gt;: a time of flight sensing system that operates in the same manner as police using lasers to catch speeding motorists. LIDAR sensors for robots use a rotating laser beam to capture a two-dimensional or three-dimensional depth scan of a surface. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RGB&lt;/strong&gt;: a camera that captures a color (red-green-blue) image from its viewpoint for processing using computer vision algorithms. Computer vision is not a current mainstay of robotic sensing for manipulation, likely because of the significant processing time required, imperfect identification (see Google image search), and susceptibility to sensory noise and artifacts. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kinect&lt;/strong&gt;: a single hardware unit that combines both depth sensing and RGB&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IMU&lt;/strong&gt;: an &lt;em&gt;inertial measurement unit&lt;/em&gt; that combines accelerometers, gyroscopic sensors, magnetometers, and (often) GPS measurements. The accelerometers give linear acceleration. The gyroscopic sensors yield angular acceleration. 
Magnetometers can help determine orientation.&lt;/li&gt;
&lt;/ul&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;A point cloud, as might be produced by LIDAR or the Kinect sensor.&lt;/caption&gt;
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/4/4c/Point_cloud_torus.gif&quot; alt=&quot;A point cloud, as might be produced by LIDAR or the Kinect sensor.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h4 id=&quot;controller&quot;&gt;Controller&lt;/h4&gt;

&lt;p&gt;A &lt;em&gt;controller&lt;/em&gt; is a real-time process that runs at a specified frequency
(commonly between 100 Hz and 10,000 Hz for robots). Controllers attempt to
regulate a dynamical system using a model of the system and/or error
between the desired state of the system and its current state.&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Extremely simple controller (thermostat), actuators (A/C unit and furnace), and sensor (thermometer). The dynamical system is the temperature in the dwelling. The controller operates using a simple _error feedback_ mechanism: a voltage signal is sent to the furnace or A/C unit to increase/decrease temperature until the desired setpoint is reached.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/thermostat.png&quot; alt=&quot;Extremely simple controller (thermostat), actuators (A/C unit and furnace), and sensor (thermometer). The dynamical system is the temperature in the dwelling. The controller operates using a simple _error feedback_ mechanism: a voltage signal is sent to the furnace or A/C unit to increase/decrease temperature until the desired setpoint is reached.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;One of the earliest controllers is the centrifugal governor. A video depiction of the centrifugal governor is shown &lt;a href=&quot;https://www.youtube.com/watch?v=iO0CxOTe4Uk&quot;&gt;here&lt;/a&gt;. An engine causes the governor to rotate, and centrifugal force on the governor- caused by its faster rotation- closes the throttle valve. When the engine slows, the centrifugal force attenuates, and the throttle re-opens. &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;A depiction of a centrifugal governor.&lt;/caption&gt;
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/1/1e/Centrifugal_governor.png&quot; alt=&quot;A depiction of a centrifugal governor.&quot; width=&quot;500&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h4 id=&quot;motor-servos&quot;&gt;Motor servos&lt;/h4&gt;

&lt;p&gt;A motor servo is the electronic interface between the robot software and
the hardware. We can communicate with this interface using low level commands
over the serial port, USB, or CAN bus or using higher level driver software.
The commands we send to the interface may consist of desired current
(for electromagnetic motors), desired torque, or desired position and velocity.
The interface will typically output some data, including joint position and-
sometimes- speed, and torque.&lt;/p&gt;

&lt;h4 id=&quot;planning-modules&quot;&gt;Planning modules&lt;/h4&gt;

&lt;p&gt;Many computational processes require more time to compute than the control 
loop frequency would allow. For example, determining viable foot placements
for a humanoid robot can require on the order of a second. Planning modules
are non-real-time processes that run &amp;quot;in the background&amp;quot;. When a planning
module has completed its computation, it is able to begin feeding inputs
to the controller. &lt;/p&gt;

&lt;h4 id=&quot;perceptual-modules&quot;&gt;Perceptual modules&lt;/h4&gt;

&lt;p&gt;Raw sensory data from robots is generally not usuable without further 
processing. We may wish to remove outlier points from range data, to
&amp;quot;self filter&amp;quot; the robot from RGB or range data, and fuse measurements from
inertial measurement units with other sensors to combat drift.&lt;/p&gt;

&lt;h3 id=&quot;mixing-real-time-reactivity-and-planning&quot;&gt;Mixing real-time reactivity and planning&lt;/h3&gt;

&lt;p&gt;Control strategies for highly dextrous (dynamic) robots, like walking robots,
humanoid robots, and mobile manipulators are still an active area of research. The researchers must usually be focused on basic functionality (standing, walking, opening a door), causing issues like providing near autonomy in dynamic environments (like those populated by humans) to be ignored.  &lt;/p&gt;

&lt;p&gt;Researchers &lt;em&gt;have&lt;/em&gt; investigated &lt;a href=&quot;http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;amp;arnumber=219995&amp;amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D219995&quot;&gt;getting wheeled robots to move rapidly through
such environments&lt;/a&gt;, for example. The strategy allowing such autonomy has been a &lt;a href=&quot;https://en.wikipedia.org/wiki/Three-layer_architecture&quot;&gt;three layer (hybrid reactive/deliberative) architecture&lt;/a&gt;. Consider the problem of having a robot navigate amongst furniture to reach a corner of a room. In a three layer architecture, a planner finds a path through the furniture, a reactive component avoids humans and other unforeseen obstacles, and an arbitration layer decides whether to follow the command from the planner or from the reactive component. &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;The planner generates a sequence of desired state configurations, which the controller transforms into commands.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/plannercontroller.png&quot; alt=&quot;The planner generates a sequence of desired state configurations, which the controller transforms into commands.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;In the case of wheeled robots, stopping movement is usually an option (though moving away from the human might be best). There is not usually such a good default command available for walking robots, however. Failing to give a good enough command to a walking robot may cause a multi-hundred pound machine to fall over, breaking itself or hurting or
killing a human.&lt;/p&gt;

&lt;h4 id=&quot;doesn-39-t-the-passage-of-time-invalidate-plans-consisting-of-long-command-sequences&quot;&gt;Doesn&amp;#39;t the passage of time invalidate plans consisting of long command sequences?&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;(This question is relevant to current questions in AI.)&lt;/em&gt; &lt;/p&gt;

&lt;p&gt;Autonomous robots are expected to execute in dynamic, human populated environments. The right level of abstraction is important: &amp;quot;go to the grocery&amp;quot; is likely a safe action for timespans of years, while &amp;quot;pickup the hundred dollar bill from the sidewalk&amp;quot; may be valid for only seconds. &lt;/p&gt;

&lt;h4 id=&quot;how-can-plans-be-adapted-as-a-robot-deviates-from-its-plan&quot;&gt;How can plans be adapted as a robot deviates from its plan?&lt;/h4&gt;

&lt;p&gt;Robots are nondeterministic systems. A robot may move one degree to the right when we command it to move ten degrees to the left. Deviation from a plan is
nearly guaranteed. Depending on the dynamic characteristics of the system and
the commands sent to the robot, deviation may become magnified or attenuated.
The conditions under which a dynamical system &lt;em&gt;stabilizes&lt;/em&gt; to a equilibrium
point or under which deviations do not tend to grow is studied under the
theory of stability of dynamical systems. &lt;/p&gt;

&lt;h4 id=&quot;what-happens-if-a-plan-does-not-consider-effects-sufficiently-far-into-the-future&quot;&gt;What happens if a plan does not consider effects sufficiently far into the future?&lt;/h4&gt;

&lt;p&gt;An engine can be controlled using a simple error feedback strategy: make
throttle closure proportional to speed above the designated setpoint (the greater the speed above the setpoint, the faster the throttle closure). Many robotic systems- particularly underactuated robots- do not admit such a straightforward control strategy. Russ Tedrake, an MIT roboticist, has an entire &lt;a href=&quot;http://underactuated.csail.mit.edu/underactuated.html&quot;&gt;online course&lt;/a&gt; dedicated to this topic. &lt;/p&gt;

&lt;p&gt;If a control strategy does not consider likely next effects of actions, the
control strategy is a &lt;em&gt;Horizon-One&lt;/em&gt; strategy. If the control stategy considers the (long-term) effect, \(n\) steps into the future, of an action, the
control strategy is a &lt;em&gt;Horizon-\(n\)&lt;/em&gt; strategy (\(n = \infty\) is commonly considered).&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;If Black greedily captures the pawn at F3, White can move the rook from D1 to F1, capturing Black&#39;s knight, and likely winning the game. Like playing Chess, robots in dynamic environments must consider longer term consequences of actions.&lt;/caption&gt;
&lt;img src=&quot;http://www.expert-chess-strategies.com/images/poisoned-pawn.jpg&quot; alt=&quot;If Black greedily captures the pawn at F3, White can move the rook from D1 to F1, capturing Black&#39;s knight, and likely winning the game. Like playing Chess, robots in dynamic environments must consider longer term consequences of actions.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h4 id=&quot;a-lookup-table-for-control&quot;&gt;A lookup table for control&lt;/h4&gt;

&lt;p&gt;Since computing controls fast enough is such a problem, one holy grail would
be a lookup table (also known as a &lt;em&gt;control policy&lt;/em&gt;) for dextrous robot control.A sketch of such a table is below. &lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x\(_1\)&lt;/th&gt;
&lt;th&gt;x\(_2\)&lt;/th&gt;
&lt;th&gt;Action&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;td&gt;Move up&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;td&gt;b&lt;/td&gt;
&lt;td&gt;Move down&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;\(\vdots\)&lt;/td&gt;
&lt;td&gt;\(\vdots\)&lt;/td&gt;
&lt;td&gt;\(\vdots\)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;z&lt;/td&gt;
&lt;td&gt;z&lt;/td&gt;
&lt;td&gt;Climb right&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;For robots, such a lookup table would have to consider- at minimum- each 
joint&amp;#39;s angle and speed; these are the &lt;a href=&quot;../dynamical-systems&quot;&gt;state variables&lt;/a&gt;. For a &amp;quot;simple&amp;quot; humanoid robot, over seventy state variables may be required.
To make a table, we have to discretize these state variables. If we assume
ten (only ten!) gradations per variable, the size of the lookup table will be \(10^{70}\) entries. If we want to explore the effect of taking each action &lt;em&gt;just once&lt;/em&gt;, and our robot could somehow perform and evaluate one million actions per second, \(10^{64}\) seconds would be required, This number far exceeds the \(10^{18}\) seconds believed to be the age of the universe. Even if this could be magically computed quickly enough, tem gradations is at least an order of magnitude too coarse! &lt;/p&gt;

&lt;p&gt;What about function approximation approaches, which have been studied heavily
by machine learning researchers? Function approximation can often
approximate highly nonlinear functions using much smaller numbers of basis
functions. How would we know that an approximation would be good enough?&lt;/p&gt;

&lt;p&gt;Even before we ask that question, we need to know how an action is chosen.
As I established above, greedily choosing the action that appears best at any time is often a poor
choice. Similarly, a legged robot might fall over if a good enough &lt;em&gt;sequence&lt;/em&gt; of commands isn&amp;#39;t found. We wish for the robot to act to optimize an objective function over a series of commands. &lt;/p&gt;

&lt;p&gt;This problem falls under the domain of &lt;em&gt;optimal control&lt;/em&gt;, for which globally
optimal solutions are often intractable to find. Current optimal control and
&lt;a href=&quot;https://en.wikipedia.org/wiki/Model_predictive_control&quot;&gt;model predictive control&lt;/a&gt; approaches often focus on algorithms for robustly finding locally optimal solutions (numerically &amp;quot;brittle&amp;quot; algorithms are presently required). Even if a tractable algorithm for solving optimal control problems were to exist, selecting 
a good objective function would remain a challenging problem. &lt;/p&gt;
</description>
        
          <description>&lt;p&gt;This module will discuss some topics for those new to robot programming. 
I&amp;#39;ll start with programming general real-time systems, talk next about
an architecture for a typical robot system- including &lt;em&gt;planners&lt;/em&gt; and 
&lt;em&gt;controllers&lt;/em&gt;, and conclude with many thoughts on the open problem of 
combining reactive and deliberative behaviors for dextrous robots.&lt;/p&gt;
</description>
        
        <pubDate>Thu, 07 Jan 2016 00:00:00 -0500</pubDate>
        <link>http://positronicslab.github.io/blog/robot-programming/</link>
        <guid isPermaLink="true">http://positronicslab.github.io/blog/robot-programming/</guid>
        
        
      </item>
      
    
      
      <item>
        <title>An engineer&#39;s guide to matrices, vectors, and numerical linear algebra</title>
        <description>&lt;p&gt;This material is not meant to replace a proper course in linear algebra, where
important concepts like vector spaces, eigenvalues/eigenvectors, and
spans are defined. My intent is to give you a practical guide to concepts
in matrix arithmetic and numerical linear algebra that I have found useful. &lt;/p&gt;

&lt;h3 id=&quot;matrix-arithmetic&quot;&gt;Matrix arithmetic&lt;/h3&gt;

&lt;h4 id=&quot;matrix-and-vector-addition-and-subtraction&quot;&gt;Matrix and vector addition and subtraction&lt;/h4&gt;

&lt;p&gt;Matrices and vectors can only be added or subtracted if they are of the
same dimensionality. Then, addition and subtraction are performed
elementwise:&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\begin{bmatrix}
u_1 \\
u_2 \\
u_3 
\end{bmatrix} + 
\begin{bmatrix}
v_1 \\
v_2 \\
v_3
\end{bmatrix} =
\begin{bmatrix}
u_1 + v_1 \\
u_2 + v_2 \\
u_3 + v_3 
\end{bmatrix}
\end{equation*}&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\begin{bmatrix}
m_{11} &amp;amp; m_{12} &amp;amp; m_{13} \\
m_{21} &amp;amp; m_{22} &amp;amp; m_{23} \\
m_{31} &amp;amp; m_{32} &amp;amp; m_{33}
\end{bmatrix} + 
\begin{bmatrix}
n_{11} &amp;amp; n_{12} &amp;amp; n_{13} \\
n_{21} &amp;amp; n_{22} &amp;amp; n_{23} \\
n_{31} &amp;amp; n_{32} &amp;amp; n_{33}
\end{bmatrix} = 
\begin{bmatrix}
m_{11} + n_{11} &amp;amp; m_{12} + n_{12} &amp;amp; m_{13} + n_{13} \\
m_{21} + n_{21} &amp;amp; m_{22} + n_{22} &amp;amp; m_{23} + n_{23} \\
m_{31} + n_{31} &amp;amp; m_{32} + n_{32} &amp;amp; m_{33} + n_{33}
\end{bmatrix}
\end{equation*}&lt;/p&gt;

&lt;h4 id=&quot;matrix-and-vector-scaling&quot;&gt;Matrix and vector scaling&lt;/h4&gt;

&lt;p&gt;Matrix and vectors can be scaled by multiplying every element in the matrix or vector by the scalar, as seen below:&lt;/p&gt;

&lt;p&gt;\begin{align*}
\mathbf{M} &amp;amp; \equiv \begin{bmatrix} 
m_{11} &amp;amp; m_{12} &amp;amp; m_{13} \\
m_{21} &amp;amp; m_{22} &amp;amp; m_{23} \\
m_{31} &amp;amp; m_{32} &amp;amp; m_{33} \end{bmatrix} \\ 
s \mathbf{M} &amp;amp; = \begin{bmatrix}
s m_{11} &amp;amp; sm_{12} &amp;amp; sm_{13} \\
s m_{21} &amp;amp; sm_{22} &amp;amp; sm_{23} \\
s m_{31} &amp;amp; sm_{32} &amp;amp; sm_{33} \end{bmatrix} 
\end{align*}
for \(s \in \mathbb{R}\)&lt;/p&gt;

&lt;h4 id=&quot;special-matrices&quot;&gt;Special matrices&lt;/h4&gt;

&lt;p&gt;I will point out three special types of matrices:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\(\mathbf{0} \in \mathbb{R}^{m \times n}\) (the &amp;quot;zero matrix&amp;quot;): a matrix with every entry set to zero&lt;/li&gt;
&lt;li&gt;Diagonal matrices: a square (\(n \times n\)) matrix with nonzero entries placed only on the diagonal (from upper left to lower right)&lt;/li&gt;
&lt;li&gt;\(\mathbf{I} \in \mathbb{R}^{n \times n}\) (the &amp;quot;identity matrix&amp;quot;): a diagonal matrix with every entry on the diagonal set to 1 &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;the-cross-product&quot;&gt;The cross product&lt;/h4&gt;

&lt;p&gt;The cross product operator (\(\times\)) does not fit neatly into matrix/vector arithmetic
and linear algebra, because it applies only to vectors in \(\mathbb{R}^3\).
For two vectors \(\mathbf{a}, \mathbf{b} \in \mathbb{R}^3\), \(\mathbf{a} \times \mathbf{b}\) yields:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A vector orthogonal to \(\mathbf{a}\) and to \(\mathbf{b}\), assuming that \(\mathbf{a}\) and \(\mathbf{b}\) are linearly independent (otherwise, \(\mathbf{a} \times \mathbf{b} = \mathbf{0}\)) &lt;/li&gt;
&lt;li&gt;The negation of \(\mathbf{b} \times \mathbf{a}\); stated again, \(\mathbf{a} \times \mathbf{b} = -\mathbf{b} \times \mathbf{a}\).&lt;/li&gt;
&lt;li&gt;A different vector depending on whether the cross product is a &lt;em&gt;right handed cross product&lt;/em&gt; or a &lt;em&gt;left handed cross product&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The cross product is distributive over addition:&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\mathbf{a} \times (\mathbf{b} + \mathbf{c}) = \mathbf{a} \times \mathbf{b} + \mathbf{a} \times \mathbf{c}
\end{equation*}&lt;/p&gt;

&lt;h5 id=&quot;computing-the-cross-product&quot;&gt;Computing the cross product&lt;/h5&gt;

&lt;p&gt;For the right handed cross product, 
$$
\mathbf{a} \times \mathbf{b} = \begin{bmatrix}
a_2b_3 - a_3b_2 \\
a_3b_1 - a_1b_3 \\
a_1b_2 - a_2b_1
\end{bmatrix}
$$&lt;/p&gt;

&lt;p&gt;The left handed cross product is just the negation of this vector.&lt;/p&gt;

&lt;h4 id=&quot;inner-products&quot;&gt;Inner products&lt;/h4&gt;

&lt;p&gt;The inner product operation (also called the &lt;em&gt;dot product&lt;/em&gt;) between two vectors \(\mathbf{a}\) and \(\mathbf{b}\) is written \(\lt \mathbf{a}, \mathbf{b}\gt \), \(\mathbf{a}^{\textsf{T}}\mathbf{b}\), or \(\mathbf{a} \cdot \mathbf{b}\). The inner product is only defined for vectors of equal dimension and consists of the sum of the products of the corresponding elements from the two vectors. An example is given below:&lt;/p&gt;

&lt;p&gt;\begin{align*}
\mathbf{a} &amp;amp; \equiv \begin{bmatrix} a_1 \\ a_2 \\ a_3 \\ a_4 \end{bmatrix} \\
\mathbf{b} &amp;amp; \equiv \begin{bmatrix} b_1 \\ b_2 \\ b_3 \\ b_4 \end{bmatrix} \\
\mathbf{a}^\textsf{T}\mathbf{b} &amp;amp; = a_1 b_1 + a_2 b_2 + a_3 b_3 + a_4 b_4
\end{align*}&lt;/p&gt;

&lt;p&gt;Some properties of the inner product follow, for real vectors \(\mathbf{a}, \mathbf{b}, \mathbf{c}\):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The dot product is commutative: \(\mathbf{a} \cdot \mathbf{b} = \mathbf{b} \cdot \mathbf{a}\)&lt;/li&gt;
&lt;li&gt;The dot product is distributive over vector addition: \(\mathbf{a} \cdot (\mathbf{b} + \mathbf{c}) = \mathbf{a} \cdot \mathbf{b} + \mathbf{a} \cdot \mathbf{c}\)&lt;/li&gt;
&lt;li&gt;If \(\mathbf{a}, \mathbf{b} \ne \mathbf{0}\), \(\mathbf{a} \cdot \mathbf{b} = 0 \) if and only if \(\mathbf{a}\) and \(\mathbf{b}\) are orthogonal&lt;/li&gt;
&lt;li&gt;\(\mathbf{a} \times (\mathbf{b} \times \mathbf{c}) = \mathbf{b}(\mathbf{a} \cdot \mathbf{c}) - \mathbf{c}(\mathbf{a} \cdot \mathbf{b})\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;matrix-vector-multiplication&quot;&gt;Matrix/vector multiplication&lt;/h4&gt;

&lt;p&gt;Matrix/vector multiplication is identical to multiple inner (dot) product
operations over the rows of \(\mathbf{A}\). Assume we define matrix \(\mathbf{A}\) as follows:&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\mathbf{A} \equiv \begin{bmatrix} \mathbf{a}_1 \\ \vdots \\ \mathbf{a}_m \end{bmatrix}
\end{equation*}&lt;/p&gt;

&lt;p&gt;We can then compute the matix vector product \(\mathbf{Av}\) using \(m\) inner products:&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\mathbf{Av} = \begin{bmatrix} \mathbf{a}_1\mathbf{v} \\ \vdots \\ \mathbf{a}_m\mathbf{v} \end{bmatrix}
\end{equation*} &lt;/p&gt;

&lt;p&gt;As a concrete example, we define:&lt;/p&gt;

&lt;p&gt;\begin{align*}
\mathbf{A} &amp;amp; \equiv \begin{bmatrix} a &amp;amp; b &amp;amp; c \\ d &amp;amp; e &amp;amp; f \end{bmatrix} \\
\mathbf{v} &amp;amp; \equiv \begin{bmatrix} g \\ h \\ i \end{bmatrix} \\
\mathbf{Av} &amp;amp; = \begin{bmatrix} ag + bh + ci \\ dg + eh + fi \end{bmatrix}
\end{align*}&lt;/p&gt;

&lt;h4 id=&quot;matrix-matrix-multiplication&quot;&gt;Matrix/matrix multiplication&lt;/h4&gt;

&lt;p&gt;Matrix/matrix multiplication proceeds the same as matrix-vector multiplication
over multiple columns:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{MN} \equiv \mathbf{M}\begin{bmatrix} \mathbf{n}_1 &amp;amp; \ldots &amp;amp; \mathbf{n}_n \end{bmatrix} \equiv \begin{bmatrix} \mathbf{Mn}_1 &amp;amp; \ldots &amp;amp; \mathbf{Mn}_n \end{bmatrix} 
\end{equation}&lt;/p&gt;

&lt;p&gt;As a concrete example:&lt;/p&gt;

&lt;p&gt;\begin{align*}
\mathbf{M} &amp;amp; \equiv 
\begin{bmatrix} a &amp;amp; b \\ c &amp;amp; d \\ e &amp;amp; f \end{bmatrix} \\
\mathbf{n}_1 &amp;amp; \equiv \begin{bmatrix} g \\ j \end{bmatrix}  \\
\mathbf{n}_2 &amp;amp; \equiv \begin{bmatrix} h \\ k \end{bmatrix}  \\
\mathbf{n}_3 &amp;amp; \equiv \begin{bmatrix} i \\ l \end{bmatrix}  \\
\mathbf{N} &amp;amp; \equiv \begin{bmatrix} \mathbf{n}_1 &amp;amp; \mathbf{n}_2 &amp;amp; \mathbf{n}_3 \end{bmatrix} \\
\mathbf{Mn}_1 &amp;amp; \equiv \begin{bmatrix} ag + bj \\ cg + dj \\ eg + fj \end{bmatrix} \\
\mathbf{Mn}_2 &amp;amp; \equiv \begin{bmatrix} ah + bk \\ ch + dk \\ eh + fk \end{bmatrix} \\
\mathbf{Mn}_3 &amp;amp; \equiv \begin{bmatrix} ai + bl \\ ci + dl \\ ei + fl \end{bmatrix} \\
\mathbf{MN} &amp;amp; \equiv \begin{bmatrix} ag+bj &amp;amp; ah+bk &amp;amp; ai+bl \\ cg+dj &amp;amp; ch+dk &amp;amp; ci + dl\\ eg+fj &amp;amp; eh+fk &amp;amp; ei+fl \end{bmatrix}
\end{align*}&lt;/p&gt;

&lt;p&gt;Note that &lt;strong&gt;matrix multiplication is not commutative&lt;/strong&gt;: \(\mathbf{AB} \ne \mathbf{BA}\) (generally), even when the dimensions are compatible.&lt;/p&gt;

&lt;h4 id=&quot;outer-products&quot;&gt;Outer products&lt;/h4&gt;

&lt;p&gt;The &lt;em&gt;outer product&lt;/em&gt; \(\mathbf{ab}^\mathsf{T}\) of two vectors \(\mathbf{a} \in \mathbb{R}^m\) and \(\mathbf{b} \in \mathbb{R}^n\) is always defined and represents a matrix/matrix multiplication operation between a \(m \times 1\) and a \(1 \times n\) matrix; in other words, normal matrix/matrix multiplication rules apply.&lt;/p&gt;

&lt;h4 id=&quot;matrix-transposition&quot;&gt;Matrix transposition&lt;/h4&gt;

&lt;p&gt;The transpose of a matrix is defined as follows:&lt;/p&gt;

&lt;p&gt;$$
A_{ij}^\mathsf{T} = A_{ji}
$$&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;A depiction of the matrix transpose operation.&lt;/caption&gt;
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/e/e4/Matrix_transpose.gif&quot; alt=&quot;A depiction of the matrix transpose operation.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;where the operator \(^\mathsf{T}\) indicates transposition. This definition implies that an \(m \times n\) matrix becomes an \(n \times m\) matrix. If \(\mathbf{A} = \mathbf{A}^\textsf{T}\) we say that \(\mathbf{A}\) is a &lt;em&gt;symmetric matrix&lt;/em&gt;. &lt;/p&gt;

&lt;p&gt;The following properties apply to matrix transposition for matrices \(\mathbf{A}\) and \(\mathbf{B}\):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\((\mathbf{A}^\textsf{T})^\mathsf{T} = \mathbf{A}\)&lt;/li&gt;
&lt;li&gt;\((\mathbf{A}+\mathbf{B})^\mathsf{T} = \mathbf{A}^\mathsf{T} + \mathbf{B}^\mathsf{T}\)&lt;/li&gt;
&lt;li&gt;\((\mathbf{AB})^\mathsf{T} = \mathbf{B}^\mathsf{T}\mathbf{A}^\mathsf{T}\)&lt;/li&gt;
&lt;li&gt;If \(\mathbf{A}\) has only real entries, then \(\mathbf{A}^\textsf{T}\mathbf{A}\) is a positive semi-definite matrix (see below).&lt;/li&gt;
&lt;li&gt;\(\mathbf{A}\mathbf{A}^\textsf{T}\) is a symmetric matrix.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;matrix-inversion&quot;&gt;Matrix inversion&lt;/h4&gt;

&lt;p&gt;The inverse of a matrix \(\mathbf{A}\), written \(\mathbf{A}^{-1}\), is characterized by the following properties:&lt;/p&gt;

&lt;p&gt;\begin{align}
\mathbf{AA}^{-1} &amp;amp; = \mathbf{I} \\
\mathbf{A}^{-1}\mathbf{A} &amp;amp; = \mathbf{I}
\end{align}&lt;/p&gt;

&lt;p&gt;The inverse exists only if \(\mathbf{A}\) is square and is &lt;em&gt;non-singular&lt;/em&gt;. Singularity can be determined multiple ways (only two are listed below):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If the determinant of the matrix is zero, the matrix is singular. The determinant can be computed using LU factorization (see below).&lt;/li&gt;
&lt;li&gt;If one or more of the singular values of the matrix is zero, the matrix is singular. The singular values can be computed using the singular value decomposition (see below).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following properties apply to matrix inversion for matrices \(\mathbf{A}\) and \(\mathbf{B}\):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\((\mathbf{A}^\textsf{-1})^\mathsf{-1} = \mathbf{A}\)&lt;/li&gt;
&lt;li&gt;\((\mathbf{A}^\textsf{T})^{-1} = (\mathbf{A}^{-1})^\textsf{T}\)&lt;/li&gt;
&lt;li&gt;\((\mathbf{AB})^{-1} = \mathbf{B}^{-1}\mathbf{A}^{-1}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In numerical linear algebra, you almost never need to explicitly form the
inverse of a matrix and &lt;strong&gt;you should avoid explicitly forming the inverse
whenever possible&lt;/strong&gt;: &lt;em&gt;the solution obtained by computing \(\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}\) is considerably slower than back/forward substitution-based methods&lt;/em&gt; (using, e.g., Cholesky factorization, LU factorization, etc.) for solving \(\mathbf{Ax} = \mathbf{b}\).&lt;/p&gt;

&lt;h3 id=&quot;empty-matrices-and-vectors&quot;&gt;Empty matrices and vectors&lt;/h3&gt;

&lt;p&gt;Empty matrices (matrices with one or more dimensions equal to zero) are often useful. They allow formulas,
optimization, etc. to be used without breaking even when the inputs to the
problem are empty: it does not become necessary to use special logic to
handle such corner cases. &lt;/p&gt;

&lt;p&gt;Given scalar \(s \in \mathbb{R}\), empty matrix \(\mathbf{M} \in \mathbb{R}^{m \times n}\) (with one of \(m,n\) equal to zero), empty matrix \(\mathbf{E} \in \mathbb{R}^{0 \times m}\), and empty matrix \(\mathbf{F} \in \mathbb{R}^{n \times 0}\), we have the following rules:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;\(s \mathbf{M} = \mathbf{M}\)&lt;/li&gt;
&lt;li&gt;\(\mathbf{M} + \mathbf{M} = \mathbf{M} \)&lt;/li&gt;
&lt;li&gt;\(\mathbf{EM} = \mathbf{F}^\mathsf{T}\)&lt;/li&gt;
&lt;li&gt;\(\mathbf{MF} = \mathbf{E}^\mathsf{T}\)&lt;/li&gt;
&lt;li&gt;\(\mathbf{E}\mathbf{F}^\mathsf{T} = \mathbf{0}\)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;computational-considerations-for-matrix-multiplication-associativity&quot;&gt;Computational considerations for matrix-multiplication associativity&lt;/h3&gt;

&lt;p&gt;Matrix multiplication &lt;em&gt;is&lt;/em&gt; associative: \((\mathbf{AB})\mathbf{C} = \mathbf{A}(\mathbf{BC})\). The amount of computation required can be very different in the two cases.
Assume that \(\mathbf{A} \in \mathbb{R}^{i \times j}, \mathbf{B} \in \mathbb{R}^{j \times k}, \mathbf{C} \in \mathbb{R}^{k \times m}\). Depending on the order of operation, two very different flop counts are possible:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;\((\mathbf{A}\mathbf{B})\mathbf{C} = O(ijk) + O(ikm)\)&lt;/li&gt;
&lt;li&gt;\(\mathbf{A}(\mathbf{B}\mathbf{C}) = O(jkm) + O(ijm)\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now consider the following variable instances: \(i = 1, j = 2, k = 3, m = 4 \). The asymptotic number of operations in Case (1) will be on the order of 15 flops and in Case (2) will be 32 flops. Takeaway: consider your multiplication ordering. &lt;/p&gt;

&lt;p&gt;Note that in the case of multiplying a chain of matrices and then a vector:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{ABv}
\end{equation}&lt;/p&gt;

&lt;p&gt;One always wants to do the vector multiplication first:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{ABv} = \mathbf{A}(\mathbf{Bv})
\end{equation}&lt;/p&gt;

&lt;p&gt;In many applications, only a few matrices may be multiplied at
one time, meaning that a little logic can be used to determine the order of operations. For longer chains of matrices, one likely wants to use &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_chain_multiplication&quot;&gt;dynamic programming to determine the optimal multiplication order&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;linear-algebra&quot;&gt;Linear algebra&lt;/h3&gt;

&lt;h4 id=&quot;orthogonality&quot;&gt;Orthogonality&lt;/h4&gt;

&lt;p&gt;Vectors \(\mathbf{u}\) and \(\mathbf{v}\) are orthogonal if their dot (inner) product is zero.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Orthogonal&lt;/em&gt; matrices are very convenient because they possess the property that their inverse is equal to their transpose:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{A}^\mathsf{T} = \mathbf{A}^{-1} \textrm{ if } \mathbf{A} \textrm{ orthogonal}
\end{equation}&lt;/p&gt;

&lt;p&gt;Computationally, this means that the inverse can be computed quickly and robustly. An orthogonal matrix has the following properties:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The determinant of the matrix is \(\pm 1\)&lt;/li&gt;
&lt;li&gt;The dot product of any two rows \(i \ne j\) of the matrix is zero&lt;/li&gt;
&lt;li&gt;The dot product of any two columns \(i \ne j\) of the matrix is zero&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;positive-and-negative-definiteness&quot;&gt;Positive and negative definiteness&lt;/h4&gt;

&lt;p&gt;A symmetric matrix \(\mathbf{A}\) is &lt;em&gt;positive definite&lt;/em&gt; if:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{x}^\mathsf{T}\mathbf{A}\mathbf{x} \gt 0
\end{equation}&lt;/p&gt;

&lt;p&gt;for any \(\mathbf{x} \in \mathbb{R}^n\) such that \(\mathbf{x} \ne \mathbf{0}\). This condition is equivalent to saying that a matrix is positive definite if all of its eigenvalues are
positive. If instead,&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{x}^\mathsf{T}\mathbf{A}\mathbf{x} \ge 0
\end{equation}&lt;/p&gt;

&lt;p&gt;for any \(\mathbf{x} \in \mathbb{R}^n\) such that \(\mathbf{x} \ne \mathbf{0}\), we say that the matrix is &lt;em&gt;positive semi-definite&lt;/em&gt;. This condition is equivalent to saying that a matrix is positive semi-definite if all of its eigenvalues are
non-negative. &lt;/p&gt;

&lt;p&gt;Similarly, a matrix is &lt;em&gt;negative definite&lt;/em&gt; if all of its eigenvalues are
strictly negative and &lt;em&gt;negative semi-definite&lt;/em&gt; if all of its eigenvalues
are non-positive. If none of these conditions hold- \(\mathbf{A}\) has both positive and negative eigenvalues- we say that the matrix is &lt;em&gt;indefinite&lt;/em&gt;.&lt;/p&gt;

&lt;h5 id=&quot;checking-positive-definiteness&quot;&gt;Checking positive-definiteness&lt;/h5&gt;

&lt;p&gt;The fastest general way to check for positive-definiteness is using the
Cholesky factorization. If the Cholesky factorization succeeds, the matrix
is positive definite. This approach for checking positive definiteness
is a (significant) constant factor faster than approaches that compute eigenvalues.&lt;/p&gt;

&lt;h5 id=&quot;applications-of-definiteness&quot;&gt;Applications of definiteness&lt;/h5&gt;

&lt;p&gt;Definite matrices have many applications in engineering applications.
As one example, if the Hessian matrix of an objective function is 
positive semi-definite, the function is convex and admits solution
via robust convex optimization codes. As another example, Lyapunov
stability analysis requires negative definiteness of the time derivative
of a Lyapunov function candidate. &lt;/p&gt;

&lt;p&gt;We often wish to avoid indefinite matrices. For example, quadratic programming
with indefinite matrices is NP-hard, while it is polynomial time solvable
with definite matrices. &lt;/p&gt;

&lt;h4 id=&quot;factorizations&quot;&gt;Factorizations&lt;/h4&gt;

&lt;p&gt;I like to consider matrix factorizations in ascending order of computational expense. Correlated with computational expense is numerical robustness. A list of the factorizations follows:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Factorization&lt;/th&gt;
&lt;th&gt;Flops&lt;/th&gt;
&lt;th&gt;Applicability&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Cholesky factorization&lt;/td&gt;
&lt;td&gt;\(n^3/3\)&lt;/td&gt;
&lt;td&gt;Positive-definite matrices only&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LDL\(^\mathsf{T}\) factorization&lt;/td&gt;
&lt;td&gt;\(n^3/2 + O(n^2)\)&lt;/td&gt;
&lt;td&gt;Symmetric matrices only&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LU factorization&lt;/td&gt;
&lt;td&gt;\(2n^3/3\)&lt;/td&gt;
&lt;td&gt;Non-singular matrices (no least squares)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;QR factorization&lt;/td&gt;
&lt;td&gt;\(4n^3/3\)&lt;/td&gt;
&lt;td&gt;Singular and non-singular matrices (least squares ok)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Singular value decomposition&lt;/td&gt;
&lt;td&gt;\(8n^3/3\)&lt;/td&gt;
&lt;td&gt;Singular and non-singular matrices (least squares ok)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4 id=&quot;nullspace&quot;&gt;Nullspace&lt;/h4&gt;

&lt;p&gt;The nullspace \(\mathbf{R}\) of matrix \(\mathbf{A}\) is a nonzero matrix
such that: &lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{AR} = \mathbf{0}
\end{equation}&lt;/p&gt;

&lt;p&gt;The nullspace of a matrix \(\mathbf{A}\) can be determined using the 
rightmost columns of \(\mathbf{V}\) (from the singular value decomposition 
of \(\mathbf{A}\)) that correspond to the zero singular values from 
\(\mathbf{\Sigma}\) (also from the SVD of \(\mathbf{A}\)).&lt;/p&gt;

&lt;h3 id=&quot;matrix-calculus&quot;&gt;Matrix calculus&lt;/h3&gt;

&lt;p&gt;If \(\mathbf{a}, \mathbf{b}\) are functions, then the derivative of (denoted by prime &amp;#39;) the matrix multiplication operation is: \(\mathbf{a}^\mathsf{T}\mathbf{b} = {\mathbf{a}&amp;#39;}^{\mathsf{T}}\mathbf{b} + \mathbf{a}^\mathsf{T} \cdot \mathbf{b}&amp;#39;\)&lt;/p&gt;

&lt;h4 id=&quot;gradient&quot;&gt;Gradient&lt;/h4&gt;

&lt;p&gt;The gradient of a function \(f(\mathbf{x})\), where \(\mathbf{x} \in \mathbb{R}^n\) is the \(n\)-dimensional vector:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\nabla f_{\mathbf{x}} \equiv
\begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\vdots \\
\frac{\partial f}{\partial x_n} \\
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;hessian&quot;&gt;Hessian&lt;/h4&gt;

&lt;p&gt;The Hessian matrix of the same function is the \(n \times n\) matrix of second order partial derivatives:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\nabla f_{\mathbf{xx}} \equiv 
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} &amp;amp; \ldots &amp;amp; \frac{\partial^2 f}{\partial x_1 \partial x_n}\\
\vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} &amp;amp; \ldots &amp;amp; \frac{\partial^2 f}{\partial x_n^2}\\
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;Given that \(\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}\), the Hessian matrix is symmetric (this helps with debugging and can reduce computation when constructing the matrix).&lt;/p&gt;

&lt;h4 id=&quot;jacobian&quot;&gt;Jacobian&lt;/h4&gt;

&lt;p&gt;The Jacobian matrix of a function with vector outputs is the partial derivative
of each function output dimension taken with respect to the partial derivative of each function input dimension. Let us examine a contrived function, \(f : \mathbb{R}^3 \to \mathbb{R}^2\); \(f(.)\) might represent a vector flow for
points in three dimensional Cartesian space. The Jacobian of \(f(.)\) is then:&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\frac{\partial f}{\partial \mathbf{x}} = \begin{bmatrix} 
\frac{\partial f_1}{\partial x_1} &amp;amp; \frac{\partial f_1}{\partial x_2} &amp;amp; \frac{\partial f_1}{\partial x_3} \\
\frac{\partial f_2}{\partial x_1} &amp;amp; \frac{\partial f_2}{\partial x_2} &amp;amp; \frac{\partial f_2}{\partial x_3} 
\end{bmatrix} 
\end{equation*}&lt;/p&gt;

&lt;p&gt;Just like the standard derivative, the Jacobian matrix gives the instantaneous change in \(f(.)\) at \(\mathbf{x}\). &lt;/p&gt;

&lt;h3 id=&quot;least-squares-problems&quot;&gt;Least squares problems&lt;/h3&gt;

&lt;p&gt;Least squares problems are ubiquitous in science and engineering applications.
Solving a least squares problem finds a line (or plane or hyperplane,
in higher dimensions) that minimizes the sum of the squared distance from a
set of points to the line/plane/hyperplane. Another way of saying this is
that least squares seeks to minimize the residual error, i.e., \(||\mathbf{A}\mathbf{x} - \mathbf{b}||\). &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;A depiction of least squares as a mechanical device. Springs are attached from each point to a rod. The amount of force increases quadratically with the distance of each point to the rod.&lt;/caption&gt;
&lt;img src=&quot;http://www.datavis.ca/papers/koln/figs/grsp2.gif&quot; alt=&quot;A depiction of least squares as a mechanical device. Springs are attached from each point to a rod. The amount of force increases quadratically with the distance of each point to the rod.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;Clearly, if \(\mathbf{A}\) is square and non-singular, the solution is \(\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}\). What if \(\mathbf{A} \in \mathbb{R}^{m \times n}\), where \(m \neq n\)? Assume that each row of \(\mathbf{A}\) is linearly independent (i.e., \(\mathbf{A}\) has full row rank) for now. If \(m \gt n\), then there are more
equations than variables and the problem is &lt;em&gt;overdetermined&lt;/em&gt;; we expect \(||\mathbf{Ax} - \mathbf{b}||\) to be nonzero. If \(m \lt n\), then there are more
variables than unknowns and the problem is &lt;em&gt;underdetermined&lt;/em&gt;; we expect there
to be multiple (infinite) assignments to \(\mathbf{x}\) that make \(||\mathbf{Ax} - \mathbf{b}|| = 0\).&lt;/p&gt;

&lt;h5 id=&quot;the-moore-penrose-pseudo-inverse&quot;&gt;The Moore-Penrose pseudo-inverse&lt;/h5&gt;

&lt;p&gt;&lt;em&gt;Two&lt;/em&gt; least squares problems become evident: (1) select \(\mathbf{x}\) such that  \(||\mathbf{Ax} - \mathbf{b}||\) is minimized. If \(||\mathbf{Ax} - \mathbf{b}|| = 0\), then (2) select the solution that minimizes \(||\mathbf{x}||\). Both solutions can be obtained using the Moore-Penrose pseudo-inverse (which is denoted using the operator \(\ ^+\), which has some of the properties of the inverse (I&amp;#39;ll only list a few below):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;\(\mathbf{A}\mathbf{A}^+ = \mathbf{I}\)&lt;/li&gt;
&lt;li&gt;\(\mathbf{A}^+\mathbf{A} = \mathbf{I}\)&lt;/li&gt;
&lt;li&gt;If \(\mathbf{A}\) is invertible, then \(\mathbf{A}^+ = \mathbf{A}^{-1}\).&lt;/li&gt;
&lt;li&gt;\(\mathbf{0}^{-1} = \mathbf{0}^\mathsf{T}\).&lt;/li&gt;
&lt;li&gt;\((\mathbf{A}^+)^+ = \mathbf{A}\).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The &lt;em&gt;left pseudo-inverse&lt;/em&gt; follows:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{A}^+ = {(\mathbf{A}^\mathsf{T}\mathbf{A})}^{-1}\mathbf{A}^\mathsf{T}
\end{equation}&lt;/p&gt;

&lt;p&gt;This pseudo-inverse constitutes a &lt;em&gt;left inverse&lt;/em&gt; (because \(\mathbf{A}^+\mathbf{A} = \mathbf{I}\)), while the &lt;em&gt;right pseudo-inverse&lt;/em&gt; (below)
constitutes a &lt;em&gt;right inverse&lt;/em&gt; (because \(\mathbf{A}\mathbf{A}^+ = \mathbf{I}\)). See &lt;a href=&quot;https://en.wikipedia.org/wiki/Inverse_function#Left_and_right_inverses&quot;&gt;Wikipedia&lt;/a&gt; for an accessible description of left and right inverses. &lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{A}^+ = \mathbf{A}^\mathsf{T}{(\mathbf{A}\mathbf{A}^\textsf{T})}^{-1}
\end{equation}&lt;/p&gt;

&lt;h5 id=&quot;solving-least-squares-problems-using-the-singular-value-decomposition&quot;&gt;Solving least squares problems using the singular value decomposition&lt;/h5&gt;

&lt;p&gt;Still working under the assumption that \(\mathbf{A}\) is of full row rank,
we can also use the singular value decomposition to solve least squares 
problems.&lt;/p&gt;

&lt;p&gt;Recall that the singular value decomposition of \(\mathbf{A} = \mathbf{U}\Sigma^{-1}\mathbf{V}^{\mathsf{T}}\). \(\mathbf{U}\) and \(\mathbf{V}\) are both orthogonal, which means that \(\mathbf{U}^{-1} = \mathbf{U}^{\mathsf{T}}\) and \(\mathbf{V}^{-1} = \mathbf{V}^{\mathsf{T}}\). From the identity above, \(\mathbf{A}^{-1} = \mathbf{V}\Sigma^{-1}\mathbf{U}^{\mathsf{T}}\). For non-square \(\mathbf{A}\), \(\mathbf{A}^+ = \mathbf{V}\mathbf{\Sigma}^+{\mathbf{U}}^\mathsf{T}\), where \(\Sigma^+\) will be defined as follows:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\Sigma_{ij}^+ \leftarrow \begin{cases}\frac{1}{\Sigma_{ij}} &amp;amp; \textrm{if } i = j, \\
0 &amp;amp; \textrm{if } i \ne j. \end{cases}
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;regularization&quot;&gt;Regularization&lt;/h4&gt;

&lt;p&gt;It&amp;#39;s too much to ask that \(\mathbf{A}\), \(\mathbf{A}^\mathsf{T}\mathbf{A}\), or \(\mathbf{A}\mathbf{A}^\textsf{T}\) be always invertible. Even if you 
&lt;em&gt;know&lt;/em&gt; the matrix is invertible (based on some theoretical properties), this does not mean that \(\mathbf{A}\) et al. will be well conditioned. And singular
matrices arise in least squares applications when two or more equations are 
linearly dependent (in other words, regularly).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Regularization&lt;/em&gt; allows one to compute least squares solutions with numerical
robustness.&lt;/p&gt;

&lt;h5 id=&quot;using-regularization-with-the-singular-value-decomposition&quot;&gt;Using regularization with the singular value decomposition&lt;/h5&gt;

&lt;p&gt;The formula for computing \(\mathbf{\Sigma}^+\) can be updated for a numerically robust solution:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\Sigma_{ij}^+ \leftarrow \begin{cases}\frac{1}{\Sigma_{ij}} &amp;amp; \textrm{if } i = j \textrm{ and } \Sigma_{ij} &amp;gt; \epsilon, \\
0 &amp;amp; \textrm{if } i \ne j \textrm{ or } \Sigma_{ij} \le \epsilon. \end{cases}
\end{equation}&lt;/p&gt;

&lt;p&gt;\(\epsilon\) can be set using &lt;em&gt;a priori&lt;/em&gt; knowledge or using the strategy
used in &lt;a href=&quot;http://www.netlib.org/lapack/&quot;&gt;LAPACK&lt;/a&gt;&amp;#39;s library:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\epsilon = \epsilon_{\textrm{mach}} \cdot \max{(m,n)} \cdot \max_{i,j} \Sigma_{ij} 
\end{equation} &lt;/p&gt;

&lt;h5 id=&quot;tikhonov-regularization&quot;&gt;Tikhonov regularization&lt;/h5&gt;

&lt;p&gt;A very simple form of regularization is known as &lt;em&gt;Tikhonov Regularization&lt;/em&gt;
and, in statistics applications, as &lt;em&gt;Ridge Regression&lt;/em&gt;. Consider the system of linear equations:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{A}\mathbf{x} = \mathbf{b}
\end{equation}&lt;/p&gt;

&lt;p&gt;for \(\mathbf{A} \in \mathbb{R}^{n \times n}\) and \(\mathbf{x}, \mathbf{b} \in \mathbb{R}^{n \times 1}\). The
matrix \(\mathbf{A} + \epsilon\mathbf{I}\), where \(\mathbf{I}\) is the identity matrix will always be invertible for sufficiently large \(\epsilon \ge 0\). Tikhonov Regularization solves the nearby system \((\mathbf{A} + \epsilon\mathbf{I})\mathbf{x} = \mathbf{b}\) for \(\mathbf{x}\).&lt;/p&gt;

&lt;p&gt;The obvious question at this point: what is the optimal value of \(\epsilon\)? If \(\epsilon\) is too small, the relevant factorization algorithm (e.g., Cholesky factorization, LU factorization) will fail with an error (at best). If \(epsilon\) is too large, the residual error \(||\mathbf{Ax} - \mathbf{b}||\) will be greater than is necessary. The optimal value of \(\epsilon\) can be
computed with a singular value decomposition, but- one might as well just use the result from the SVD to compute a regularization solution (as described above)- if one goes down this computationally expensive route.&lt;/p&gt;

&lt;p&gt;An effective \(\epsilon\) will keep the condition number (the ratio of
largest to smallest singular values) relatively small: \(10^6\) or so. An 
example of a quick and dirty way to compute \(\epsilon\) is:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\epsilon = \epsilon_{\textrm{mach}} \cdot \max{(m,n)} \cdot ||\mathbf{A}||_\infty
\end{equation} &lt;/p&gt;

&lt;p&gt;where \(\epsilon_{\textrm{mach}}\) is machine epsilon and&lt;/p&gt;

&lt;p&gt;\begin{equation}
||\mathbf{A}||_\infty = \max_{i \in m, j \in n} |A_{ij}|
\end{equation} &lt;/p&gt;
</description>
        
          <description>&lt;p&gt;This material is not meant to replace a proper course in linear algebra, where
important concepts like vector spaces, eigenvalues/eigenvectors, and
spans are defined. My intent is to give you a practical guide to concepts
in matrix arithmetic and numerical linear algebra that I have found useful. &lt;/p&gt;
</description>
        
        <pubDate>Fri, 18 Dec 2015 00:00:00 -0500</pubDate>
        <link>http://positronicslab.github.io/blog/linear-algebra/</link>
        <guid isPermaLink="true">http://positronicslab.github.io/blog/linear-algebra/</guid>
        
        
      </item>
      
    
  </channel>
</rss>
