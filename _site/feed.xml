<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Robotics</title>
    <description>Robotics</description>
    <link>/robotics-course-materials/robotics-course-materials/</link>
    <atom:link href="/robotics-course-materials/robotics-course-materials/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 14 Jan 2016 13:06:30 -0500</pubDate>
    <lastBuildDate>Thu, 14 Jan 2016 13:06:30 -0500</lastBuildDate>
    <generator>Jekyll v2.4.0</generator>
    
      
      <item>
        <title>Programming autonomous robots</title>
        <description>&lt;p&gt;This module will discuss some topics for those new to robot programming. 
I&amp;#39;ll start with programming general real-time systems, talk next about
an architecture for a typical robot system- including &lt;em&gt;planners&lt;/em&gt; and 
&lt;em&gt;controllers&lt;/em&gt;, and conclude with many thoughts on the open problem of 
combining reactive and deliberative behaviors for dextrous robots.&lt;/p&gt;

&lt;h2 id=&quot;programming-real-time-systems&quot;&gt;Programming real-time systems&lt;/h2&gt;

&lt;p&gt;Real-time systems are generally best controlled by a real time operating 
system, which provide specific process scheduling policies and minimal
interrupt latency. The key feature of a real time operating system is its
predictability.&lt;/p&gt;

&lt;p&gt;For controlling robots, the user selects a control frequency (e.g., 100 Hz),
and the real time operating system ensures that this process is run 
regularly- every 10 ms (plus or minus some small amount of error). &lt;em&gt;It is the
programmer&amp;#39;s responsibility to ensure that their process does not overrun the
process&amp;#39; allotted time&lt;/em&gt; (10 ms in the example above). If such an overrun
occurs, the real time operating system will typically generate an exception.
Programmers typically avoid I/O and memory allocation in real-time control
loops, as these procedures are often unpredictable. &lt;/p&gt;

&lt;h2 id=&quot;architecture-of-a-robotic-system&quot;&gt;Architecture of a robotic system&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Autonomous&lt;/em&gt; robotic systems- as opposed to those robots situated in controlled
environments (like factories)- are typically driven using a small number of 
&lt;em&gt;behaviors&lt;/em&gt;, modular components that focus on getting the robot to perform a single task (like avoiding obstacles, homing to a light source, or following a
wall). Many robot architectures are built on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Finite-state_machine&quot;&gt;finite state machine&lt;/a&gt; model, with behaviors representing the states and transitions between behaviors occuring in response to events. &lt;/p&gt;

&lt;p&gt;The reason these interacting behaviors are used, instead of the previously
dominant &lt;em&gt;sense-plan-act&lt;/em&gt; approach, is depicted in &lt;a href=&quot;https://www.youtube.com/watch?v=qXdn6ynwpiI&quot;&gt;this video of an early robot, Shakey&lt;/a&gt;: the robot moves too
slowly to act and react in uncontrolled environments.&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Data flow and computation using the sense-plan-act approach to robot architecture.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/SPA.png&quot; alt=&quot;Data flow and computation using the sense-plan-act approach to robot architecture.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;For example, here is a finite state machine-based architecture for a simple
foraging robot:&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Architecture for a simple foraging robot. Graph nodes represent machine states and transitions represent events and sensory signals. Image from Bristol Robotics Laboratory.&lt;/caption&gt;
&lt;img src=&quot;http://www.brl.ac.uk/images/finite-state-machine.png&quot; alt=&quot;Architecture for a simple foraging robot. Graph nodes represent machine states and transitions represent events and sensory signals. Image from Bristol Robotics Laboratory.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;The figure below shows the architecture of a more sophisticated autonomous 
robot capable of manipulating its environment. The depicted architecture does 
not represent a single task, but rather depicts the&lt;br&gt;
flow of data between software and hardware and real-time and non-real-time
software components. &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;An example of a software architecture for an autonomous robot, the Willow Garage PR2 mobile manipulator.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/robot-software-architecture.png&quot; alt=&quot;An example of a software architecture for an autonomous robot, the Willow Garage PR2 mobile manipulator.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h3 id=&quot;important-components&quot;&gt;Important components&lt;/h3&gt;

&lt;h4 id=&quot;interprocess-communication&quot;&gt;Interprocess communication&lt;/h4&gt;

&lt;p&gt;As can be seen from the figure, communication between multiple computational
processes running simultaneously is important. The particular mechanism
chosen for &lt;em&gt;interprocess communication&lt;/em&gt; (IPC) is critical: latency between
one process transmitting data and the other receiving it should be minimized.
The IPC mechanism depicted in this figure is &lt;em&gt;shared memory&lt;/em&gt;, which is not
necessarily friendly to program with, but is quite fast.&lt;/p&gt;

&lt;h4 id=&quot;sensors&quot;&gt;Sensors&lt;/h4&gt;

&lt;p&gt;The robot depicted in this example possesses several types of sensors:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;LIDAR&lt;/strong&gt;: a time of flight sensing system that operates in the same manner as police using lasers to catch speeding motorists. LIDAR sensors for robots use a rotating laser beam to capture a two-dimensional or three-dimensional depth scan of a surface. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RGB&lt;/strong&gt;: a camera that captures a color (red-green-blue) image from its viewpoint for processing using computer vision algorithms. Computer vision is not a current mainstay of robotic sensing for manipulation, likely because of the significant processing time required, imperfect identification (see Google image search), and susceptibility to sensory noise and artifacts. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kinect&lt;/strong&gt;: a single hardware unit that combines both depth sensing and RGB&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IMU&lt;/strong&gt;: an &lt;em&gt;inertial measurement unit&lt;/em&gt; that combines accelerometers, gyroscopic sensors, magnetometers, and (often) GPS measurements. The accelerometers give linear acceleration. The gyroscopic sensors yield angular acceleration. 
Magnetometers can help determine orientation.&lt;/li&gt;
&lt;/ul&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;A point cloud, as might be produced by LIDAR or the Kinect sensor.&lt;/caption&gt;
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/4/4c/Point_cloud_torus.gif&quot; alt=&quot;A point cloud, as might be produced by LIDAR or the Kinect sensor.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h4 id=&quot;controller&quot;&gt;Controller&lt;/h4&gt;

&lt;p&gt;A &lt;em&gt;controller&lt;/em&gt; is a real-time process that runs at a specified frequency
(commonly between 100 Hz and 10,000 Hz for robots). Controllers attempt to
regulate a dynamical system using a model of the system and/or error
between the desired state of the system and its current state.&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Extremely simple controller (thermostat), actuators (A/C unit and furnace), and sensor (thermometer). The dynamical system is the temperature in the dwelling. The controller operates using a simple _error feedback_ mechanism: a voltage signal is sent to the furnace or A/C unit to increase/decrease temperature until the desired setpoint is reached.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/thermostat.png&quot; alt=&quot;Extremely simple controller (thermostat), actuators (A/C unit and furnace), and sensor (thermometer). The dynamical system is the temperature in the dwelling. The controller operates using a simple _error feedback_ mechanism: a voltage signal is sent to the furnace or A/C unit to increase/decrease temperature until the desired setpoint is reached.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;One of the earliest controllers is the centrifugal governor. A video depiction of the centrifugal governor is shown &lt;a href=&quot;https://www.youtube.com/watch?v=iO0CxOTe4Uk&quot;&gt;here&lt;/a&gt;. An engine causes the governor to rotate, and centrifugal force on the governor- caused by its faster rotation- closes the throttle valve. When the engine slows, the centrifugal force attenuates, and the throttle re-opens. &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;A depiction of a centrifugal governor.&lt;/caption&gt;
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/1/1e/Centrifugal_governor.png&quot; alt=&quot;A depiction of a centrifugal governor.&quot; width=&quot;500&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h4 id=&quot;motor-servos&quot;&gt;Motor servos&lt;/h4&gt;

&lt;p&gt;A motor servo is the electronic interface between the robot software and
the hardware. We can communicate with this interface using low level commands
over the serial port, USB, or CAN bus or using higher level driver software.
The commands we send to the interface may consist of desired current
(for electromagnetic motors), desired torque, or desired position and velocity.
The interface will typically output some data, including joint position and-
sometimes- speed, and torque.&lt;/p&gt;

&lt;h4 id=&quot;planning-modules&quot;&gt;Planning modules&lt;/h4&gt;

&lt;p&gt;Many computational processes require more time to compute than the control 
loop frequency would allow. For example, determining viable foot placements
for a humanoid robot can require on the order of a second. Planning modules
are non-real-time processes that run &amp;quot;in the background&amp;quot;. When a planning
module has completed its computation, it is able to begin feeding inputs
to the controller. &lt;/p&gt;

&lt;h4 id=&quot;perceptual-modules&quot;&gt;Perceptual modules&lt;/h4&gt;

&lt;p&gt;Raw sensory data from robots is generally not usuable without further 
processing. We may wish to remove outlier points from range data, to
&amp;quot;self filter&amp;quot; the robot from RGB or range data, and fuse measurements from
inertial measurement units with other sensors to combat drift.&lt;/p&gt;

&lt;h3 id=&quot;mixing-real-time-reactivity-and-planning&quot;&gt;Mixing real-time reactivity and planning&lt;/h3&gt;

&lt;p&gt;Control strategies for highly dextrous (dynamic) robots, like walking robots,
humanoid robots, and mobile manipulators are still an active area of research. The researchers must usually be focused on basic functionality (standing, walking, opening a door), causing issues like providing near autonomy in dynamic environments (like those populated by humans) to be ignored.  &lt;/p&gt;

&lt;p&gt;Researchers &lt;em&gt;have&lt;/em&gt; investigated &lt;a href=&quot;http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;amp;arnumber=219995&amp;amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D219995&quot;&gt;getting wheeled robots to move rapidly through
such environments&lt;/a&gt;, for example. The strategy allowing such autonomy has been a &lt;a href=&quot;https://en.wikipedia.org/wiki/Three-layer_architecture&quot;&gt;three layer (hybrid reactive/deliberative) architecture&lt;/a&gt;. Consider the problem of having a robot navigate amongst furniture to reach a corner of a room. In a three layer architecture, a planner finds a path through the furniture, a reactive component avoids humans and other unforeseen obstacles, and an arbitration layer decides whether to follow the command from the planner or from the reactive component. &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;The planner generates a sequence of desired state configurations, which the controller transforms into commands.&lt;/caption&gt;
&lt;img src=&quot;../../assets/img/plannercontroller.png&quot; alt=&quot;The planner generates a sequence of desired state configurations, which the controller transforms into commands.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;In the case of wheeled robots, stopping movement is usually an option (though moving away from the human might be best). There is not usually such a good default command available for walking robots, however. Failing to give a good enough command to a walking robot may cause a multi-hundred pound machine to fall over, breaking itself or hurting or
killing a human.&lt;/p&gt;

&lt;h4 id=&quot;doesn-39-t-the-passage-of-time-invalidate-plans-consisting-of-long-command-sequences&quot;&gt;Doesn&amp;#39;t the passage of time invalidate plans consisting of long command sequences?&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;(This question is relevant to current questions in AI.)&lt;/em&gt; &lt;/p&gt;

&lt;p&gt;Autonomous robots are expected to execute in dynamic, human populated environments. The right level of abstraction is important: &amp;quot;go to the grocery&amp;quot; is likely a safe action for timespans of years, while &amp;quot;pickup the hundred dollar bill from the sidewalk&amp;quot; may be valid for only seconds. &lt;/p&gt;

&lt;h4 id=&quot;how-can-plans-be-adapted-as-a-robot-deviates-from-its-plan&quot;&gt;How can plans be adapted as a robot deviates from its plan?&lt;/h4&gt;

&lt;p&gt;Robots are nondeterministic systems. A robot may move one degree to the right when we command it to move ten degrees to the left. Deviation from a plan is
nearly guaranteed. Depending on the dynamic characteristics of the system and
the commands sent to the robot, deviation may become magnified or attenuated.
The conditions under which a dynamical system &lt;em&gt;stabilizes&lt;/em&gt; to a equilibrium
point or under which deviations do not tend to grow is studied under the
theory of stability of dynamical systems. &lt;/p&gt;

&lt;h4 id=&quot;what-happens-if-a-plan-does-not-consider-effects-sufficiently-far-into-the-future&quot;&gt;What happens if a plan does not consider effects sufficiently far into the future?&lt;/h4&gt;

&lt;p&gt;An engine can be controlled using a simple error feedback strategy: make
throttle closure proportional to speed above the designated setpoint (the greater the speed above the setpoint, the faster the throttle closure). Many robotic systems- particularly underactuated robots- do not admit such a straightforward control strategy. Russ Tedrake, an MIT roboticist, has an entire &lt;a href=&quot;http://underactuated.csail.mit.edu/underactuated.html&quot;&gt;online course&lt;/a&gt; dedicated to this topic. &lt;/p&gt;

&lt;p&gt;If a control strategy does not consider likely next effects of actions, the
control strategy is a &lt;em&gt;Horizon-One&lt;/em&gt; strategy. If the control stategy considers the (long-term) effect, \(n\) steps into the future, of an action, the
control strategy is a &lt;em&gt;Horizon-\(n\)&lt;/em&gt; strategy (\(n = \infty\) is commonly considered).&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;If Black greedily captures the pawn at F3, White can move the rook from D1 to F1, capturing Black&#39;s knight, and likely winning the game. Like playing Chess, robots in dynamic environments must consider longer term consequences of actions.&lt;/caption&gt;
&lt;img src=&quot;http://www.expert-chess-strategies.com/images/poisoned-pawn.jpg&quot; alt=&quot;If Black greedily captures the pawn at F3, White can move the rook from D1 to F1, capturing Black&#39;s knight, and likely winning the game. Like playing Chess, robots in dynamic environments must consider longer term consequences of actions.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;h4 id=&quot;a-lookup-table-for-control&quot;&gt;A lookup table for control&lt;/h4&gt;

&lt;p&gt;Since computing controls fast enough is such a problem, one holy grail would
be a lookup table (also known as a &lt;em&gt;control policy&lt;/em&gt;) for dextrous robot control.A sketch of such a table is below. &lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x\(_1\)&lt;/th&gt;
&lt;th&gt;x\(_2\)&lt;/th&gt;
&lt;th&gt;Action&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;td&gt;Move up&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;td&gt;b&lt;/td&gt;
&lt;td&gt;Move down&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;\(\vdots\)&lt;/td&gt;
&lt;td&gt;\(\vdots\)&lt;/td&gt;
&lt;td&gt;\(\vdots\)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;z&lt;/td&gt;
&lt;td&gt;z&lt;/td&gt;
&lt;td&gt;Climb right&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;For robots, such a lookup table would have to consider- at minimum- each 
joint&amp;#39;s angle and speed; these are the &lt;a href=&quot;../dynamical-systems&quot;&gt;state variables&lt;/a&gt;. For a &amp;quot;simple&amp;quot; humanoid robot, over seventy state variables may be required.
To make a table, we have to discretize these state variables. If we assume
ten (only ten!) gradations per variable, the size of the lookup table will be \(10^{70}\) entries. If we want to explore the effect of taking each action &lt;em&gt;just once&lt;/em&gt;, and our robot could somehow perform and evaluate one million actions per second, \(10^{64}\) seconds would be required, This number far exceeds the \(10^{18}\) seconds believed to be the age of the universe. Even if this could be magically computed quickly enough, tem gradations is at least an order of magnitude too coarse! &lt;/p&gt;

&lt;p&gt;What about function approximation approaches, which have been studied heavily
by machine learning researchers? Function approximation can often
approximate highly nonlinear functions using much smaller numbers of basis
functions. How would we know that an approximation would be good enough?&lt;/p&gt;

&lt;p&gt;Even before we ask that question, we need to know how an action is chosen.
As I established above, greedily choosing the action that appears best at any time is often a poor
choice. Similarly, a legged robot might fall over if a good enough &lt;em&gt;sequence&lt;/em&gt; of commands isn&amp;#39;t found. We wish for the robot to act to optimize an objective function over a series of commands. &lt;/p&gt;

&lt;p&gt;This problem falls under the domain of &lt;em&gt;optimal control&lt;/em&gt;, for which globally
optimal solutions are often intractable to find. Current optimal control and
&lt;a href=&quot;https://en.wikipedia.org/wiki/Model_predictive_control&quot;&gt;model predictive control&lt;/a&gt; approaches often focus on algorithms for robustly finding locally optimal solutions (numerically &amp;quot;brittle&amp;quot; algorithms are presently required). Even if a tractable algorithm for solving optimal control problems were to exist, selecting 
a good objective function would remain a challenging problem. &lt;/p&gt;
</description>
        
          <description>&lt;p&gt;This module will discuss some topics for those new to robot programming. 
I&amp;#39;ll start with programming general real-time systems, talk next about
an architecture for a typical robot system- including &lt;em&gt;planners&lt;/em&gt; and 
&lt;em&gt;controllers&lt;/em&gt;, and conclude with many thoughts on the open problem of 
combining reactive and deliberative behaviors for dextrous robots.&lt;/p&gt;
</description>
        
        <pubDate>Thu, 07 Jan 2016 00:00:00 -0500</pubDate>
        <link>/robotics-course-materials/robotics-course-materials/blog/robot-programming/</link>
        <guid isPermaLink="true">/robotics-course-materials/robotics-course-materials/blog/robot-programming/</guid>
        
        
      </item>
      
    
      
      <item>
        <title>An engineer&#39;s guide to matrices, vectors, and numerical linear algebra</title>
        <description>&lt;p&gt;This material is not meant to replace a proper course in linear algebra, where
important concepts like vector spaces, eigenvalues/eigenvectors, and
spans are defined. My intent is to give you a practical guide to concepts
in matrix arithmetic and numerical linear algebra that I have found useful. &lt;/p&gt;

&lt;h3 id=&quot;matrix-arithmetic&quot;&gt;Matrix arithmetic&lt;/h3&gt;

&lt;h4 id=&quot;matrix-and-vector-addition-and-subtraction&quot;&gt;Matrix and vector addition and subtraction&lt;/h4&gt;

&lt;p&gt;Matrices and vectors can only be added or subtracted if they are of the
same dimensionality. Then, addition and subtraction are performed
elementwise:&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\begin{bmatrix}
u_1 \\
u_2 \\
u_3 
\end{bmatrix} + 
\begin{bmatrix}
v_1 \\
v_2 \\
v_3
\end{bmatrix} =
\begin{bmatrix}
u_1 + v_1 \\
u_2 + v_2 \\
u_3 + v_3 
\end{bmatrix}
\end{equation*}&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\begin{bmatrix}
m_{11} &amp;amp; m_{12} &amp;amp; m_{13} \\
m_{21} &amp;amp; m_{22} &amp;amp; m_{23} \\
m_{31} &amp;amp; m_{32} &amp;amp; m_{33}
\end{bmatrix} + 
\begin{bmatrix}
n_{11} &amp;amp; n_{12} &amp;amp; n_{13} \\
n_{21} &amp;amp; n_{22} &amp;amp; n_{23} \\
n_{31} &amp;amp; n_{32} &amp;amp; n_{33}
\end{bmatrix} = 
\begin{bmatrix}
m_{11} + n_{11} &amp;amp; m_{12} + n_{12} &amp;amp; m_{13} + n_{13} \\
m_{21} + n_{21} &amp;amp; m_{22} + n_{22} &amp;amp; m_{23} + n_{23} \\
m_{31} + n_{31} &amp;amp; m_{32} + n_{32} &amp;amp; m_{33} + n_{33}
\end{bmatrix}
\end{equation*}&lt;/p&gt;

&lt;h4 id=&quot;matrix-and-vector-scaling&quot;&gt;Matrix and vector scaling&lt;/h4&gt;

&lt;p&gt;Matrix and vectors can be scaled by multiplying every element in the matrix or vector by the scalar, as seen below:&lt;/p&gt;

&lt;p&gt;\begin{align*}
\mathbf{M} &amp;amp; \equiv \begin{bmatrix} 
m_{11} &amp;amp; m_{12} &amp;amp; m_{13} \\
m_{21} &amp;amp; m_{22} &amp;amp; m_{23} \\
m_{31} &amp;amp; m_{32} &amp;amp; m_{33} \end{bmatrix} \\ 
s \mathbf{M} &amp;amp; = \begin{bmatrix}
s m_{11} &amp;amp; sm_{12} &amp;amp; sm_{13} \\
s m_{21} &amp;amp; sm_{22} &amp;amp; sm_{23} \\
s m_{31} &amp;amp; sm_{32} &amp;amp; sm_{33} \end{bmatrix} 
\end{align*}
for \(s \in \mathbb{R}\)&lt;/p&gt;

&lt;h4 id=&quot;special-matrices&quot;&gt;Special matrices&lt;/h4&gt;

&lt;p&gt;I will point out three special types of matrices:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\(\mathbf{0} \in \mathbb{R}^{m \times n}\) (the &amp;quot;zero matrix&amp;quot;): a matrix with every entry set to zero&lt;/li&gt;
&lt;li&gt;Diagonal matrices: a square (\(n \times n\)) matrix with nonzero entries placed only on the diagonal (from upper left to lower right)&lt;/li&gt;
&lt;li&gt;\(\mathbf{I} \in \mathbb{R}^{n \times n}\) (the &amp;quot;identity matrix&amp;quot;): a diagonal matrix with every entry on the diagonal set to 1 &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;the-cross-product&quot;&gt;The cross product&lt;/h4&gt;

&lt;p&gt;The cross product operator (\(\times\)) does not fit neatly into matrix/vector arithmetic
and linear algebra, because it applies only to vectors in \(\mathbb{R}^3\).
For two vectors \(\mathbf{a}, \mathbf{b} \in \mathbb{R}^3\), \(\mathbf{a} \times \mathbf{b}\) yields:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A vector orthogonal to \(\mathbf{a}\) and to \(\mathbf{b}\), assuming that \(\mathbf{a}\) and \(\mathbf{b}\) are linearly independent (otherwise, \(\mathbf{a} \times \mathbf{b} = \mathbf{0}\)) &lt;/li&gt;
&lt;li&gt;The negation of \(\mathbf{b} \times \mathbf{a}\); stated again, \(\mathbf{a} \times \mathbf{b} = -\mathbf{b} \times \mathbf{a}\).&lt;/li&gt;
&lt;li&gt;A different vector depending on whether the cross product is a &lt;em&gt;right handed cross product&lt;/em&gt; or a &lt;em&gt;left handed cross product&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The cross product is distributive over addition:&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\mathbf{a} \times (\mathbf{b} + \mathbf{c}) = \mathbf{a} \times \mathbf{b} + \mathbf{a} \times \mathbf{c}
\end{equation*}&lt;/p&gt;

&lt;h5 id=&quot;computing-the-cross-product&quot;&gt;Computing the cross product&lt;/h5&gt;

&lt;p&gt;For the right handed cross product, 
$$
\mathbf{a} \times \mathbf{b} = \begin{bmatrix}
a_2b_3 - a_3b_2 \\
a_3b_1 - a_1b_3 \\
a_1b_2 - a_2b_1
\end{bmatrix}
$$&lt;/p&gt;

&lt;p&gt;The left handed cross product is just the negation of this vector.&lt;/p&gt;

&lt;h4 id=&quot;inner-products&quot;&gt;Inner products&lt;/h4&gt;

&lt;p&gt;The inner product operation (also called the &lt;em&gt;dot product&lt;/em&gt;) between two vectors \(\mathbf{a}\) and \(\mathbf{b}\) is written \(\lt \mathbf{a}, \mathbf{b}\gt \), \(\mathbf{a}^{\textsf{T}}\mathbf{b}\), or \(\mathbf{a} \cdot \mathbf{b}\). The inner product is only defined for vectors of equal dimension and consists of the sum of the products of the corresponding elements from the two vectors. An example is given below:&lt;/p&gt;

&lt;p&gt;\begin{align*}
\mathbf{a} &amp;amp; \equiv \begin{bmatrix} a_1 \\ a_2 \\ a_3 \\ a_4 \end{bmatrix} \\
\mathbf{b} &amp;amp; \equiv \begin{bmatrix} b_1 \\ b_2 \\ b_3 \\ b_4 \end{bmatrix} \\
\mathbf{a}^\textsf{T}\mathbf{b} &amp;amp; = a_1 b_1 + a_2 b_2 + a_3 b_3 + a_4 b_4
\end{align*}&lt;/p&gt;

&lt;p&gt;Some properties of the inner product follow, for real vectors \(\mathbf{a}, \mathbf{b}, \mathbf{c}\):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The dot product is commutative: \(\mathbf{a} \cdot \mathbf{b} = \mathbf{b} \cdot \mathbf{a}\)&lt;/li&gt;
&lt;li&gt;The dot product is distributive over vector addition: \(\mathbf{a} \cdot (\mathbf{b} + \mathbf{c}) = \mathbf{a} \cdot \mathbf{b} + \mathbf{a} \cdot \mathbf{c}\)&lt;/li&gt;
&lt;li&gt;If \(\mathbf{a}, \mathbf{b} \ne \mathbf{0}\), \(\mathbf{a} \cdot \mathbf{b} = 0 \) if and only if \(\mathbf{a}\) and \(\mathbf{b}\) are orthogonal&lt;/li&gt;
&lt;li&gt;\(\mathbf{a} \times (\mathbf{b} \times \mathbf{c}) = \mathbf{b}(\mathbf{a} \cdot \mathbf{c}) - \mathbf{c}(\mathbf{a} \cdot \mathbf{b})\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;matrix-vector-multiplication&quot;&gt;Matrix/vector multiplication&lt;/h4&gt;

&lt;p&gt;Matrix/vector multiplication is identical to multiple inner (dot) product
operations over the rows of \(\mathbf{A}\). Assume we define matrix \(\mathbf{A}\) as follows:&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\mathbf{A} \equiv \begin{bmatrix} \mathbf{a}_1 \\ \vdots \\ \mathbf{a}_m \end{bmatrix}
\end{equation*}&lt;/p&gt;

&lt;p&gt;We can then compute the matix vector product \(\mathbf{Av}\) using \(m\) inner products:&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\mathbf{Av} = \begin{bmatrix} \mathbf{a}_1\mathbf{v} \\ \vdots \\ \mathbf{a}_m\mathbf{v} \end{bmatrix}
\end{equation*} &lt;/p&gt;

&lt;p&gt;As a concrete example, we define:&lt;/p&gt;

&lt;p&gt;\begin{align*}
\mathbf{A} &amp;amp; \equiv \begin{bmatrix} a &amp;amp; b &amp;amp; c \\ d &amp;amp; e &amp;amp; f \end{bmatrix} \\
\mathbf{v} &amp;amp; \equiv \begin{bmatrix} g \\ h \\ i \end{bmatrix} \\
\mathbf{Av} &amp;amp; = \begin{bmatrix} ag + bh + ci \\ dg + eh + fi \end{bmatrix}
\end{align*}&lt;/p&gt;

&lt;h4 id=&quot;matrix-matrix-multiplication&quot;&gt;Matrix/matrix multiplication&lt;/h4&gt;

&lt;p&gt;Matrix/matrix multiplication proceeds the same as matrix-vector multiplication
over multiple columns:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{MN} \equiv \mathbf{M}\begin{bmatrix} \mathbf{n}_1 &amp;amp; \ldots &amp;amp; \mathbf{n}_n \end{bmatrix} \equiv \begin{bmatrix} \mathbf{Mn}_1 &amp;amp; \ldots &amp;amp; \mathbf{Mn}_n \end{bmatrix} 
\end{equation}&lt;/p&gt;

&lt;p&gt;As a concrete example:&lt;/p&gt;

&lt;p&gt;\begin{align*}
\mathbf{M} &amp;amp; \equiv 
\begin{bmatrix} a &amp;amp; b \\ c &amp;amp; d \\ e &amp;amp; f \end{bmatrix} \\
\mathbf{n}_1 &amp;amp; \equiv \begin{bmatrix} g \\ j \end{bmatrix}  \\
\mathbf{n}_2 &amp;amp; \equiv \begin{bmatrix} h \\ k \end{bmatrix}  \\
\mathbf{n}_3 &amp;amp; \equiv \begin{bmatrix} i \\ l \end{bmatrix}  \\
\mathbf{N} &amp;amp; \equiv \begin{bmatrix} \mathbf{n}_1 &amp;amp; \mathbf{n}_2 &amp;amp; \mathbf{n}_3 \end{bmatrix} \\
\mathbf{Mn}_1 &amp;amp; \equiv \begin{bmatrix} ag + bj \\ cg + dj \\ eg + fj \end{bmatrix} \\
\mathbf{Mn}_2 &amp;amp; \equiv \begin{bmatrix} ah + bk \\ ch + dk \\ eh + fk \end{bmatrix} \\
\mathbf{Mn}_3 &amp;amp; \equiv \begin{bmatrix} ai + bl \\ ci + dl \\ ei + fl \end{bmatrix} \\
\mathbf{MN} &amp;amp; \equiv \begin{bmatrix} ag+bj &amp;amp; ah+bk &amp;amp; ai+bl \\ cg+dj &amp;amp; ch+dk &amp;amp; ci + dl\\ eg+fj &amp;amp; eh+fk &amp;amp; ei+fl \end{bmatrix}
\end{align*}&lt;/p&gt;

&lt;p&gt;Note that &lt;strong&gt;matrix multiplication is not commutative&lt;/strong&gt;: \(\mathbf{AB} \ne \mathbf{BA}\) (generally), even when the dimensions are compatible.&lt;/p&gt;

&lt;h4 id=&quot;outer-products&quot;&gt;Outer products&lt;/h4&gt;

&lt;p&gt;The &lt;em&gt;outer product&lt;/em&gt; \(\mathbf{ab}^\mathsf{T}\) of two vectors \(\mathbf{a} \in \mathbb{R}^m\) and \(\mathbf{b} \in \mathbb{R}^n\) is always defined and represents a matrix/matrix multiplication operation between a \(m \times 1\) and a \(1 \times n\) matrix; in other words, normal matrix/matrix multiplication rules apply.&lt;/p&gt;

&lt;h4 id=&quot;matrix-transposition&quot;&gt;Matrix transposition&lt;/h4&gt;

&lt;p&gt;The transpose of a matrix is defined as follows:&lt;/p&gt;

&lt;p&gt;$$
A_{ij}^\mathsf{T} = A_{ji}
$$&lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;A depiction of the matrix transpose operation.&lt;/caption&gt;
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/e/e4/Matrix_transpose.gif&quot; alt=&quot;A depiction of the matrix transpose operation.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;where the operator \(^\mathsf{T}\) indicates transposition. This definition implies that an \(m \times n\) matrix becomes an \(n \times m\) matrix. If \(\mathbf{A} = \mathbf{A}^\textsf{T}\) we say that \(\mathbf{A}\) is a &lt;em&gt;symmetric matrix&lt;/em&gt;. &lt;/p&gt;

&lt;p&gt;The following properties apply to matrix transposition for matrices \(\mathbf{A}\) and \(\mathbf{B}\):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\((\mathbf{A}^\textsf{T})^\mathsf{T} = \mathbf{A}\)&lt;/li&gt;
&lt;li&gt;\((\mathbf{A}+\mathbf{B})^\mathsf{T} = \mathbf{A}^\mathsf{T} + \mathbf{B}^\mathsf{T}\)&lt;/li&gt;
&lt;li&gt;\((\mathbf{AB})^\mathsf{T} = \mathbf{B}^\mathsf{T}\mathbf{A}^\mathsf{T}\)&lt;/li&gt;
&lt;li&gt;If \(\mathbf{A}\) has only real entries, then \(\mathbf{A}^\textsf{T}\mathbf{A}\) is a positive semi-definite matrix (see below).&lt;/li&gt;
&lt;li&gt;\(\mathbf{A}\mathbf{A}^\textsf{T}\) is a symmetric matrix.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;matrix-inversion&quot;&gt;Matrix inversion&lt;/h4&gt;

&lt;p&gt;The inverse of a matrix \(\mathbf{A}\), written \(\mathbf{A}^{-1}\), is characterized by the following properties:&lt;/p&gt;

&lt;p&gt;\begin{align}
\mathbf{AA}^{-1} &amp;amp; = \mathbf{I} \\
\mathbf{A}^{-1}\mathbf{A} &amp;amp; = \mathbf{I}
\end{align}&lt;/p&gt;

&lt;p&gt;The inverse exists only if \(\mathbf{A}\) is square and is &lt;em&gt;non-singular&lt;/em&gt;. Singularity can be determined multiple ways (only two are listed below):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If the determinant of the matrix is zero, the matrix is singular. The determinant can be computed using LU factorization (see below).&lt;/li&gt;
&lt;li&gt;If one or more of the singular values of the matrix is zero, the matrix is singular. The singular values can be computed using the singular value decomposition (see below).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following properties apply to matrix inversion for matrices \(\mathbf{A}\) and \(\mathbf{B}\):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\((\mathbf{A}^\textsf{-1})^\mathsf{-1} = \mathbf{A}\)&lt;/li&gt;
&lt;li&gt;\((\mathbf{A}^\textsf{T})^{-1} = (\mathbf{A}^{-1})^\textsf{T}\)&lt;/li&gt;
&lt;li&gt;\((\mathbf{AB})^{-1} = \mathbf{B}^{-1}\mathbf{A}^{-1}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In numerical linear algebra, you almost never need to explicitly form the
inverse of a matrix and &lt;strong&gt;you should avoid explicitly forming the inverse
whenever possible&lt;/strong&gt;: &lt;em&gt;the solution obtained by computing \(\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}\) is considerably slower than back/forward substitution-based methods&lt;/em&gt; (using, e.g., Cholesky factorization, LU factorization, etc.) for solving \(\mathbf{Ax} = \mathbf{b}\).&lt;/p&gt;

&lt;h3 id=&quot;empty-matrices-and-vectors&quot;&gt;Empty matrices and vectors&lt;/h3&gt;

&lt;p&gt;Empty matrices (matrices with one or more dimensions equal to zero) are often useful. They allow formulas,
optimization, etc. to be used without breaking even when the inputs to the
problem are empty: it does not become necessary to use special logic to
handle such corner cases. &lt;/p&gt;

&lt;p&gt;Given scalar \(s \in \mathbb{R}\), empty matrix \(\mathbf{M} \in \mathbb{R}^{m \times n}\) (with one of \(m,n\) equal to zero), empty matrix \(\mathbf{E} \in \mathbb{R}^{0 \times m}\), and empty matrix \(\mathbf{F} \in \mathbb{R}^{n \times 0}\), we have the following rules:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;\(s \mathbf{M} = \mathbf{M}\)&lt;/li&gt;
&lt;li&gt;\(\mathbf{M} + \mathbf{M} = \mathbf{M} \)&lt;/li&gt;
&lt;li&gt;\(\mathbf{EM} = \mathbf{F}^\mathsf{T}\)&lt;/li&gt;
&lt;li&gt;\(\mathbf{MF} = \mathbf{E}^\mathsf{T}\)&lt;/li&gt;
&lt;li&gt;\(\mathbf{E}\mathbf{F}^\mathsf{T} = \mathbf{0}\)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;computational-considerations-for-matrix-multiplication-associativity&quot;&gt;Computational considerations for matrix-multiplication associativity&lt;/h3&gt;

&lt;p&gt;Matrix multiplication &lt;em&gt;is&lt;/em&gt; associative: \((\mathbf{AB})\mathbf{C} = \mathbf{A}(\mathbf{BC})\). The amount of computation required can be very different in the two cases.
Assume that \(\mathbf{A} \in \mathbb{R}^{i \times j}, \mathbf{B} \in \mathbb{R}^{j \times k}, \mathbf{C} \in \mathbb{R}^{k \times m}\). Depending on the order of operation, two very different flop counts are possible:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;\((\mathbf{A}\mathbf{B})\mathbf{C} = O(ijk) + O(ikm)\)&lt;/li&gt;
&lt;li&gt;\(\mathbf{A}(\mathbf{B}\mathbf{C}) = O(jkm) + O(ijm)\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now consider the following variable instances: \(i = 1, j = 2, k = 3, m = 4 \). The asymptotic number of operations in Case (1) will be on the order of 15 flops and in Case (2) will be 32 flops. Takeaway: consider your multiplication ordering. &lt;/p&gt;

&lt;p&gt;Note that in the case of multiplying a chain of matrices and then a vector:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{ABv}
\end{equation}&lt;/p&gt;

&lt;p&gt;One always wants to do the vector multiplication first:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{ABv} = \mathbf{A}(\mathbf{Bv})
\end{equation}&lt;/p&gt;

&lt;p&gt;In many applications, only a few matrices may be multiplied at
one time, meaning that a little logic can be used to determine the order of operations. For longer chains of matrices, one likely wants to use &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_chain_multiplication&quot;&gt;dynamic programming to determine the optimal multiplication order&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;linear-algebra&quot;&gt;Linear algebra&lt;/h3&gt;

&lt;h4 id=&quot;orthogonality&quot;&gt;Orthogonality&lt;/h4&gt;

&lt;p&gt;Vectors \(\mathbf{u}\) and \(\mathbf{v}\) are orthogonal if their dot (inner) product is zero.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Orthogonal&lt;/em&gt; matrices are very convenient because they possess the property that their inverse is equal to their transpose:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{A}^\mathsf{T} = \mathbf{A}^{-1} \textrm{ if } \mathbf{A} \textrm{ orthogonal}
\end{equation}&lt;/p&gt;

&lt;p&gt;Computationally, this means that the inverse can be computed quickly and robustly. An orthogonal matrix has the following properties:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The determinant of the matrix is \(\pm 1\)&lt;/li&gt;
&lt;li&gt;The dot product of any two rows \(i \ne j\) of the matrix is zero&lt;/li&gt;
&lt;li&gt;The dot product of any two columns \(i \ne j\) of the matrix is zero&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;positive-and-negative-definiteness&quot;&gt;Positive and negative definiteness&lt;/h4&gt;

&lt;p&gt;A symmetric matrix \(\mathbf{A}\) is &lt;em&gt;positive definite&lt;/em&gt; if:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{x}^\mathsf{T}\mathbf{A}\mathbf{x} \gt 0
\end{equation}&lt;/p&gt;

&lt;p&gt;for any \(\mathbf{x} \in \mathbb{R}^n\) such that \(\mathbf{x} \ne \mathbf{0}\). This condition is equivalent to saying that a matrix is positive definite if all of its &lt;a href=&quot;https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors&quot;&gt;eigenvalues&lt;/a&gt; are positive; eigenvalues are readily computable with GNU Octave/Matlab (using &lt;code&gt;eig&lt;/code&gt;) and with most libraries for numerical linear algebra. If \(\mathbf{A}\) is not positive definite and instead,&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{x}^\mathsf{T}\mathbf{A}\mathbf{x} \ge 0
\end{equation}&lt;/p&gt;

&lt;p&gt;for any \(\mathbf{x} \in \mathbb{R}^n\) such that \(\mathbf{x} \ne \mathbf{0}\), we say that the matrix is &lt;em&gt;positive semi-definite&lt;/em&gt;. This condition is equivalent to saying that a matrix is positive semi-definite if all of its eigenvalues are
non-negative. &lt;/p&gt;

&lt;p&gt;Similarly, a matrix is &lt;em&gt;negative definite&lt;/em&gt; if all of its eigenvalues are
strictly negative and &lt;em&gt;negative semi-definite&lt;/em&gt; if all of its eigenvalues
are non-positive. If none of these conditions hold- \(\mathbf{A}\) has both positive and negative eigenvalues- we say that the matrix is &lt;em&gt;indefinite&lt;/em&gt;.&lt;/p&gt;

&lt;h5 id=&quot;checking-positive-definiteness&quot;&gt;Checking positive-definiteness&lt;/h5&gt;

&lt;p&gt;The fastest general way to check for positive-definiteness is using the
Cholesky factorization. If the Cholesky factorization succeeds, the matrix
is positive definite. This approach for checking positive definiteness
is a (significant) constant factor faster than approaches that compute eigenvalues.&lt;/p&gt;

&lt;h5 id=&quot;applications-of-definiteness&quot;&gt;Applications of definiteness&lt;/h5&gt;

&lt;p&gt;Definite matrices have many applications in engineering applications.
As one example, if the Hessian matrix of an objective function is 
positive semi-definite, the function is convex and admits solution
via robust convex optimization codes. As another example, Lyapunov
stability analysis requires negative definiteness of the time derivative
of a Lyapunov function candidate. &lt;/p&gt;

&lt;p&gt;We often wish to avoid indefinite matrices. For example, quadratic programming
with indefinite matrices is NP-hard, while it is polynomial time solvable
with definite matrices. &lt;/p&gt;

&lt;h4 id=&quot;factorizations&quot;&gt;Factorizations&lt;/h4&gt;

&lt;p&gt;I like to consider matrix factorizations in ascending order of computational expense. Correlated with computational expense is numerical robustness. A list of the factorizations follows:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Factorization&lt;/th&gt;
&lt;th&gt;Flops&lt;/th&gt;
&lt;th&gt;Applicability&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Cholesky factorization&lt;/td&gt;
&lt;td&gt;\(n^3/3\)&lt;/td&gt;
&lt;td&gt;Positive-definite matrices only&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LDL\(^\mathsf{T}\) factorization&lt;/td&gt;
&lt;td&gt;\(n^3/2 + O(n^2)\)&lt;/td&gt;
&lt;td&gt;Symmetric matrices only&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LU factorization&lt;/td&gt;
&lt;td&gt;\(2n^3/3\)&lt;/td&gt;
&lt;td&gt;Non-singular matrices (no least squares)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;QR factorization&lt;/td&gt;
&lt;td&gt;\(4n^3/3\)&lt;/td&gt;
&lt;td&gt;Singular and non-singular matrices (least squares ok)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Singular value decomposition&lt;/td&gt;
&lt;td&gt;\(8n^3/3\)&lt;/td&gt;
&lt;td&gt;Singular and non-singular matrices (least squares ok)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4 id=&quot;nullspace&quot;&gt;Nullspace&lt;/h4&gt;

&lt;p&gt;The nullspace \(\mathbf{R}\) of matrix \(\mathbf{A}\) is a nonzero matrix
such that: &lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{AR} = \mathbf{0}
\end{equation}&lt;/p&gt;

&lt;p&gt;The nullspace of a matrix \(\mathbf{A}\) can be determined using the 
rightmost columns of \(\mathbf{V}\) (from the singular value decomposition 
of \(\mathbf{A}\)) that correspond to the zero singular values from 
\(\mathbf{\Sigma}\) (also from the SVD of \(\mathbf{A}\)).&lt;/p&gt;

&lt;p&gt;Nullspaces are particularly good for optimization and least squares problems.
For example, the nullspace allows optimizing along multiple criteria in
hierarchical fashion.&lt;/p&gt;

&lt;h3 id=&quot;matrix-calculus&quot;&gt;Matrix calculus&lt;/h3&gt;

&lt;p&gt;If \(\mathbf{a}, \mathbf{b}\) are functions, then the derivative of (denoted by prime &amp;#39;) the matrix multiplication operation is: \(\mathbf{a}^\mathsf{T}\mathbf{b} = {\mathbf{a}&amp;#39;}^{\mathsf{T}}\mathbf{b} + \mathbf{a}^\mathsf{T} \cdot \mathbf{b}&amp;#39;\)&lt;/p&gt;

&lt;h4 id=&quot;gradient&quot;&gt;Gradient&lt;/h4&gt;

&lt;p&gt;The gradient of a function \(f(\mathbf{x})\), where \(\mathbf{x} \in \mathbb{R}^n\) is the \(n\)-dimensional vector:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\nabla f_{\mathbf{x}} \equiv
\begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\vdots \\
\frac{\partial f}{\partial x_n} \\
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;hessian&quot;&gt;Hessian&lt;/h4&gt;

&lt;p&gt;The Hessian matrix of the same function is the \(n \times n\) matrix of second order partial derivatives:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\nabla f_{\mathbf{xx}} \equiv 
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} &amp;amp; \ldots &amp;amp; \frac{\partial^2 f}{\partial x_1 \partial x_n}\\
\vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} &amp;amp; \ldots &amp;amp; \frac{\partial^2 f}{\partial x_n^2}\\
\end{bmatrix}
\end{equation}&lt;/p&gt;

&lt;p&gt;Given that \(\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}\), the Hessian matrix is symmetric (this helps with debugging and can reduce computation when constructing the matrix).&lt;/p&gt;

&lt;h4 id=&quot;jacobian&quot;&gt;Jacobian&lt;/h4&gt;

&lt;p&gt;The Jacobian matrix of a function with vector outputs is the partial derivative
of each function output dimension taken with respect to the partial derivative of each function input dimension. Let us examine a contrived function, \(f : \mathbb{R}^3 \to \mathbb{R}^2\); \(f(.)\) might represent a vector flow for
points in three dimensional Cartesian space. The Jacobian of \(f(.)\) is then:&lt;/p&gt;

&lt;p&gt;\begin{equation*}
\frac{\partial f}{\partial \mathbf{x}} = \begin{bmatrix} 
\frac{\partial f_1}{\partial x_1} &amp;amp; \frac{\partial f_1}{\partial x_2} &amp;amp; \frac{\partial f_1}{\partial x_3} \\
\frac{\partial f_2}{\partial x_1} &amp;amp; \frac{\partial f_2}{\partial x_2} &amp;amp; \frac{\partial f_2}{\partial x_3} 
\end{bmatrix} 
\end{equation*}&lt;/p&gt;

&lt;p&gt;Just like the standard derivative, the Jacobian matrix gives the instantaneous change in \(f(.)\) at \(\mathbf{x}\). &lt;/p&gt;

&lt;h3 id=&quot;least-squares-problems&quot;&gt;Least squares problems&lt;/h3&gt;

&lt;p&gt;Least squares problems are ubiquitous in science and engineering applications.
Solving a least squares problem finds a line (or plane or hyperplane,
in higher dimensions) that minimizes the sum of the squared distance from a
set of points to the line/plane/hyperplane. Another way of saying this is
that least squares seeks to minimize the residual error, i.e., \(||\mathbf{A}\mathbf{x} - \mathbf{b}||\). &lt;/p&gt;

&lt;P&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;A depiction of least squares as a mechanical device. Springs are attached from each point to a rod. The amount of force increases quadratically with the distance of each point to the rod.&lt;/caption&gt;
&lt;img src=&quot;http://www.datavis.ca/papers/koln/figs/grsp2.gif&quot; alt=&quot;A depiction of least squares as a mechanical device. Springs are attached from each point to a rod. The amount of force increases quadratically with the distance of each point to the rod.&quot; width=&quot;&quot;/&gt;
&lt;/table&gt;
&lt;/P&gt;

&lt;p&gt;Clearly, if \(\mathbf{A}\) is square and non-singular, the solution is \(\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}\). What if \(\mathbf{A} \in \mathbb{R}^{m \times n}\), where \(m \neq n\)? Assume that each row of \(\mathbf{A}\) is linearly independent (i.e., \(\mathbf{A}\) has full row rank) for now. If \(m \gt n\), then there are more
equations than variables and the problem is &lt;em&gt;overdetermined&lt;/em&gt;; we expect \(||\mathbf{Ax} - \mathbf{b}||\) to be nonzero. If \(m \lt n\), then there are more
variables than unknowns and the problem is &lt;em&gt;underdetermined&lt;/em&gt;; we expect there
to be multiple (infinite) assignments to \(\mathbf{x}\) that make \(||\mathbf{Ax} - \mathbf{b}|| = 0\).&lt;/p&gt;

&lt;h5 id=&quot;the-moore-penrose-pseudo-inverse&quot;&gt;The Moore-Penrose pseudo-inverse&lt;/h5&gt;

&lt;p&gt;&lt;em&gt;Two&lt;/em&gt; least squares problems become evident: (1) select \(\mathbf{x}\) such that  \(||\mathbf{Ax} - \mathbf{b}||\) is minimized. If \(||\mathbf{Ax} - \mathbf{b}|| = 0\), then (2) select the solution that minimizes \(||\mathbf{x}||\). Both solutions can be obtained using the Moore-Penrose pseudo-inverse (which is denoted using the operator \(\ ^+\), which has some of the properties of the inverse (I&amp;#39;ll only list a few below):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;\(\mathbf{A}\mathbf{A}^+ = \mathbf{I}\)&lt;/li&gt;
&lt;li&gt;\(\mathbf{A}^+\mathbf{A} = \mathbf{I}\)&lt;/li&gt;
&lt;li&gt;If \(\mathbf{A}\) is invertible, then \(\mathbf{A}^+ = \mathbf{A}^{-1}\).&lt;/li&gt;
&lt;li&gt;\(\mathbf{0}^{-1} = \mathbf{0}^\mathsf{T}\).&lt;/li&gt;
&lt;li&gt;\((\mathbf{A}^+)^+ = \mathbf{A}\).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The &lt;em&gt;left pseudo-inverse&lt;/em&gt; follows:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{A}^+ = {(\mathbf{A}^\mathsf{T}\mathbf{A})}^{-1}\mathbf{A}^\mathsf{T}
\end{equation}&lt;/p&gt;

&lt;p&gt;This pseudo-inverse constitutes a &lt;em&gt;left inverse&lt;/em&gt; (because \(\mathbf{A}^+\mathbf{A} = \mathbf{I}\)), while the &lt;em&gt;right pseudo-inverse&lt;/em&gt; (below)
constitutes a &lt;em&gt;right inverse&lt;/em&gt; (because \(\mathbf{A}\mathbf{A}^+ = \mathbf{I}\)). See &lt;a href=&quot;https://en.wikipedia.org/wiki/Inverse_function#Left_and_right_inverses&quot;&gt;Wikipedia&lt;/a&gt; for an accessible description of left and right inverses. &lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{A}^+ = \mathbf{A}^\mathsf{T}{(\mathbf{A}\mathbf{A}^\textsf{T})}^{-1}
\end{equation}&lt;/p&gt;

&lt;h5 id=&quot;solving-least-squares-problems-using-the-singular-value-decomposition&quot;&gt;Solving least squares problems using the singular value decomposition&lt;/h5&gt;

&lt;p&gt;Still working under the assumption that \(\mathbf{A}\) is of full row rank,
we can also use the singular value decomposition to solve least squares 
problems.&lt;/p&gt;

&lt;p&gt;Recall that the singular value decomposition of \(\mathbf{A} = \mathbf{U}\Sigma^{-1}\mathbf{V}^{\mathsf{T}}\). \(\mathbf{U}\) and \(\mathbf{V}\) are both orthogonal, which means that \(\mathbf{U}^{-1} = \mathbf{U}^{\mathsf{T}}\) and \(\mathbf{V}^{-1} = \mathbf{V}^{\mathsf{T}}\). From the identity above, \(\mathbf{A}^{-1} = \mathbf{V}\Sigma^{-1}\mathbf{U}^{\mathsf{T}}\). For non-square \(\mathbf{A}\), \(\mathbf{A}^+ = \mathbf{V}\mathbf{\Sigma}^+{\mathbf{U}}^\mathsf{T}\), where \(\Sigma^+\) will be defined as follows:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\Sigma_{ij}^+ \leftarrow \begin{cases}\frac{1}{\Sigma_{ij}} &amp;amp; \textrm{if } i = j, \\
0 &amp;amp; \textrm{if } i \ne j. \end{cases}
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;regularization&quot;&gt;Regularization&lt;/h4&gt;

&lt;p&gt;It&amp;#39;s too much to ask that \(\mathbf{A}\), \(\mathbf{A}^\mathsf{T}\mathbf{A}\), or \(\mathbf{A}\mathbf{A}^\textsf{T}\) be always invertible. Even if you 
&lt;em&gt;know&lt;/em&gt; the matrix is invertible (based on some theoretical properties), this does not mean that \(\mathbf{A}\) et al. will be well conditioned. And singular
matrices arise in least squares applications when two or more equations are 
linearly dependent (in other words, regularly).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Regularization&lt;/em&gt; allows one to compute least squares solutions with numerical
robustness.&lt;/p&gt;

&lt;h5 id=&quot;using-regularization-with-the-singular-value-decomposition&quot;&gt;Using regularization with the singular value decomposition&lt;/h5&gt;

&lt;p&gt;The formula for computing \(\mathbf{\Sigma}^+\) can be updated for a numerically robust solution:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\Sigma_{ij}^+ \leftarrow \begin{cases}\frac{1}{\Sigma_{ij}} &amp;amp; \textrm{if } i = j \textrm{ and } \Sigma_{ij} &amp;gt; \epsilon, \\
0 &amp;amp; \textrm{if } i \ne j \textrm{ or } \Sigma_{ij} \le \epsilon. \end{cases}
\end{equation}&lt;/p&gt;

&lt;p&gt;\(\epsilon\) can be set using &lt;em&gt;a priori&lt;/em&gt; knowledge or using the strategy
used in &lt;a href=&quot;http://www.netlib.org/lapack/&quot;&gt;LAPACK&lt;/a&gt;&amp;#39;s library:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\epsilon = \epsilon_{\textrm{mach}} \cdot \max{(m,n)} \cdot \max_{i,j} \Sigma_{ij} 
\end{equation} &lt;/p&gt;

&lt;h5 id=&quot;tikhonov-regularization&quot;&gt;Tikhonov regularization&lt;/h5&gt;

&lt;p&gt;A very simple form of regularization is known as &lt;em&gt;Tikhonov Regularization&lt;/em&gt;
and, in statistics applications, as &lt;em&gt;Ridge Regression&lt;/em&gt;. Consider the system of linear equations:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathbf{A}\mathbf{x} = \mathbf{b}
\end{equation}&lt;/p&gt;

&lt;p&gt;for \(\mathbf{A} \in \mathbb{R}^{n \times n}\) and \(\mathbf{x}, \mathbf{b} \in \mathbb{R}^{n \times 1}\). The
matrix \(\mathbf{A} + \epsilon\mathbf{I}\), where \(\mathbf{I}\) is the identity matrix will always be invertible for sufficiently large \(\epsilon \ge 0\). Tikhonov Regularization solves the nearby system \((\mathbf{A} + \epsilon\mathbf{I})\mathbf{x} = \mathbf{b}\) for \(\mathbf{x}\).&lt;/p&gt;

&lt;p&gt;The obvious question at this point: what is the optimal value of \(\epsilon\)? If \(\epsilon\) is too small, the relevant factorization algorithm (e.g., Cholesky factorization, LU factorization) will fail with an error (at best). If \(epsilon\) is too large, the residual error \(||\mathbf{Ax} - \mathbf{b}||\) will be greater than is necessary. The optimal value of \(\epsilon\) can be
computed with a singular value decomposition, but- one might as well just use the result from the SVD to compute a regularization solution (as described above)- if one goes down this computationally expensive route.&lt;/p&gt;

&lt;p&gt;An effective \(\epsilon\) will keep the condition number (the ratio of
largest to smallest singular values) relatively small: \(10^6\) or so. An 
example of a quick and dirty way to compute \(\epsilon\) is:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\epsilon = \epsilon_{\textrm{mach}} \cdot \max{(m,n)} \cdot ||\mathbf{A}||_\infty
\end{equation} &lt;/p&gt;

&lt;p&gt;where \(\epsilon_{\textrm{mach}}\) is machine epsilon and&lt;/p&gt;

&lt;p&gt;\begin{equation}
||\mathbf{A}||_\infty = \max_{i \in m, j \in n} |A_{ij}|
\end{equation} &lt;/p&gt;
</description>
        
          <description>&lt;p&gt;This material is not meant to replace a proper course in linear algebra, where
important concepts like vector spaces, eigenvalues/eigenvectors, and
spans are defined. My intent is to give you a practical guide to concepts
in matrix arithmetic and numerical linear algebra that I have found useful. &lt;/p&gt;
</description>
        
        <pubDate>Fri, 18 Dec 2015 00:00:00 -0500</pubDate>
        <link>/robotics-course-materials/robotics-course-materials/blog/linear-algebra/</link>
        <guid isPermaLink="true">/robotics-course-materials/robotics-course-materials/blog/linear-algebra/</guid>
        
        
      </item>
      
    
      
      <item>
        <title>C++ overview and OpenSceneGraph introduction</title>
        <description>&lt;p&gt;&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;&lt;/span&gt;
This learning module will provide an overview of C++, targeted toward
those with some background in Java or C.  &lt;/p&gt;

&lt;h2 id=&quot;overview-of-c&quot;&gt;Overview of C++&lt;/h2&gt;

&lt;h3 id=&quot;c-data-types&quot;&gt;C++ data types&lt;/h3&gt;

&lt;p&gt;See &lt;a href=&quot;http://en.cppreference.com/w/cpp/language/types&quot;&gt;this page&lt;/a&gt; for a
description of all C++ primitive data types, including concepts like minimum and maximum numbers, not-a-number,
and infinity.  The most commonly used types are
&lt;code&gt;void&lt;/code&gt;, &lt;code&gt;bool&lt;/code&gt;, &lt;code&gt;char&lt;/code&gt;, &lt;code&gt;int&lt;/code&gt;, &lt;code&gt;unsigned&lt;/code&gt;, &lt;code&gt;long&lt;/code&gt;, &lt;code&gt;float&lt;/code&gt;, and &lt;code&gt;double&lt;/code&gt;.
Since C++ is &amp;quot;close to the metal&amp;quot;, like C, it can help you to know the
number of bits used for each representation, &lt;em&gt;which can change depending on
machine architecture&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;some-differences-between-c-and-java&quot;&gt;Some differences between C++ and Java&lt;/h3&gt;

&lt;p&gt;Notes below will be useful even to those programmers without a background in
Java.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;C++ uses &lt;code&gt;bool&lt;/code&gt; for a Boolean type (Java calls this &lt;code&gt;boolean&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Java uses &lt;code&gt;System.out.println&lt;/code&gt; for output to &lt;code&gt;stdout&lt;/code&gt;. C++ uses &lt;code&gt;std::cout&lt;/code&gt;, the &lt;code&gt;&amp;lt;&amp;lt;&lt;/code&gt; operator, and &lt;code&gt;std::endl&lt;/code&gt;. The Java statement &lt;code&gt;System.out.println(&amp;quot;Hello world!&amp;quot;);&lt;/code&gt; would be &lt;code&gt;std::cout &amp;lt;&amp;lt; &amp;quot;Hello world!&amp;quot; &amp;lt;&amp;lt; std::endl;&lt;/code&gt; in C++.&lt;/li&gt;
&lt;li&gt;Java forces you to allocate non-primitive types on the heap, where C++ allows you to allocate non-primitive types on the stack (the latter is faster and more amenable to real-time performance). &lt;/li&gt;
&lt;li&gt;Java automatically de-allocates memory (using relatively slow garbage collection).&lt;/li&gt;
&lt;li&gt;Array allocation is slightly different. Arrays are allocated in Java like &lt;code&gt;int[] array = new int[20]&lt;/code&gt;. Arrays are allocated in C++ like &lt;code&gt;int* array = new int[20]&lt;/code&gt;. &lt;/li&gt;
&lt;li&gt;In Java, a member function is defined like &lt;code&gt;public void tabulateScores()&lt;/code&gt; while the function would be declared in C++ like &lt;code&gt;public: void tabulateScores()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;All primitive types (&lt;code&gt;int&lt;/code&gt;, &lt;code&gt;float&lt;/code&gt;, etc.) are &lt;a href=&quot;http://courses.washington.edu/css342/zander/css332/passby.html&quot;&gt;passed by value&lt;/a&gt; to functions and all non-primitive types are &lt;a href=&quot;http://courses.washington.edu/css342/zander/css332/passby.html&quot;&gt;passed by reference&lt;/a&gt;. C++ gives the option to pass any type by reference or by value to a function.&lt;/li&gt;
&lt;li&gt;C++ requires you to &lt;a href=&quot;http://stackoverflow.com/questions/4757565/c-forward-declaration&quot;&gt;declare&lt;/a&gt; function prototypes and classes when you refer to them (before they have been &lt;em&gt;defined&lt;/em&gt;- fleshed out). If you refer to a class before it has been defined, C++ requires you to do a &lt;a href=&quot;http://stackoverflow.com/questions/4757565/c-forward-declaration&quot;&gt;forward declaration&lt;/a&gt;. Java was smart to avoid declarations, in my opinion. &lt;/li&gt;
&lt;li&gt;C++ does not have &lt;em&gt;interfaces&lt;/em&gt; but it does have &lt;a href=&quot;https://en.wikipedia.org/wiki/Virtual_function&quot;&gt;pure virtual functions&lt;/a&gt;,
which serve an identical purpose.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some resources for Java programmers to learn C++:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.horstmann.com/ccj2/ccjapp3.html&quot;&gt;Moving from Java to C++&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://cs.brown.edu/courses/cs123/docs/java_to_cpp.shtml&quot;&gt;Java to C++ Transition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;object-oriented-programming-in-c&quot;&gt;Object-oriented programming in C++&lt;/h3&gt;

&lt;p&gt;Object oriented programming (OOP) is a programming model centered around data and
the functions used to operate on that data rather than &lt;em&gt;procedural programming languages&lt;/em&gt; (like C) that focus on decomposing a task into subroutines (procedures). A tutorial to OOP in C++ can be found &lt;a href=&quot;http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-088-introduction-to-c-memory-management-and-c-object-oriented-programming-january-iap-2010/lecture-notes/MIT6_088IAP10_lec04.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;memory-allocation-and-shared-pointers&quot;&gt;Memory allocation and shared pointers&lt;/h3&gt;

&lt;p&gt;One price you pay for the additional speed and control that C++ offers is
the need to manage heap memory allocation and deallocation. Memory is allocated
from the heap using the &lt;code&gt;new&lt;/code&gt; operator:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;int* x;       // x is a pointer
x = new int;  // allocated memory for x on the heap
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and memory must be deallocated from the heap using the &lt;code&gt;delete&lt;/code&gt; operator:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;delete [] x;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For every &lt;code&gt;new&lt;/code&gt; in your code, there should be a matching &lt;code&gt;delete&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&quot;shared-pointers&quot;&gt;Shared pointers&lt;/h4&gt;

&lt;p&gt;Shared pointers provide automatic memory deallocation. &lt;em&gt;I suggest using them
instead of regular pointers for memory allocation/deallocation.&lt;/em&gt; The idea
is simple- when no more references point to a block of memory, the block 
is deallocated- though a few caveats exist. Shared pointers work like this:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;shared_ptr&amp;lt;int&amp;gt; x;              // x is a shared pointer to an int
x = shared_ptr&amp;lt;int&amp;gt;(new int);   // allocated memory
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;There no longer needs to be a matching &lt;code&gt;delete&lt;/code&gt; statement. The advantages
of shared pointers over garbage collection are that the former is considerably
faster and that memory is reclaimed as soon as possible. The disadvantage is
that circular pointer references must be explicitly managed by the programmer
or memory leaks will occur.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Using the following example class definitions will result in a memory leak&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;class B; // the forward reference is necessary

class A
{
  shared_ptr&amp;lt;B&amp;gt; b;
};

class B
{
  shared_ptr&amp;lt;A&amp;gt; a;
};
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This situation is fixable using a &lt;a href=&quot;http://www.boost.org/doc/libs/1_58_0/libs/smart_ptr/weak_ptr.htm&quot;&gt;weak pointer&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;class B; // the forward reference is necessary

class A
{
  shared_ptr&amp;lt;B&amp;gt; b;
};

class B
{
  weak_ptr&amp;lt;A&amp;gt; a;  // the weak pointer breaks the circular reference
};
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&quot;passing-by-reference-and-passing-by-value&quot;&gt;Passing by reference and passing by value&lt;/h3&gt;

&lt;p&gt;(&lt;em&gt;ED: I have seen this advice somewhere but cannot locate it at the present moment. I will cite my source when I find it again.&lt;/em&gt;)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pass variables by reference when the function is to modify the variable. 
As a matter of fact, indicating that the variable is passed by reference
&lt;em&gt;and without the &lt;code&gt;const&lt;/code&gt;&lt;/em&gt; keyword indicates to the caller that the function is &lt;em&gt;expected&lt;/em&gt; to modify the variable.&lt;/li&gt;
&lt;li&gt;Else, for primitive types, pass by value &lt;/li&gt;
&lt;li&gt;Pass objects by reference and use the &lt;code&gt;const&lt;/code&gt; modifier when the object uses more memory than a pointer (64-bits on most systems) and the function is not expected to change the object (e.g., &lt;code&gt;void sum_inertias(const SpatialRBInertiad&amp;amp; J)&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Pass objects by value when the object uses less memory than a pointer and the function is not expected to modify the object &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;compiling-linking-c-on-unix-type-systems&quot;&gt;Compiling/linking C++ on Unix-type systems&lt;/h3&gt;

&lt;p&gt;Whether producing an executable file or a software library, C++ requires
two processes: &lt;em&gt;compiling&lt;/em&gt; the C++ source code into machine code 
(&amp;quot;object files&amp;quot;) and &lt;em&gt;linking&lt;/em&gt; the object files together (which resolves 
symbolic references to functions and data). A description of the compilation
and linking processes is &lt;a href=&quot;http://stackoverflow.com/questions/6264249/how-does-the-compilation-linking-process-work&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As a very simple example,&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;g++ -c hello.cpp -o hello.o
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;compiles &lt;code&gt;hello.cpp&lt;/code&gt; to produce &lt;code&gt;hello.o&lt;/code&gt; and&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;g++ hello.o -o hello
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;links &lt;code&gt;hello.o&lt;/code&gt; with the C++ standard libraries to produce the executable &lt;code&gt;hello&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I recommend getting and learning &lt;a href=&quot;http://cmake.org&quot;&gt;CMake&lt;/a&gt; to build your projects, which can take care of the compiling and linking process for you automatically. Otherwise, you have to compile your source files manually, forcing you to remember all of the arcane command line options, and then manually link your objects together. One warning: &lt;a href=&quot;http://stackoverflow.com/questions/45135/why-does-the-order-in-which-libraries-are-linked-sometimes-cause-errors-in-gcc&quot;&gt;the linker (g++) is sensitive to the order that libraries and object files are specified on the command line on Linux systems&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;newer-language-features&quot;&gt;Newer language features&lt;/h3&gt;

&lt;p&gt;C++ continues to support more and more features over time. The language&amp;#39;s evolution reminds me of this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://images.knifecenter.com/knifecenter/wenger/images/WR16999a.jpg&quot; alt=&quot;Big swiss army knife&quot;&gt;&lt;/p&gt;

&lt;p&gt;because few language features are ever removed. The 
&lt;a href=&quot;http://www.cplusplus.com/reference/stl/&quot;&gt;C++ standard template library&lt;/a&gt; contains a number of useful data structures- including vectors, linked lists, queues, stacks, sets, and maps- and algorithms (finding maximum elements, binary search, sorting, and more). You will be able to increase your programming proficiency in C++ many fold when you understand the concept of &lt;a href=&quot;http://www.cs.northwestern.edu/%7Eriesbeck/programming/c++/stl-iterators.html&quot;&gt;iterators&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A staging ground for many C++ algorithms that often make their way into
the language is &lt;a href=&quot;http://www.boost.org&quot;&gt;Boost&lt;/a&gt;. This functionality goes part
of the way toward replicating the utility of other languages&amp;#39; standard 
libraries (Python and Java in particular). &lt;/p&gt;

&lt;h3 id=&quot;templates&quot;&gt;Templates&lt;/h3&gt;

&lt;p&gt;Templates allow us to avoid code like this:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;void swap(int&amp;amp; x, int&amp;amp; y)
{
  int tmp = x;
  x = y;
  y = tmp;
}

void swap(float&amp;amp; x, float&amp;amp; y)
{
  float tmp = x;
  x = y;
  y = tmp;
}

.
.
.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can do this instead:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;template &amp;lt;typename T&amp;gt;
void swap(T&amp;amp; x, T&amp;amp; y)
{
  T tmp = x;
  x = y;
  y = tmp;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This saves typing and, more importantly, reduces possibility of bugs from
copy and paste (a great way to introduce bugs in programming). On the downside,
templates make code a little harder to read, make it slower to compile,
and tends to generate really hard to read compiler error messages for syntax errors (&lt;a href=&quot;https://isocpp.org/wiki/faq/templates#template-error-msgs&quot;&gt;see this part of the C++ FAQ for a fix&lt;/a&gt;). Learning templates well will help you understand the
Boost, the STL, and will give you the ability to read the majority of C++ code.&lt;/p&gt;

&lt;h3 id=&quot;exceptions&quot;&gt;Exceptions&lt;/h3&gt;

&lt;p&gt;Before exceptions, programmers would check for errors like this:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;FILE* fp = fopen(&amp;quot;/tmp/dat&amp;quot;, &amp;quot;w&amp;quot;);
if (!fp)
{
  std::cerr &amp;lt;&amp;lt; &amp;quot;Unable to open file!&amp;quot; &amp;lt;&amp;lt; std::endl;
  return false;
}

...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using exceptions we check for errors like this:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;try
{
  fp = open(&amp;quot;/tmp/dat&amp;quot;);
}
catch (IOException e)
{
  std::cerr &amp;lt;&amp;lt; &amp;quot;Unable to open file!&amp;quot; &amp;lt;&amp;lt; std::endl;
  return false;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;One advantage is that if we don&amp;#39;t care about the error at this level- it&amp;#39;s
apparent that we already signal to the calling function that there was a 
problem by 
the &lt;code&gt;return false&lt;/code&gt; statement- then we can keep our code very neat by doing
this instead:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;fp = open(&amp;quot;/tmp/dat&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now if we do not &amp;quot;catch&amp;quot; the exception, the function above is responsible
for catching it, on up the &lt;a href=&quot;https://en.wikipedia.org/wiki/Call_stack&quot;&gt;call stack&lt;/a&gt;, until- if the &lt;code&gt;main&lt;/code&gt; function does not catch it- the exception will cause
the program to terminate with an error.&lt;/p&gt;

&lt;p&gt;A commentor on &lt;a href=&quot;http://stackoverflow.com/questions/196522/in-c-what-are-the-benefits-of-using-exceptions-and-try-catch-instead-of-just&quot;&gt;Stack overflow&lt;/a&gt; indicates two benefits:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;They can&amp;#39;t be ignored: you must deal with them at some level or they will terminate your program. If you do not explicitly check for the error code, it is lost.&lt;/li&gt;
&lt;li&gt;They &lt;em&gt;can&lt;/em&gt; be ignored: if you explicitly wish to ignore an exception, it
will propagate up to higher levels until some piece of code does handle it.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This same Stack overflow thread has many more viewpoints on why exceptions are useful. No commentor argues that checking for error codes is a better solution.&lt;/p&gt;

&lt;h3 id=&quot;programming-debugging-advice&quot;&gt;Programming / debugging advice&lt;/h3&gt;

&lt;p&gt;Some general programming advice (beyond C++):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;readability&lt;/strong&gt;: One of your primary goals when programming is to carefully 
guide another programmer through your code. Even if you expect to be the
only person to ever see your code, you will be that other programmer in six months.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;minimize cognitive load&lt;/strong&gt;: Toward keeping your code readable, minimize the
cognitive load. Name variables and functions descriptively (&lt;code&gt;num_iterations&lt;/code&gt; instead of &lt;code&gt;n&lt;/code&gt;, &lt;code&gt;calc_inertias(.)&lt;/code&gt; instead of &lt;code&gt;compute(.)&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;use STL containers instead of arrays&lt;/strong&gt;: Arrays do no range checking and
the correct size must be allocated at runtime; accidentally overwriting memory
outside of the array is a common bug &lt;a href=&quot;https://en.wikipedia.org/wiki/Buffer_overflow&quot;&gt;and is a common vector for security attacks&lt;/a&gt;. I prefer the &lt;a href=&quot;http://www.cplusplus.com/reference/vector/vector/&quot;&gt;STL vector&lt;/a&gt;, which can be accessed like an array (e.g., &lt;code&gt;x[5] = 3&lt;/code&gt;), can be queried for its size, automatically deallocates memory when the variable goes out of scope, performs range checking, and can increase its capacity automatically. &lt;a href=&quot;http://cs.brown.edu/%7Ejak/proglang/cpp/stltut/tut.html&quot;&gt;Here&lt;/a&gt; is a nice tutorial on the STL (Standard Template Library). &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;put reusable code in functions and keep functions small&lt;/strong&gt;: longer
functions are more likely to have defects (see a dissenting viewpoint plus several that backup my point of view &lt;a href=&quot;http://c2.com/cgi/wiki?LongFunctionHeresy&quot;&gt;here&lt;/a&gt;). The longer your function is, than say 50 lines of code, the more you should consider breaking it into multiple functions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;http://stackoverflow.com/questions/14041453/why-are-preprocessor-macros-evil-and-what-are-the-alternatives&quot;&gt;beware of macros&lt;/a&gt;&lt;/strong&gt; &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;write the comments first&lt;/strong&gt;: This is a strategy I use when programming. Writing the comments first helps you focus on organizing the logic. Filling in the code from the comments is pretty easy when you know the language syntax.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;address the first compiler errors first&lt;/strong&gt;: Many errors found by the C++ compiler will disappear after you correct the first in a list of errors.&lt;br&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;fix all compiler warnings&lt;/strong&gt;: C++ compilers tend to generate warnings in places where compilers for other languages would generate errors. Take compiler warnings seriously- treat them as errors. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;write &lt;a href=&quot;https://en.wikipedia.org/wiki/Unit_testing&quot;&gt;unit tests&lt;/a&gt;&lt;/strong&gt;: Unit tests allow you to catch problems in a function while you remember the ins and outs of that function as opposed to six months down the road when you locate a bug in the function.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;use a debugger&lt;/strong&gt;: see below&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;c-tools&quot;&gt;C++ tools&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;git / version control&lt;/strong&gt;: While not a C++ tool &lt;em&gt;per se&lt;/em&gt;, use version control to track your changes. Advanced features of version control even allow you to, as examples: run unit tests, run regression tests, and build binary releases
upon committing code.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;gdb / lldb&lt;/strong&gt;: Debugging using &lt;code&gt;printf&lt;/code&gt; (or its variants among programming languages) is usually an order of magnitude faster than using a debugger. Learn at least the main features of a debugger. A good tutorial on gdb is found &lt;a href=&quot;http://www.unknownroad.com/rtfm/gdbtut/&quot;&gt;here&lt;/a&gt;. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;valgrind&lt;/strong&gt;: If you have a bug that you are having difficulty locating using gdb, &lt;a href=&quot;http://valgrind.org&quot;&gt;valgrind&lt;/a&gt; should be your next stop. Valgrind can locate problems like illegal memory reads and writes that gdb will not catch. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;performance tools&lt;/strong&gt;: Do not &lt;a href=&quot;https://shreevatsa.wordpress.com/2008/05/16/premature-optimization-is-the-root-of-all-evil/&quot;&gt;prematurely optimize&lt;/a&gt;: you will find that your intuition about the time sinks in your software are often wrong anyway. Use a &lt;em&gt;profiler&lt;/em&gt;, my favorite on Linux is currently &lt;a href=&quot;https://github.com/gperftools/gperftools&quot;&gt;google-perftools&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;additional-reference-materials&quot;&gt;Additional reference materials&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.parashift.com/c++-faq/&quot;&gt;C++ FAQ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Google&lt;/li&gt;
&lt;/ul&gt;

&lt;hr&gt;

&lt;h2 id=&quot;overview-of-openscenegraph&quot;&gt;Overview of OpenSceneGraph&lt;/h2&gt;

&lt;p&gt;You have two clear options to program in 3D: &lt;a href=&quot;https://en.wikipedia.org/wiki/OpenGL&quot;&gt;OpenGL&lt;/a&gt;, which is a &lt;em&gt;state system&lt;/em&gt; (the rendering is completed determined
by state variables), and &lt;a href=&quot;https://en.wikipedia.org/wiki/Scene_graph&quot;&gt;scene graph-based systems&lt;/a&gt;, like &lt;a href=&quot;https://en.wikipedia.org/wiki/OpenSceneGraph&quot;&gt;OpenSceneGraph&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Open_Inventor&quot;&gt;Open Inventor&lt;/a&gt;, and &lt;a href=&quot;http://www.oracle.com/technetwork/articles/javase/index-jsp-138252.html&quot;&gt;Java 3D&lt;/a&gt;. The
earliest technology for viewing 3D content on the web, &lt;a href=&quot;https://en.wikipedia.org/wiki/VRML97&quot;&gt;VRML&lt;/a&gt;, is based on a scene graph representation (and this is a pretty good file format too).&lt;/p&gt;

&lt;p&gt;I will discuss the scene graph representation because it is intuitive to
understand- it fits well into the object-oriented paradigm, in particular-
and 3D rendering can be achieved with very little code. For example, this tiny bit of code renders many 3D models that you can view using mouse controls:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;// simple.cpp (Evan Drumwright)
#include &amp;lt;osgDB/ReadFile&amp;gt;
#include &amp;lt;osgViewer/Viewer&amp;gt;

int main(int argc, char** argv)
{
  if (argc &amp;lt; 2)
  {
    std::cerr &amp;lt;&amp;lt; &amp;quot;syntax: simple &amp;lt;filename&amp;gt;&amp;quot; &amp;lt;&amp;lt; std::endl;
    return -1;
  }
  osgViewer::Viewer viewer;
  viewer.setSceneData(osgDB::readNodeFile(argv[1]));
  return viewer.run();
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can build this program using &lt;a href=&quot;../../assets/other/simpleosg/CMakeLists.txt&quot;&gt;this&lt;/a&gt; CMake build file. You can then run the program on many 3D files. One example is this &lt;a href=&quot;http://scv.bu.edu/documentation/software-help/graphics-programming/osg_examples/materials/cessna.osg&quot;&gt;cessna airplane&lt;/a&gt;. Once you build the program, you run it like this: &lt;code&gt;simple cessna.osg&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;the-scene-graph&quot;&gt;The scene graph&lt;/h3&gt;

&lt;p&gt;A scene graph is a collection of nodes in a tree (or, more generally, a graph) structure. A node in the tree may have many children but only a single parent, with the effect of a parent applied to all its child nodes. An operation performed on a group automatically propagates its effect to all of its members. &lt;/p&gt;

&lt;p&gt;Associating a geometrical transformation matrix (which I will describe in a future learning module) at a node will apply the transformation (rotation, translation, scaling) to all nodes below it. Materials are applied The scene graph paradigm is 
particularly good for rendering and animating animals, humans, and robots.&lt;/p&gt;

&lt;p&gt;(Adapted from &lt;a href=&quot;https://en.wikipedia.org/wiki/Scene_graph&quot;&gt;this page&lt;/a&gt;). &lt;/p&gt;

&lt;p&gt;An example scene graph for a virtual human is depicted below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/img/scene_graph.png&quot; alt=&quot;example virtual human scene graph&quot;&gt;&lt;/p&gt;

&lt;p&gt;The types of nodes in the graph are described below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://trac.openscenegraph.org/documentation/OpenSceneGraphReferenceDocs/a00910.html&quot;&gt;Transform&lt;/a&gt;: A group node for which all children are transformed by a 4x4 (homogeneous) &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformation_matrix&quot;&gt;transformation matrix&lt;/a&gt;- again, I will discuss this in a future learning module.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://trac.openscenegraph.org/documentation/OpenSceneGraphReferenceDocs/a00357.html&quot;&gt;Group&lt;/a&gt;: A generic node for grouping children together&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://trac.openscenegraph.org/documentation/OpenSceneGraphReferenceDocs/a00775.html&quot;&gt;Sphere&lt;/a&gt;: A geometric primitive node for rendering a sphere&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://trac.openscenegraph.org/documentation/OpenSceneGraphReferenceDocs/a00479.html&quot;&gt;Material&lt;/a&gt;: An object for setting the color properties (color, shininess, transparency) of an object&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;simple-animation&quot;&gt;Simple animation&lt;/h3&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;#include &amp;lt;osgDB/ReadFile&amp;gt;
#include &amp;lt;osgViewer/Viewer&amp;gt;
#include &amp;lt;osg/MatrixTransform&amp;gt;
#include &amp;lt;osgGA/TrackballManipulator&amp;gt;
#include &amp;lt;osgGA/StateSetManipulator&amp;gt;
#include &amp;lt;unistd.h&amp;gt;

int main(int argc, char** argv)
{
  if (argc &amp;lt; 2)
  {
    std::cerr &amp;lt;&amp;lt; &amp;quot;syntax: anim &amp;lt;filename&amp;gt;&amp;quot; &amp;lt;&amp;lt; std::endl;
    return -1;
  }

  // create the viewer, as before, but now we need to add 
  // a trackball manipulator
  osgViewer::Viewer viewer;

  // create a transform
  osg::MatrixTransform* group = new osg::MatrixTransform;
  viewer.setCameraManipulator(new osgGA::TrackballManipulator());

  // read the file and add it to the transform group
  group-&amp;gt;addChild(osgDB::readNodeFile(argv[1]));

  // point the viewer to the scene graph
  viewer.setSceneData(group);
  viewer.realize();

  // set the angle (in radians)
  const double ANGLE = M_PI/180.0;
  unsigned i = 0;

  // loop until done
  while (true)
  {
    if (viewer.done()) break;

    // render a frame
    viewer.frame();

    // update the transform to do a rotation around axis .577 .577 .577
    osg::Matrixd T;
    T.makeRotate(ANGLE*i, 0.57735, 0.57735, 0.57735);
    group-&amp;gt;setMatrix(T);
    i++;

    // sleep a little (10000 microseconds = 10ms = 100 frames per second)
    usleep(10000);
  }

  return 0;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This code fragment covers 90% of animation cases: simply update a matrix transform
and then render a frame (using &lt;code&gt;frame()&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;You can build this program using &lt;a href=&quot;../../assets/other/animosg/CMakeLists.txt&quot;&gt;this&lt;/a&gt; CMake build file. Again, you can then run the program on many 3D files. Once you build the program, you run it like this: &lt;code&gt;anim cessna.osg&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;One important note about animation&lt;/strong&gt;: if your code between calls to &lt;code&gt;frame()&lt;/code&gt; takes too long, then the frame rate will naturally suffer.&lt;/p&gt;

&lt;h3 id=&quot;3d-file-formats-and-tools&quot;&gt;3D file formats and tools&lt;/h3&gt;

&lt;p&gt;To do anything cool with 3D, you need models, and models require considerable
time and expertise to create. You can search for models using Google (try
&amp;quot;3D model spaceship&amp;quot;, for example), convert between models using tools, or
even try building your own or modifying someone else&amp;#39;s. Some useful tools are linked to below: &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Wavefront_.obj_file&quot;&gt;Wavefont OBJ&lt;/a&gt; is a file format that is extremely simple both to parse and to write. I prefer it less
than other file formats when colors should be applied, because these &amp;quot;materials&amp;quot; are stored outside of the file (all data cannot be stored in a single file). The file extension is &amp;quot;.obj&amp;quot;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/VRML&quot;&gt;VRML&lt;/a&gt; comes in two formats, both still
popular, &lt;a href=&quot;http://www.martinreddy.net/gfx/3d/VRML.spec&quot;&gt;VRML 1.0&lt;/a&gt; and &lt;a href=&quot;http://gun.teipir.gr/VRML-amgem/spec/index.html&quot;&gt;VRML 97 (also known as VRML 2.0)&lt;/a&gt;. The VRML 1.0 file extension is &amp;quot;.iv&amp;quot;; the VRML 2.0 file extensions are &amp;quot;.wrl&amp;quot; and
&amp;quot;.vrml&amp;quot; (less common). VRML 1.0 is easy to parse and write to; VRML 2.0 is
easier to write to. There exist tools for converting between VRML 1.0 and 2.0, but your mileage will vary.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.blender.org&quot;&gt;Blender&lt;/a&gt; is free, professional (or near professional grade) 3D modeling and rendering software. It can help you edit 3D models
and convert between various representations. The only problems: its interface is not very intuitive, the interface has changed multiple times in the 10+ years that I&amp;#39;ve used it, and the documentation has historically been poor.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;learning-more&quot;&gt;Learning more&lt;/h3&gt;

&lt;p&gt;There are a number of tutorials available for OpenSceneGraph &lt;a href=&quot;http://trac.openscenegraph.org/projects/osg//wiki/Support/Tutorials&quot;&gt;here&lt;/a&gt;. API documentation for OpenSceneGraph is located &lt;a href=&quot;http://trac.openscenegraph.org/documentation/OpenSceneGraphReferenceDocs/&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;
</description>
        
          <description>&lt;p&gt;&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;&lt;/span&gt;
This learning module will provide an overview of C++, targeted toward
those with some background in Java or C.  &lt;/p&gt;
</description>
        
        <pubDate>Fri, 18 Dec 2015 00:00:00 -0500</pubDate>
        <link>/robotics-course-materials/robotics-course-materials/blog/C++/</link>
        <guid isPermaLink="true">/robotics-course-materials/robotics-course-materials/blog/C++/</guid>
        
        
      </item>
      
    
  </channel>
</rss>
