<!DOCTYPE html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7"><![endif]-->
<!--[if IE 7]><html class="no-js lt-ie9 lt-ie8" <![endif]-->
<!--[if IE 8]><html class="no-js lt-ie9" <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <title>An engineer's guide to matrices, vectors, and numerical linear algebra</title>

    <!-- Favicon -->
    <link rel="icon" type="image/x-icon" href="/robotics-course-materials/assets/img/favicon.ico" />

    <!-- Come and get me RSS readers -->
    <link rel="alternate" type="application/rss+xml" title="Robotics" href="/robotics-course-materials/robotics-course-materials/feed.xml" />
    
    <!-- Stylesheet -->
    <link rel="stylesheet" href="/robotics-course-materials/assets/css/style.css">
    <!--[if IE 8]><link rel="stylesheet" href="/robotics-course-materials/assets/css/ie.css"><![endif]-->
    <link rel="canonical" href="/robotics-course-materials/robotics-course-materials/blog/linear-algebra/">

    <!-- Modernizr -->
    <script src="/robotics-course-materials/assets/js/modernizr.custom.15390.js" type="text/javascript"></script>

    
</head>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" >
</script>

<body>

    <div class="header">
     <div class="container">
         <h1 class="logo"><a href="/robotics-course-materials/">Robotics</a></h1>
         <nav class="nav-collapse">
             <ul class="noList">
                 
             </ul>
         </nav>
     </div>
 </div><!-- end .header -->


   <div class="content">
      <div class="container">
         <div class="post">
  
  <h1 class="postTitle">An engineer's guide to matrices, vectors, and numerical linear algebra</h1>
  <p class="meta">December 18, 2015 | <span class="time">19</span> Minute Read</p>
  
  <p>This material is not meant to replace a proper course in linear algebra, where
important concepts like vector spaces, eigenvalues/eigenvectors, and
spans are defined. My intent is to give you a practical guide to concepts
in matrix arithmetic and numerical linear algebra that I have found useful. </p>

<h3 id="matrix-arithmetic">Matrix arithmetic</h3>

<h4 id="matrix-and-vector-addition-and-subtraction">Matrix and vector addition and subtraction</h4>

<p>Matrices and vectors can only be added or subtracted if they are of the
same dimensionality. Then, addition and subtraction are performed
elementwise:</p>

<p>\begin{equation*}
\begin{bmatrix}
u_1 \\
u_2 \\
u_3 
\end{bmatrix} + 
\begin{bmatrix}
v_1 \\
v_2 \\
v_3
\end{bmatrix} =
\begin{bmatrix}
u_1 + v_1 \\
u_2 + v_2 \\
u_3 + v_3 
\end{bmatrix}
\end{equation*}</p>

<p>\begin{equation*}
\begin{bmatrix}
m_{11} &amp; m_{12} &amp; m_{13} \\
m_{21} &amp; m_{22} &amp; m_{23} \\
m_{31} &amp; m_{32} &amp; m_{33}
\end{bmatrix} + 
\begin{bmatrix}
n_{11} &amp; n_{12} &amp; n_{13} \\
n_{21} &amp; n_{22} &amp; n_{23} \\
n_{31} &amp; n_{32} &amp; n_{33}
\end{bmatrix} = 
\begin{bmatrix}
m_{11} + n_{11} &amp; m_{12} + n_{12} &amp; m_{13} + n_{13} \\
m_{21} + n_{21} &amp; m_{22} + n_{22} &amp; m_{23} + n_{23} \\
m_{31} + n_{31} &amp; m_{32} + n_{32} &amp; m_{33} + n_{33}
\end{bmatrix}
\end{equation*}</p>

<h4 id="matrix-and-vector-scaling">Matrix and vector scaling</h4>

<p>Matrix and vectors can be scaled by multiplying every element in the matrix or vector by the scalar, as seen below:</p>

<p>\begin{align*}
\mathbf{M} &amp; \equiv \begin{bmatrix} 
m_{11} &amp; m_{12} &amp; m_{13} \\
m_{21} &amp; m_{22} &amp; m_{23} \\
m_{31} &amp; m_{32} &amp; m_{33} \end{bmatrix} \\ 
s \mathbf{M} &amp; = \begin{bmatrix}
s m_{11} &amp; sm_{12} &amp; sm_{13} \\
s m_{21} &amp; sm_{22} &amp; sm_{23} \\
s m_{31} &amp; sm_{32} &amp; sm_{33} \end{bmatrix} 
\end{align*}
for \(s \in \mathbb{R}\)</p>

<h4 id="special-matrices">Special matrices</h4>

<p>I will point out three special types of matrices:</p>

<ul>
<li>\(\mathbf{0} \in \mathbb{R}^{m \times n}\) (the &quot;zero matrix&quot;): a matrix with every entry set to zero</li>
<li>Diagonal matrices: a square (\(n \times n\)) matrix with nonzero entries placed only on the diagonal (from upper left to lower right)</li>
<li>\(\mathbf{I} \in \mathbb{R}^{n \times n}\) (the &quot;identity matrix&quot;): a diagonal matrix with every entry on the diagonal set to 1 </li>
</ul>

<h4 id="the-cross-product">The cross product</h4>

<p>The cross product operator (\(\times\)) does not fit neatly into matrix/vector arithmetic
and linear algebra, because it applies only to vectors in \(\mathbb{R}^3\).
For two vectors \(\mathbf{a}, \mathbf{b} \in \mathbb{R}^3\), \(\mathbf{a} \times \mathbf{b}\) yields:</p>

<ol>
<li>A vector orthogonal to \(\mathbf{a}\) and to \(\mathbf{b}\), assuming that \(\mathbf{a}\) and \(\mathbf{b}\) are linearly independent (otherwise, \(\mathbf{a} \times \mathbf{b} = \mathbf{0}\)) </li>
<li>The negation of \(\mathbf{b} \times \mathbf{a}\); stated again, \(\mathbf{a} \times \mathbf{b} = -\mathbf{b} \times \mathbf{a}\).</li>
<li>A different vector depending on whether the cross product is a <em>right handed cross product</em> or a <em>left handed cross product</em></li>
</ol>

<p>The cross product is distributive over addition:</p>

<p>\begin{equation*}
\mathbf{a} \times (\mathbf{b} + \mathbf{c}) = \mathbf{a} \times \mathbf{b} + \mathbf{a} \times \mathbf{c}
\end{equation*}</p>

<h5 id="computing-the-cross-product">Computing the cross product</h5>

<p>For the right handed cross product, 
$$
\mathbf{a} \times \mathbf{b} = \begin{bmatrix}
a_2b_3 - a_3b_2 \\
a_3b_1 - a_1b_3 \\
a_1b_2 - a_2b_1
\end{bmatrix}
$$</p>

<p>The left handed cross product is just the negation of this vector.</p>

<h4 id="inner-products">Inner products</h4>

<p>The inner product operation (also called the <em>dot product</em>) between two vectors \(\mathbf{a}\) and \(\mathbf{b}\) is written \(\lt \mathbf{a}, \mathbf{b}\gt \), \(\mathbf{a}^{\textsf{T}}\mathbf{b}\), or \(\mathbf{a} \cdot \mathbf{b}\). The inner product is only defined for vectors of equal dimension and consists of the sum of the products of the corresponding elements from the two vectors. An example is given below:</p>

<p>\begin{align*}
\mathbf{a} &amp; \equiv \begin{bmatrix} a_1 \\ a_2 \\ a_3 \\ a_4 \end{bmatrix} \\
\mathbf{b} &amp; \equiv \begin{bmatrix} b_1 \\ b_2 \\ b_3 \\ b_4 \end{bmatrix} \\
\mathbf{a}^\textsf{T}\mathbf{b} &amp; = a_1 b_1 + a_2 b_2 + a_3 b_3 + a_4 b_4
\end{align*}</p>

<p>Some properties of the inner product follow, for real vectors \(\mathbf{a}, \mathbf{b}, \mathbf{c}\):</p>

<ul>
<li>The dot product is commutative: \(\mathbf{a} \cdot \mathbf{b} = \mathbf{b} \cdot \mathbf{a}\)</li>
<li>The dot product is distributive over vector addition: \(\mathbf{a} \cdot (\mathbf{b} + \mathbf{c}) = \mathbf{a} \cdot \mathbf{b} + \mathbf{a} \cdot \mathbf{c}\)</li>
<li>If \(\mathbf{a}, \mathbf{b} \ne \mathbf{0}\), \(\mathbf{a} \cdot \mathbf{b} = 0 \) if and only if \(\mathbf{a}\) and \(\mathbf{b}\) are orthogonal</li>
<li>\(\mathbf{a} \times (\mathbf{b} \times \mathbf{c}) = \mathbf{b}(\mathbf{a} \cdot \mathbf{c}) - \mathbf{c}(\mathbf{a} \cdot \mathbf{b})\)</li>
<li>\(\mathbf{a} \cdot (\mathbf{b} \times \mathbf{c}) = \mathbf{b} \cdot (\mathbf{c} \times \mathbf{a}) = \mathbf{c} \cdot (\mathbf{a} \times \mathbf{b})\) </li>
</ul>

<h4 id="matrix-vector-multiplication">Matrix/vector multiplication</h4>

<p>Matrix/vector multiplication is identical to multiple inner (dot) product
operations over the rows of \(\mathbf{A}\). Assume we define matrix \(\mathbf{A}\) as follows:</p>

<p>\begin{equation*}
\mathbf{A} \equiv \begin{bmatrix} \mathbf{a}_1 \\ \vdots \\ \mathbf{a}_m \end{bmatrix}
\end{equation*}</p>

<p>We can then compute the matix vector product \(\mathbf{Av}\) using \(m\) inner products:</p>

<p>\begin{equation*}
\mathbf{Av} = \begin{bmatrix} \mathbf{a}_1\mathbf{v} \\ \vdots \\ \mathbf{a}_m\mathbf{v} \end{bmatrix}
\end{equation*} </p>

<p>As a concrete example, we define:</p>

<p>\begin{align*}
\mathbf{A} &amp; \equiv \begin{bmatrix} a &amp; b &amp; c \\ d &amp; e &amp; f \end{bmatrix} \\
\mathbf{v} &amp; \equiv \begin{bmatrix} g \\ h \\ i \end{bmatrix} \\
\mathbf{Av} &amp; = \begin{bmatrix} ag + bh + ci \\ dg + eh + fi \end{bmatrix}
\end{align*}</p>

<h4 id="matrix-matrix-multiplication">Matrix/matrix multiplication</h4>

<p>Matrix/matrix multiplication proceeds the same as matrix-vector multiplication
over multiple columns:</p>

<p>\begin{equation}
\mathbf{MN} \equiv \mathbf{M}\begin{bmatrix} \mathbf{n}_1 &amp; \ldots &amp; \mathbf{n}_n \end{bmatrix} \equiv \begin{bmatrix} \mathbf{Mn}_1 &amp; \ldots &amp; \mathbf{Mn}_n \end{bmatrix} 
\end{equation}</p>

<p>As a concrete example:</p>

<p>\begin{align*}
\mathbf{M} &amp; \equiv 
\begin{bmatrix} a &amp; b \\ c &amp; d \\ e &amp; f \end{bmatrix} \\
\mathbf{n}_1 &amp; \equiv \begin{bmatrix} g \\ j \end{bmatrix}  \\
\mathbf{n}_2 &amp; \equiv \begin{bmatrix} h \\ k \end{bmatrix}  \\
\mathbf{n}_3 &amp; \equiv \begin{bmatrix} i \\ l \end{bmatrix}  \\
\mathbf{N} &amp; \equiv \begin{bmatrix} \mathbf{n}_1 &amp; \mathbf{n}_2 &amp; \mathbf{n}_3 \end{bmatrix} \\
\mathbf{Mn}_1 &amp; \equiv \begin{bmatrix} ag + bj \\ cg + dj \\ eg + fj \end{bmatrix} \\
\mathbf{Mn}_2 &amp; \equiv \begin{bmatrix} ah + bk \\ ch + dk \\ eh + fk \end{bmatrix} \\
\mathbf{Mn}_3 &amp; \equiv \begin{bmatrix} ai + bl \\ ci + dl \\ ei + fl \end{bmatrix} \\
\mathbf{MN} &amp; \equiv \begin{bmatrix} ag+bj &amp; ah+bk &amp; ai+bl \\ cg+dj &amp; ch+dk &amp; ci + dl\\ eg+fj &amp; eh+fk &amp; ei+fl \end{bmatrix}
\end{align*}</p>

<p>Note that <strong>matrix multiplication is not commutative</strong>: \(\mathbf{AB} \ne \mathbf{BA}\) (generally), even when the dimensions are compatible.</p>

<h4 id="outer-products">Outer products</h4>

<p>The <em>outer product</em> \(\mathbf{ab}^\mathsf{T}\) of two vectors \(\mathbf{a} \in \mathbb{R}^m\) and \(\mathbf{b} \in \mathbb{R}^n\) is always defined and represents a matrix/matrix multiplication operation between a \(m \times 1\) and a \(1 \times n\) matrix; in other words, normal matrix/matrix multiplication rules apply.</p>

<h4 id="matrix-transposition">Matrix transposition</h4>

<p>The transpose of a matrix is defined as follows:</p>

<p>$$
A_{ij}^\mathsf{T} = A_{ji}
$$</p>

<P>
<table class="image">
<caption align="bottom">A depiction of the matrix transpose operation.</caption>
<img src="https://upload.wikimedia.org/wikipedia/commons/e/e4/Matrix_transpose.gif" alt="A depiction of the matrix transpose operation." width=""/>
</table>
</P>

<p>where the operator \(^\mathsf{T}\) indicates transposition. This definition implies that an \(m \times n\) matrix becomes an \(n \times m\) matrix. If \(\mathbf{A} = \mathbf{A}^\textsf{T}\) we say that \(\mathbf{A}\) is a <em>symmetric matrix</em>. </p>

<p>The following properties apply to matrix transposition for matrices \(\mathbf{A}\) and \(\mathbf{B}\):</p>

<ul>
<li>\((\mathbf{A}^\textsf{T})^\mathsf{T} = \mathbf{A}\)</li>
<li>\((\mathbf{A}+\mathbf{B})^\mathsf{T} = \mathbf{A}^\mathsf{T} + \mathbf{B}^\mathsf{T}\)</li>
<li>\((\mathbf{AB})^\mathsf{T} = \mathbf{B}^\mathsf{T}\mathbf{A}^\mathsf{T}\)</li>
<li>If \(\mathbf{A}\) has only real entries, then \(\mathbf{A}^\textsf{T}\mathbf{A}\) is a positive semi-definite matrix (see below).</li>
<li>\(\mathbf{A}\mathbf{A}^\textsf{T}\) is a symmetric matrix.</li>
</ul>

<h4 id="matrix-inversion">Matrix inversion</h4>

<p>The inverse of a matrix \(\mathbf{A}\), written \(\mathbf{A}^{-1}\), is characterized by the following properties:</p>

<p>\begin{align}
\mathbf{AA}^{-1} &amp; = \mathbf{I} \\
\mathbf{A}^{-1}\mathbf{A} &amp; = \mathbf{I}
\end{align}</p>

<p>The inverse exists only if \(\mathbf{A}\) is square and is <em>non-singular</em>. Singularity can be determined multiple ways (only two are listed below):</p>

<ul>
<li>If the determinant of the matrix is zero, the matrix is singular. The determinant can be computed using LU factorization (see below).</li>
<li>If one or more of the singular values of the matrix is zero, the matrix is singular. The singular values can be computed using the singular value decomposition (see below).</li>
</ul>

<p>The following properties apply to matrix inversion for matrices \(\mathbf{A}\) and \(\mathbf{B}\):</p>

<ul>
<li>\((\mathbf{A}^\textsf{-1})^\mathsf{-1} = \mathbf{A}\)</li>
<li>\((\mathbf{A}^\textsf{T})^{-1} = (\mathbf{A}^{-1})^\textsf{T}\)</li>
<li>\((\mathbf{AB})^{-1} = \mathbf{B}^{-1}\mathbf{A}^{-1}\)</li>
</ul>

<p>In numerical linear algebra, you almost never need to explicitly form the
inverse of a matrix and <strong>you should avoid explicitly forming the inverse
whenever possible</strong>: <em>the solution obtained by computing \(\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}\) is considerably slower than back/forward substitution-based methods</em> (using, e.g., Cholesky factorization, LU factorization, etc.) for solving \(\mathbf{Ax} = \mathbf{b}\).</p>

<h3 id="empty-matrices-and-vectors">Empty matrices and vectors</h3>

<p>Empty matrices (matrices with one or more dimensions equal to zero) are often useful. They allow formulas,
optimization, etc. to be used without breaking even when the inputs to the
problem are empty: it does not become necessary to use special logic to
handle such corner cases. </p>

<p>Given scalar \(s \in \mathbb{R}\), empty matrix \(\mathbf{M} \in \mathbb{R}^{m \times n}\) (with one of \(m,n\) equal to zero), empty matrix \(\mathbf{E} \in \mathbb{R}^{0 \times m}\), and empty matrix \(\mathbf{F} \in \mathbb{R}^{n \times 0}\), we have the following rules:</p>

<ol>
<li>\(s \mathbf{M} = \mathbf{M}\)</li>
<li>\(\mathbf{M} + \mathbf{M} = \mathbf{M} \)</li>
<li>\(\mathbf{EM} = \mathbf{F}^\mathsf{T}\)</li>
<li>\(\mathbf{MF} = \mathbf{E}^\mathsf{T}\)</li>
<li>\(\mathbf{E}\mathbf{F}^\mathsf{T} = \mathbf{0}\)</li>
</ol>

<h3 id="computational-considerations-for-matrix-multiplication-associativity">Computational considerations for matrix-multiplication associativity</h3>

<p>Matrix multiplication <em>is</em> associative: \((\mathbf{AB})\mathbf{C} = \mathbf{A}(\mathbf{BC})\). The amount of computation required can be very different in the two cases.
Assume that \(\mathbf{A} \in \mathbb{R}^{i \times j}, \mathbf{B} \in \mathbb{R}^{j \times k}, \mathbf{C} \in \mathbb{R}^{k \times m}\). Depending on the order of operation, two very different flop counts are possible:</p>

<ol>
<li>\((\mathbf{A}\mathbf{B})\mathbf{C} = O(ijk) + O(ikm)\)</li>
<li>\(\mathbf{A}(\mathbf{B}\mathbf{C}) = O(jkm) + O(ijm)\)</li>
</ol>

<p>Now consider the following variable instances: \(i = 1, j = 2, k = 3, m = 4 \). The asymptotic number of operations in Case (1) will be on the order of 15 flops and in Case (2) will be 32 flops. Takeaway: consider your multiplication ordering. </p>

<p>Note that in the case of multiplying a chain of matrices and then a vector:</p>

<p>\begin{equation}
\mathbf{ABv}
\end{equation}</p>

<p>One always wants to do the vector multiplication first:</p>

<p>\begin{equation}
\mathbf{ABv} = \mathbf{A}(\mathbf{Bv})
\end{equation}</p>

<p>In many applications, only a few matrices may be multiplied at
one time, meaning that a little logic can be used to determine the order of operations. For longer chains of matrices, one likely wants to use <a href="https://en.wikipedia.org/wiki/Matrix_chain_multiplication">dynamic programming to determine the optimal multiplication order</a>.</p>

<h3 id="linear-algebra">Linear algebra</h3>

<h4 id="orthogonality">Orthogonality</h4>

<p>Vectors \(\mathbf{u}\) and \(\mathbf{v}\) are orthogonal if their dot (inner) product is zero.</p>

<p><em>Orthogonal</em> matrices are very convenient because they possess the property that their inverse is equal to their transpose:</p>

<p>\begin{equation}
\mathbf{A}^\mathsf{T} = \mathbf{A}^{-1} \textrm{ if } \mathbf{A} \textrm{ orthogonal}
\end{equation}</p>

<p>Computationally, this means that the inverse can be computed quickly and robustly. An orthogonal matrix has the following properties:</p>

<ol>
<li>The determinant of the matrix is \(\pm 1\)</li>
<li>The dot product of any two rows \(i \ne j\) of the matrix is zero</li>
<li>The dot product of any two columns \(i \ne j\) of the matrix is zero</li>
</ol>

<h4 id="positive-and-negative-definiteness">Positive and negative definiteness</h4>

<p>A symmetric matrix \(\mathbf{A}\) is <em>positive definite</em> if:</p>

<p>\begin{equation}
\mathbf{x}^\mathsf{T}\mathbf{A}\mathbf{x} \gt 0
\end{equation}</p>

<p>for any \(\mathbf{x} \in \mathbb{R}^n\) such that \(\mathbf{x} \ne \mathbf{0}\). This condition is equivalent to saying that a matrix is positive definite if all of its <a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors">eigenvalues</a> are positive; eigenvalues are readily computable with GNU Octave/Matlab (using <code>eig</code>) and with most libraries for numerical linear algebra. If \(\mathbf{A}\) is not positive definite and instead,</p>

<p>\begin{equation}
\mathbf{x}^\mathsf{T}\mathbf{A}\mathbf{x} \ge 0
\end{equation}</p>

<p>for any \(\mathbf{x} \in \mathbb{R}^n\) such that \(\mathbf{x} \ne \mathbf{0}\), we say that the matrix is <em>positive semi-definite</em>. This condition is equivalent to saying that a matrix is positive semi-definite if all of its eigenvalues are
non-negative. </p>

<p>Similarly, a matrix is <em>negative definite</em> if all of its eigenvalues are
strictly negative and <em>negative semi-definite</em> if all of its eigenvalues
are non-positive. If none of these conditions hold- \(\mathbf{A}\) has both positive and negative eigenvalues- we say that the matrix is <em>indefinite</em>.</p>

<h5 id="checking-positive-definiteness">Checking positive-definiteness</h5>

<p>The fastest general way to check for positive-definiteness is using the
Cholesky factorization. If the Cholesky factorization succeeds, the matrix
is positive definite. This approach for checking positive definiteness
is a (significant) constant factor faster than approaches that compute eigenvalues.</p>

<h5 id="applications-of-definiteness">Applications of definiteness</h5>

<p>Definite matrices have many applications in engineering applications.
As one example, if the Hessian matrix of an objective function is 
positive semi-definite, the function is convex and admits solution
via robust convex optimization codes. As another example, Lyapunov
stability analysis requires negative definiteness of the time derivative
of a Lyapunov function candidate. </p>

<p>We often wish to avoid indefinite matrices. For example, quadratic programming
with indefinite matrices is NP-hard, while it is polynomial time solvable
with definite matrices. </p>

<h4 id="factorizations">Factorizations</h4>

<p>I like to consider matrix factorizations in ascending order of computational expense. Correlated with computational expense is numerical robustness. A list of the factorizations follows:</p>

<table><thead>
<tr>
<th>Factorization</th>
<th>Flops</th>
<th>Applicability</th>
</tr>
</thead><tbody>
<tr>
<td>Cholesky factorization</td>
<td>\(n^3/3\)</td>
<td>Positive-definite matrices only</td>
</tr>
<tr>
<td>LDL\(^\mathsf{T}\) factorization</td>
<td>\(n^3/2 + O(n^2)\)</td>
<td>Symmetric matrices only</td>
</tr>
<tr>
<td>LU factorization</td>
<td>\(2n^3/3\)</td>
<td>Non-singular matrices (no least squares)</td>
</tr>
<tr>
<td>QR factorization</td>
<td>\(4n^3/3\)</td>
<td>Singular and non-singular matrices (least squares ok)</td>
</tr>
<tr>
<td>Singular value decomposition</td>
<td>\(8n^3/3\)</td>
<td>Singular and non-singular matrices (least squares ok)</td>
</tr>
</tbody></table>

<h4 id="nullspace">Nullspace</h4>

<p>The nullspace \(\mathbf{R}\) of matrix \(\mathbf{A}\) is a nonzero matrix
such that: </p>

<p>\begin{equation}
\mathbf{AR} = \mathbf{0}
\end{equation}</p>

<p>The nullspace of a matrix \(\mathbf{A}\) can be determined using the 
rightmost columns of \(\mathbf{V}\) (from the singular value decomposition 
of \(\mathbf{A}\)) that correspond to the zero singular values from 
\(\mathbf{\Sigma}\) (also from the SVD of \(\mathbf{A}\)).</p>

<p>Nullspaces are particularly good for optimization and least squares problems.
For example, the nullspace allows optimizing along multiple criteria in
hierarchical fashion.</p>

<h3 id="matrix-calculus">Matrix calculus</h3>

<p>If \(\mathbf{a}, \mathbf{b}\) are functions, then the derivative of (denoted by prime &#39;) the matrix multiplication operation is: \(\mathbf{a}^\mathsf{T}\mathbf{b} = {\mathbf{a}&#39;}^{\mathsf{T}}\mathbf{b} + \mathbf{a}^\mathsf{T} \cdot \mathbf{b}&#39;\)</p>

<h4 id="gradient">Gradient</h4>

<p>The gradient of a function \(f(\mathbf{x})\), where \(\mathbf{x} \in \mathbb{R}^n\) is the \(n\)-dimensional vector:</p>

<p>\begin{equation}
\nabla f_{\mathbf{x}} \equiv
\begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\vdots \\
\frac{\partial f}{\partial x_n} \\
\end{bmatrix}
\end{equation}</p>

<h4 id="hessian">Hessian</h4>

<p>The Hessian matrix of the same function is the \(n \times n\) matrix of second order partial derivatives:</p>

<p>\begin{equation}
\nabla f_{\mathbf{xx}} \equiv 
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} &amp; \ldots &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n}\\
\vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \ldots &amp; \frac{\partial^2 f}{\partial x_n^2}\\
\end{bmatrix}
\end{equation}</p>

<p>Given that \(\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}\), the Hessian matrix is symmetric (this helps with debugging and can reduce computation when constructing the matrix).</p>

<h4 id="jacobian">Jacobian</h4>

<p>The Jacobian matrix of a function with vector outputs is the partial derivative
of each function output dimension taken with respect to the partial derivative of each function input dimension. Let us examine a contrived function, \(f : \mathbb{R}^3 \to \mathbb{R}^2\); \(f(.)\) might represent a vector flow for
points in three dimensional Cartesian space. The Jacobian of \(f(.)\) is then:</p>

<p>\begin{equation*}
\frac{\partial f}{\partial \mathbf{x}} = \begin{bmatrix} 
\frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_1}{\partial x_2} &amp; \frac{\partial f_1}{\partial x_3} \\
\frac{\partial f_2}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_2} &amp; \frac{\partial f_2}{\partial x_3} 
\end{bmatrix} 
\end{equation*}</p>

<p>Just like the standard derivative, the Jacobian matrix gives the instantaneous change in \(f(.)\) at \(\mathbf{x}\). </p>

<h3 id="least-squares-problems">Least squares problems</h3>

<p>Least squares problems are ubiquitous in science and engineering applications.
Solving a least squares problem finds a line (or plane or hyperplane,
in higher dimensions) that minimizes the sum of the squared distance from a
set of points to the line/plane/hyperplane. Another way of saying this is
that least squares seeks to minimize the residual error, i.e., \(||\mathbf{A}\mathbf{x} - \mathbf{b}||\). </p>

<P>
<table class="image">
<caption align="bottom">A depiction of least squares as a mechanical device. Springs are attached from each point to a rod. The amount of force increases quadratically with the distance of each point to the rod.</caption>
<img src="http://www.datavis.ca/papers/koln/figs/grsp2.gif" alt="A depiction of least squares as a mechanical device. Springs are attached from each point to a rod. The amount of force increases quadratically with the distance of each point to the rod." width=""/>
</table>
</P>

<p>Clearly, if \(\mathbf{A}\) is square and non-singular, the solution is \(\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}\). What if \(\mathbf{A} \in \mathbb{R}^{m \times n}\), where \(m \neq n\)? Assume that each row of \(\mathbf{A}\) is linearly independent (i.e., \(\mathbf{A}\) has full row rank) for now. If \(m \gt n\), then there are more
equations than variables and the problem is <em>overdetermined</em>; we expect \(||\mathbf{Ax} - \mathbf{b}||\) to be nonzero. If \(m \lt n\), then there are more
variables than unknowns and the problem is <em>underdetermined</em>; we expect there
to be multiple (infinite) assignments to \(\mathbf{x}\) that make \(||\mathbf{Ax} - \mathbf{b}|| = 0\).</p>

<h5 id="the-moore-penrose-pseudo-inverse">The Moore-Penrose pseudo-inverse</h5>

<p><em>Two</em> least squares problems become evident: (1) select \(\mathbf{x}\) such that  \(||\mathbf{Ax} - \mathbf{b}||\) is minimized. If \(||\mathbf{Ax} - \mathbf{b}|| = 0\), then (2) select the solution that minimizes \(||\mathbf{x}||\). Both solutions can be obtained using the Moore-Penrose pseudo-inverse (which is denoted using the operator \(\ ^+\), which has some of the properties of the inverse (I&#39;ll only list a few below):</p>

<ol>
<li>\(\mathbf{A}\mathbf{A}^+ = \mathbf{I}\)</li>
<li>\(\mathbf{A}^+\mathbf{A} = \mathbf{I}\)</li>
<li>If \(\mathbf{A}\) is invertible, then \(\mathbf{A}^+ = \mathbf{A}^{-1}\).</li>
<li>\(\mathbf{0}^{-1} = \mathbf{0}^\mathsf{T}\).</li>
<li>\((\mathbf{A}^+)^+ = \mathbf{A}\).</li>
</ol>

<p>The <em>left pseudo-inverse</em> follows:</p>

<p>\begin{equation}
\mathbf{A}^+ = {(\mathbf{A}^\mathsf{T}\mathbf{A})}^{-1}\mathbf{A}^\mathsf{T}
\end{equation}</p>

<p>This pseudo-inverse constitutes a <em>left inverse</em> (because \(\mathbf{A}^+\mathbf{A} = \mathbf{I}\)), while the <em>right pseudo-inverse</em> (below)
constitutes a <em>right inverse</em> (because \(\mathbf{A}\mathbf{A}^+ = \mathbf{I}\)). See <a href="https://en.wikipedia.org/wiki/Inverse_function#Left_and_right_inverses">Wikipedia</a> for an accessible description of left and right inverses. </p>

<p>\begin{equation}
\mathbf{A}^+ = \mathbf{A}^\mathsf{T}{(\mathbf{A}\mathbf{A}^\textsf{T})}^{-1}
\end{equation}</p>

<h5 id="solving-least-squares-problems-using-the-singular-value-decomposition">Solving least squares problems using the singular value decomposition</h5>

<p>Still working under the assumption that \(\mathbf{A}\) is of full row rank,
we can also use the singular value decomposition to solve least squares 
problems.</p>

<p>Recall that the singular value decomposition of \(\mathbf{A} = \mathbf{U\Sigma}mathbf{V}^{\mathsf{T}}\). \(\mathbf{U}\) and \(\mathbf{V}\) are both orthogonal, which means that \(\mathbf{U}^{-1} = \mathbf{U}^{\mathsf{T}}\) and \(\mathbf{V}^{-1} = \mathbf{V}^{\mathsf{T}}\). From the identity above, \(\mathbf{A}^{-1} = \mathbf{V}\Sigma^{-1}\mathbf{U}^{\mathsf{T}}\). For non-square \(\mathbf{A}\), \(\mathbf{A}^+ = \mathbf{V}\mathbf{\Sigma}^+{\mathbf{U}}^\mathsf{T}\), where \(\Sigma^+\) will be defined as follows:</p>

<p>\begin{equation}
\Sigma_{ij}^+ \leftarrow \begin{cases}\frac{1}{\Sigma_{ij}} &amp; \textrm{if } i = j, \\
0 &amp; \textrm{if } i \ne j. \end{cases}
\end{equation}</p>

<h4 id="regularization">Regularization</h4>

<p>It&#39;s too much to ask that \(\mathbf{A}\), \(\mathbf{A}^\mathsf{T}\mathbf{A}\), or \(\mathbf{A}\mathbf{A}^\textsf{T}\) be always invertible. Even if you 
<em>know</em> the matrix is invertible (based on some theoretical properties), this does not mean that \(\mathbf{A}\) et al. will be well conditioned. And singular
matrices arise in least squares applications when two or more equations are 
linearly dependent (in other words, regularly).</p>

<p><em>Regularization</em> allows one to compute least squares solutions with numerical
robustness.</p>

<h5 id="using-regularization-with-the-singular-value-decomposition">Using regularization with the singular value decomposition</h5>

<p>The formula for computing \(\mathbf{\Sigma}^+\) can be updated for a numerically robust solution:</p>

<p>\begin{equation}
\Sigma_{ij}^+ \leftarrow \begin{cases}\frac{1}{\Sigma_{ij}} &amp; \textrm{if } i = j \textrm{ and } \Sigma_{ij} &gt; \epsilon, \\
0 &amp; \textrm{if } i \ne j \textrm{ or } \Sigma_{ij} \le \epsilon. \end{cases}
\end{equation}</p>

<p>\(\epsilon\) can be set using <em>a priori</em> knowledge or using the strategy
used in <a href="http://www.netlib.org/lapack/">LAPACK</a>&#39;s library:</p>

<p>\begin{equation}
\epsilon = \epsilon_{\textrm{mach}} \cdot \max{(m,n)} \cdot \max_{i,j} \Sigma_{ij} 
\end{equation} </p>

<h5 id="tikhonov-regularization">Tikhonov regularization</h5>

<p>A very simple form of regularization is known as <em>Tikhonov Regularization</em>
and, in statistics applications, as <em>Ridge Regression</em>. Consider the system of linear equations:</p>

<p>\begin{equation}
\mathbf{A}\mathbf{x} = \mathbf{b}
\end{equation}</p>

<p>for \(\mathbf{A} \in \mathbb{R}^{n \times n}\) and \(\mathbf{x}, \mathbf{b} \in \mathbb{R}^{n \times 1}\). The
matrix \(\mathbf{A} + \epsilon\mathbf{I}\), where \(\mathbf{I}\) is the identity matrix will always be invertible for sufficiently large \(\epsilon \ge 0\). Tikhonov Regularization solves the nearby system \((\mathbf{A} + \epsilon\mathbf{I})\mathbf{x} = \mathbf{b}\) for \(\mathbf{x}\).</p>

<p>The obvious question at this point: what is the optimal value of \(\epsilon\)? If \(\epsilon\) is too small, the relevant factorization algorithm (e.g., Cholesky factorization, LU factorization) will fail with an error (at best). If \(epsilon\) is too large, the residual error \(||\mathbf{Ax} - \mathbf{b}||\) will be greater than is necessary. The optimal value of \(\epsilon\) can be
computed with a singular value decomposition, but- one might as well just use the result from the SVD to compute a regularization solution (as described above)- if one goes down this computationally expensive route.</p>

<p>An effective \(\epsilon\) will keep the condition number (the ratio of
largest to smallest singular values) relatively small: \(10^6\) or so. An 
example of a quick and dirty way to compute \(\epsilon\) is:</p>

<p>\begin{equation}
\epsilon = \epsilon_{\textrm{mach}} \cdot \max{(m,n)} \cdot ||\mathbf{A}||_\infty
\end{equation} </p>

<p>where \(\epsilon_{\textrm{mach}}\) is machine epsilon and</p>

<p>\begin{equation}
||\mathbf{A}||_\infty = \max_{i \in m, j \in n} |A_{ij}|
\end{equation} </p>


  <!-- POST NAVIGATION -->
  <div class="postNav clearfix">
     
      <a class="prev" href="/robotics-course-materials/blog/C++/"><span>&laquo;&nbsp;C++ overview and OpenSceneGraph introduction</span>
      
    </a>
      
      
      <a class="next" href="/robotics-course-materials/blog/dynamical-systems/"><span>Introduction to dynamical systems and ordinary differential equations&nbsp;&raquo;</span>
       
      </a>
     
  </div>
</div>

      </div>
   </div><!-- end .content -->

   <div class="footer">
   <div class="container">

      <div class="footer-links"> 
         <ul class="noList"> 
            
            
             
            
         </ul>
      </div>
   </div>
</div><!-- end .footer -->


   <!-- Add jQuery and other scripts -->
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="/robotics-course-materials"><\/script>')</script>
<script src="/robotics-course-materials/assets/js/dropcap.min.js"></script>
<script src="/robotics-course-materials/assets/js/responsive-nav.min.js"></script>
<script src="/robotics-course-materials/assets/js/scripts.js"></script>


</body>

</html>
